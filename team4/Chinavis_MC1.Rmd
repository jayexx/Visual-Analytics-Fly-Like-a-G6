---
title: "ChinaVis"
author: "SMU Student"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      include = TRUE,
                      warning = FALSE,
                      message = FALSE)

```

## Introduction

NorthClass is a well-known higher education training institution that offers over 100 courses covering various academic disciplines such as literature, science, engineering, medicine, economics, and management, attracting approximately 300,000 registered learners. The institution establishes a flexible and convenient learning environment by providing excellent instructional services. In order to adapt to the development trend of the digital age and enhance its market competitiveness in the field of technology, the institution has planned and launched a programming course. Learners are required to complete specified programming tasks during the learning period, with multiple attempts and submissions allowed to ensure full mastery and application of the knowledge learned. After the completion of the course, the organization gathers learners' temporal learning data to assess whether the quality of instruction meets the predetermined standards and requirements. To optimize teaching resources and improve teaching quality, the institution aims to establish a dedicated Smart Education Development and Innovation Group that will explore how to empower education with next-generation artificial intelligence technology, in order to better cultivate innovative talents who are suited to the development needs of the new era. Visualization and Visual Analytics harness the power of high-bandwidth visual perception channels to transform complex temporal learning behavior data into comprehensible graphical representations. These techniques not only diagnose and analyze learners' knowledge mastery levels but also dynamically monitor the evolving trends in their learning behaviors. Additionally, they effectively identify and dissect potential factors that contribute to learning difficulties. If you were a member of the Smart Education Development and Innovation Group, please design and implement a Visual Analytics solution to help the institution intuitively perceive the learning status of learners and provide feasible suggestions for adjusting teaching strategies and course designs.

## Tasks and Questions:

-   Analyze the log records of learners' question-answering behaviors, quantitatively assess the degree of knowledge mastery based on multi-dimensional attributes such as answer scores and answer status, and identify weak links in their knowledge system. (It is recommended that participants answer this question with no more than 800 words and no more than 5 pictures)

-   Mine personalized learning behavior patterns based on learners' characteristics, and design and present learners' profiles from various perspectives, including peak answering hours, preferred question types, correct answering rates, etc. (It is recommended that participants answer this question with no more than 800 words and 5 pictures)

-   Different learning modes directly impact learners' ability to absorb, integrate, and apply knowledge. Efficient learning modes can enhance deep understanding and long-term memory retention of knowledge. Please model the potential relationship between learning modes and knowledge acquisition, present the results in the form of a graph, and provide a brief analysis. (It is recommended that participants answer this question with no more than 800 words and 5 pictures)

-   The difficulty level of questions should align with the learner's level of knowledge. When a learner possesses a high level of knowledge but achieves a low percentage of correct answers, it indicates that the question's difficulty exceeds their ability. Please utilize Visual Analytics to identify these inappropriate questions. (It is recommended that participants answer this question with no more than 800 words and no more than 5 pictures)

-   Based on the outcomes of the aforementioned analysis, it is crucial to offer valuable recommendations for topic designers and course managers to optimize question bank content settings and enhance the quality of teaching and learning. Please briefly explain the rationale behind these suggestions. (It is recommended that participants answer this question with no more than 800 words and 3 pictures)

## Libraries

For this analysis, we will use the following packages from CRAN.

[`rjson`](https://cran.r-project.org/web/packages/cluster/index.html) - Methods for Cluster analysis. Much extended the original from Peter Rousseeuw, Anja Struyf and Mia Hubert, based on Kaufman and Rousseeuw (1990) "Finding Groups in Data".\
[`tidyverse`](https://www.tidyverse.org/packages/) - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\
[`visNetwork`](https://cran.r-project.org/web/packages/factoextra/readme/README.html#:~:text=The%20R%20package%20factoextra%20has,data%20visualization%20with%20less%20typing.) - Extract and Visualize the Results of Multivariate Data Analyses. [`ggasym`](https://ggobi.github.io/ggally/) Extension of `ggplot2` by adding several functions to reduce the complexity of combining geometric objects with transformed data. [`igraph`](https://ggobi.github.io/ggally/) Extension of `ggplot2` by adding several functions to reduce the complexity of combining geometric objects with transformed data.

```{r}
pacman::p_load(dplyr,tidyr,stringr,readr,fs,purrr,ggplot2, plotly, ggstatsplot,igraph,lubridate,hms, tidyverse)
```

```{r}
df_StudentInfo <- read_csv("data/Data_StudentInfo.csv")
df_TitleInfo <- read_csv("data/Data_TitleInfo.csv")

```

```{r}

saveRDS(title_info_aggregated, "aggregate_title_info.RDS")
```

```{r}
csv_file_list <- dir('data/Data_SubmitRecord')
csv_file_list <- paste0("./data/Data_SubmitRecord/",csv_file_list)


df_StudentRecord <- NULL
for (file in csv_file_list) { # for every file...
  file <- read_csv(file)
    df_StudentRecord <- rbind(df_StudentRecord, file) # then stick together by rows
}
df_StudentRecord %>% glimpse()
```

```{r}
# Step 1: Identify students with multiple classes
students_multiple_classes <- df_StudentRecord %>%
  group_by(student_ID) %>%
  summarise(unique_classes = n_distinct(class)) %>%
  filter(unique_classes > 1)

# Step 2: Identify the correct class for each student (the class with the highest frequency)
correct_classes <- df_StudentRecord %>%
  filter(student_ID %in% students_multiple_classes$student_ID) %>%
  group_by(student_ID, class) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice(1) %>%
  select(student_ID, correct_class = class)

# Step 3: Replace wrong class values
df_StudentRecord <- df_StudentRecord %>%
  left_join(correct_classes, by = "student_ID") %>%
  mutate(class = ifelse(!is.na(correct_class), correct_class, class)) %>%
  select(-correct_class)

# Display the updated dataframe
print(df_StudentRecord)

```

```{r}

# Identify students with multiple classes
students_multiple_classes <- df_StudentRecord %>%
  group_by(student_ID) %>%
  summarise(unique_classes = n_distinct(title_ID)) %>%
  filter(unique_classes > 1)

# Display the results
print(students_multiple_classes)
```

```{r}
#remove index column
#df_StudentRecord <- df_StudentRecord %>% select(-1)

df_TitleInfo <- df_TitleInfo %>% select(-1)
df_StudentInfo <- df_StudentInfo %>% select(-1)

```

```{r}
# Convert time from timestamp to POSIXct
df_StudentRecord$time_change <- as.POSIXct(df_StudentRecord$time, origin="1970-01-01", tz="UTC")

df_StudentRecord <- df_StudentRecord %>%
  mutate(
    time_change = ymd_hms(time_change),
    date = as.Date(time_change),
    time = as_hms(format(time_change, "%H:%M:%S")),
    score = as.factor(score),
    timeconsume = as.numeric(timeconsume)
  ) 

df_TitleInfo <- df_TitleInfo %>%
  rename (
     question_score = score
  )
```

```{r}
missing_students <- anti_join(df_StudentRecord, df_StudentInfo, by = "student_ID")

# Display the missing student IDs
missing_student_ids <- missing_students %>% select(student_ID) %>% distinct()
print(missing_student_ids)


unique(df_StudentRecord$state)

df_StudentRecord <- df_StudentRecord %>%
  filter (state != '�������')%>%
  filter (class != "class")
```

```{r}
# Self join on knowledge to find pairs of title_IDs that share knowledge
edges <- df_TitleInfo %>%
  inner_join(df_TitleInfo, by = "knowledge") %>%
  filter(title_ID.x != title_ID.y) %>%
  select(title_ID.x, title_ID.y) %>%
  distinct()
```

```{r}
# Create a graph from the edges
g <- graph_from_data_frame(edges, directed = FALSE)

# Find connected components
components <- clusters(g)

# Map the component membership back to the title_info dataframe
df_TitleInfo$domain <- components$membership[match(df_TitleInfo$title_ID, names(components$membership))]


```

```{r}
# Aggregate knowledge and sub_knowledge into lists
title_info_aggregated <- df_TitleInfo %>%
  group_by(title_ID,score) %>%
  summarise(knowledge_list = list(unique(knowledge)),
            sub_knowledge_list = list(unique(sub_knowledge)),
            .groups = 'drop')
```

```{r}
# Merge StudentInfo with SubmitRecord based on student_ID
merged_data <- merge(df_StudentRecord, df_StudentInfo, by = "student_ID")

# Merge TitleInfo with the already merged data based on title_ID
merged_data <- merge(merged_data, df_TitleInfo, by = "title_ID")

merged_data <- merged_data %>%
  rename(
    actual_score = score
  ) %>%
  mutate (actual_score = as.numeric(as.character(actual_score)))
```

### Based on point system for mastery.

If student get partially correct, award 1 point, if student get abosultely correct, award 2 points. When the student keep practicing the question, we assume he will be a master of the topic.

```{r}

adjusted_scores <- merged_data %>%
  group_by(student_ID, title_ID, knowledge,class) %>%
  summarise(
    unique_methods = n_distinct(method),
    total_points = sum(case_when(
      state == "Absolutely_Correct" ~ 2,
      state == "Partially_Correct" ~ actual_score/question_score,
      TRUE ~ -1
    )),
    attempts = n(),
    .groups = 'drop'
  ) %>%
  mutate(
    method_bonus = (unique_methods - 1) * 0.5,
    adjusted_total_points = total_points + method_bonus
  )

# Expand rows for questions with multiple knowledge groups
knowledge_expanded <- merged_data %>%
  separate_rows(knowledge, sep = ",") %>%
  mutate(knowledge = str_trim(knowledge))

# Identify students who have never gotten a title_ID Absolutely Correct
never_absolutely_correct <- knowledge_expanded %>%
  group_by(student_ID, title_ID, knowledge) %>%
  summarise(
    never_absolutely_correct = all(state != "Absolutely_Correct"),
    .groups = 'drop'
  ) %>%
  filter(never_absolutely_correct)

# Calculate knowledge mastery scores for each student
knowledge_mastery <- adjusted_scores %>%
  group_by(student_ID, class, knowledge) %>%
  summarise(average_score = mean(adjusted_total_points / attempts, na.rm = TRUE), .groups = 'drop') %>%
  left_join(df_StudentInfo %>% select(student_ID, sex, age, major), by = "student_ID") %>%
  mutate(age = as.character(age))

# Save processed datasets
saveRDS(merged_data, file = "merged_data.RDS")
saveRDS(knowledge_expanded, file = "knowledge_expanded.RDS")
saveRDS(never_absolutely_correct, file = "never_absolutely_correct.RDS")
saveRDS(knowledge_mastery, file = "knowledge_mastery.RDS")

```

```{r}
percentage_correct <- merged_data %>%
  group_by(student_ID, title_ID) %>%
  summarise(
    total_attempts = n(),
    wrong_attempts = sum(state != "Absolutely_Correct"),
    percentage_wrong = wrong_attempts / total_attempts * 100,
    .groups = 'drop'
  )

# Save processed dataset
saveRDS(percentage_correct, file = "percentage_correct.RDS")

```

```{r}
# Calculate total points and number of attempts for each student per title
attempt_scores <- merged_data %>%
  group_by(student_ID, title_ID) %>%
  summarise(total_points = sum(true_points),
            attempts = n(),
            .groups = 'drop')

# Mutate the Never_correct column
attempt_scores <- attempt_scores %>%
  mutate(Never_correct = case_when(
    total_points == -attempts ~ "yes",
    TRUE ~ "no"
  ))

# Save the attempt_scores dataframe as an RDS file
saveRDS(attempt_scores, file = "attempt_scores.RDS")

class_levels <- paste0("Class", 1:15)
domain_mastery <- merged_data %>%
  group_by(student_ID, class, domain) %>%
  summarise(average_score = mean(points), .groups = 'drop') %>%
  left_join(df_StudentInfo %>% select(student_ID, sex, age, major), by = "student_ID") %>%
  mutate(age = as.character(age)) %>%
  mutate(class = factor(class, levels = class_levels))

# Save the domain mastery dataframe as an RDS file
saveRDS(domain_mastery, file = "domain_mastery.RDS")

saveRDS(df_TitleInfo, "title_info.RDS")

```

```{r}

threshold_scores <- calculate_threshold_scores(domain_mastery, "90%")

calculate_threshold_scores <- function(domain_mastery, threshold) {
  percentile <- as.numeric(sub("%", "", threshold)) / 100
  threshold_scores <- domain_mastery %>%
    group_by(domain) %>%
    summarise(threshold_score = quantile(average_score, percentile))
  
  return(threshold_scores)
}

check <- domain_mastery %>%
    inner_join(threshold_scores, by = "domain")

get_high_mastery_students <- function(domain_mastery, threshold_scores) {
  high_mastery_students <- domain_mastery %>%
    inner_join(threshold_scores, by = "domain") %>%
    filter(average_score > threshold_score) %>%
    ungroup()
  return(high_mastery_students)
}

high_mastery_students <- get_high_mastery_students(domain_mastery, threshold_scores)

never_correct_questions <- merged_data %>%
      inner_join(high_mastery_students, by = c("student_ID", "domain", "class"), relationship = "many-to-many") %>%
      group_by(student_ID, title_ID, domain) %>%
      summarise(total_points = sum(points), attempts = n(), .groups = 'drop') %>%
      mutate(Never_correct = if_else(total_points == -attempts, "yes", "no")) %>%
      filter(Never_correct == "yes")

filtered_questions <- never_correct_questions %>%
      left_join(title_info, by = c("title_ID" = "title_ID"))
```

```{r}

calculate_threshold_scores <- function(knowledge_mastery, threshold) {
  percentile <- as.numeric(sub("%", "", threshold)) / 100
  threshold_scores <- knowledge_mastery %>%
    group_by(knowledge) %>%
    summarise(threshold_score = quantile(average_score, percentile, na.rm = TRUE))
  
  return(threshold_scores)
}

get_high_mastery_students <- function(knowledge_mastery, threshold_scores) {
  high_mastery_students <- knowledge_mastery %>%
    inner_join(threshold_scores, by = "knowledge") %>%
    filter(average_score > threshold_score) %>%
    ungroup()
  return(high_mastery_students)
}

get_low_mastery_students <- function(knowledge_mastery, low_threshold_scores) {
  low_mastery_students <- knowledge_mastery %>%
    inner_join(low_threshold_scores, by = "knowledge") %>%
    filter(average_score < threshold_score) %>%
    ungroup()
  return(low_mastery_students)
}



threshold_scores <- calculate_threshold_scores(knowledge_mastery, "99%")
    
high_mastery_students <- get_high_mastery_students(knowledge_mastery, threshold_scores)

low_threshold_scores <- calculate_threshold_scores(knowledge_mastery, "1%")

low_mastery_students <- knowledge_mastery %>%
    inner_join(low_threshold_scores, by = "knowledge")%>%
    filter(average_score < threshold_score)%>%
    ungroup()

```

```{r, fig.width= 15}
filtered_never_correct <- never_absolutely_correct %>%
      filter(student_ID %in% high_mastery_students$student_ID) %>%
      distinct(student_ID, title_ID) %>%
      group_by(title_ID) %>%
      summarise(number_of_students = n(), .groups = 'drop') %>%
      left_join(aggregate_title_info, by = "title_ID") %>%
      mutate(percentage_of_student_wrong = (number_of_students/length(high_mastery_students$student_ID)*100)) 


bottom_percentage_never_correct <- knowledge_expanded %>%
      filter(student_ID %in% low_mastery_students$student_ID) %>%
      filter(title_ID %in% filtered_never_correct$title_ID) %>%
      filter(state == "Absolutely_Correct") %>%
      distinct(student_ID, title_ID) %>%
      group_by(title_ID) %>%
      summarise(no_of_students = n(), .groups = 'drop') %>%
      mutate(percentage_of_student_right = (no_of_students/length(low_mastery_students$student_ID)*100)) 


# Calculate percentages for high mastery and low mastery students
    high_mastery_percent <- filtered_never_correct %>%
      mutate(high_mastery = number_of_students / nrow(high_mastery_students) * 100) %>%
      select (title_ID, high_mastery)
    
    low_mastery_percent <- bottom_percentage_never_correct %>%
      mutate(low_mastery = no_of_students / nrow(low_mastery_students) * 100) %>%
      select (title_ID, low_mastery)
    
    comparison_data <- high_mastery_percent %>%
      left_join(low_mastery_percent, by = "title_ID") %>%
      mutate(diff = round(low_mastery - high_mastery, digits=2)) %>%
      pivot_longer (cols = c(low_mastery,high_mastery)) %>%
      rename (type_of_student = name,
              percentage = value)
    
    low_mastery <-  comparison_data %>%
      filter(type_of_student == "low_mastery")
    
    high_mastery <-  comparison_data %>%
      filter(type_of_student == "high_mastery")
    
    
    ggplot(comparison_data) +
        geom_segment(data = low_mastery,
                     aes(x = percentage, y = title_ID,
                         yend = high_mastery$title_ID, xend = high_mastery$percentage),
                     color = "#aeb6bf",
                     size = 4.5,
                     alpha = 0.5) +
        geom_point(aes(x = percentage, y = title_ID, color = type_of_student), size = 4, show.legend = TRUE) +
        ggtitle("Comparison of High Mastery Student vs Low Mastery Student")+
        geom_text (data = diff,
                   aes(label = paste("D:", diff, "%"), x = x_pos, y = title_ID),
                   fill = "white",
                   color = "#4a4e4d",
                   size = 2.5)+
        facet_grid(title_ID ~ ., scales = "free", switch = "y") +
        theme_minimal()+
        theme(panel.grid.major.y = element_blank(),
              panel.grid.minor.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank(),
              axis.title.y = element_blank(),
              axis.text.y = element_blank(),
              axis.ticks.y = element_blank(),
              axis.ticks.x = element_line(color = "#4a4e4d"),
              text = element_text(family = "Segoe UI Semibold", color = "#4a4e4d"),
              strip.text.y.left  = element_text(angle = 0),
              panel.background = element_rect(fill = "white", color = "white"),
              strip.background = element_rect(fill = "white", color = "white"),
              strip.text = element_text(color = "#4a4e4d", family = "Segoe UI"),
              plot.background = element_rect(fill = "white", color = "white"),
              panel.spacing = unit(0, "lines"),
              plot.margin = margin(1,1,.5,1, "cm"))

```

```{r}

    stats <- comparison_data %>%
      group_by(type_of_student) %>%
      summarise(mean = mean(percentage),
                SE = sd(percentage)) %>%
      mutate(meanpos = mean + 1 *SE,
             meanneg = mean - 1 *SE)
    
    stats_low_mastery <- stats %>%
      filter(type_of_student == "low_mastery")
    stats_high_mastery <- stats %>%
      filter(type_of_student == "high_mastery")
    
    diff <- comparison_data %>% 
      filter(type_of_student == "low_mastery") %>%
      mutate(x_pos = percentage + (-diff/2)) 

```

### NEW ANALYSIS

```{r}

calculate_threshold_scores <- function(knowledge_mastery, threshold) {
  percentile <- as.numeric(sub("%", "", threshold)) / 100
  threshold_scores <- knowledge_mastery %>%
    group_by(knowledge) %>%
    summarise(threshold_score = quantile(average_score, percentile, na.rm = TRUE))
}

get_high_mastery_students <- function(knowledge_mastery, threshold_scores) {
  high_mastery_students <- knowledge_mastery %>%
    inner_join(threshold_scores, by = "knowledge") %>%
    filter(average_score > threshold_score) %>%
    ungroup()
  return(high_mastery_students)
}

get_low_mastery_students <- function(knowledge_mastery, low_threshold_scores) {
  low_mastery_students <- knowledge_mastery %>%
    inner_join(low_threshold_scores, by = "knowledge") %>%
    filter(average_score < threshold_score) %>%
    ungroup()
  return(low_mastery_students)
}

threshold_scores <- calculate_threshold_scores(knowledge_mastery, "99%")
    
high_mastery_students <- get_high_mastery_students(knowledge_mastery, threshold_scores)

low_threshold_scores <- calculate_threshold_scores(knowledge_mastery, "1%")

```

```{r}

high_mastery_incorrect <- merged_data %>%
  filter(student_ID %in% high_mastery_students$student_ID) %>%
  filter(state != "Absolutely_Correct") %>%
  group_by(title_ID, knowledge) %>%
  summarise(incorrect_count = n(),
            total_attempts = n_distinct(student_ID),
            incorrect_percentage = incorrect_count / total_attempts * 100,
            .groups = 'drop')

# Plot: Incorrect Attempts by High Mastery Students
ggplot(high_mastery_incorrect, aes(x = knowledge, y = title_ID, fill = incorrect_percentage)) +
  geom_tile() +
  scale_fill_gradient(low = "lightblue", high = "red") +
  theme_minimal() +
  labs(title = "Incorrect Attempts by High Mastery Students", x = "Knowledge Group", y = "Title ID", fill = "Incorrect %")



```

```{r}

# Data Cleaning and Preparation
method_impact <- adjusted_scores %>%
  group_by(unique_methods) %>%
  summarise(average_score = mean(adjusted_total_points / attempts, na.rm = TRUE), .groups = 'drop')

# Plot: Method Usage Impact
plot4 <- ggplot(method_impact, aes(x = factor(unique_methods), y = average_score)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  theme_minimal() +
  labs(title = "Impact of Method Usage on Mastery Scores", x = "Unique Methods Used", y = "Average Adjusted Score")

plotly::ggplotly(plot4)

```

```{r}

# Data Cleaning and Preparation
library(ltm)

irt_data <- adjusted_scores %>%
  mutate(correct = ifelse(adjusted_total_points > 0, 1, 0))

# Fit IRT model
irt_model <- ltm(correct ~ z1, data = irt_data)

# Plot: Item Characteristic Curves
plot5 <- plot(irt_model, type = "ICC", lwd = 2, col = c("blue", "red", "green"))

plotly::ggplotly(plot5)

```

```{r}
reliability_data <- adjusted_scores %>%
  select(student_ID, title_ID, adjusted_total_points) %>%
  spread(title_ID, adjusted_total_points, fill = NA)

# Calculate Cronbach's alpha
cronbach_alpha <- alpha(reliability_data)

# Display Result
print(cronbach_alpha)


```

```{r}
# Data Cleaning and Preparation
distractor_analysis <- adjusted_scores %>%
  filter(attempts > 0) %>%
  group_by(title_ID, knowledge) %>%
  summarise(distractor_count = n(), .groups = 'drop')

# Plot: Distractor Analysis
plot7 <- ggplot(distractor_analysis, aes(x = knowledge, y = title_ID, fill = distractor_count)) +
  geom_tile() +
  scale_fill_gradient(low = "yellow", high = "red") +
  theme_minimal() +
  labs(title = "Distractor Analysis", x = "Knowledge Group", y = "Title ID", fill = "Distractor Count")

plotly::ggplotly(plot7)



```

```{r}

# Data Cleaning and Preparation
biserial_data <- adjusted_scores %>%
  filter(student_ID %in% high_mastery_students$student_ID) %>%
  group_by(title_ID) %>%
  summarise(biserial_correlation = biserial.cor(adjusted_total_points, as.numeric(adjusted_total_points > 0)), .groups = 'drop')

# Plot: Biserial Correlation
plot8 <- ggplot(biserial_data, aes(x = reorder(title_ID, biserial_correlation), y = biserial_correlation)) +
  geom_bar(stat = "identity", fill = "purple") +
  theme_minimal() +
  labs(title = "Biserial Correlation", x = "Title ID", y = "Biserial Correlation")

plotly::ggplotly(plot8)

```
