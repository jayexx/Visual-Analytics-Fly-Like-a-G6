---
title: "Take Home Exercise 3: Learning Behavior Patterns Analysis"
author: "Wang Yuhui"
date: "June 2, 2024"
date-modified: "last-modified"
execute: 
  warning: false
  freeze: true
---

```{r}
#| code-fold: true
#| code-summary: "show the code"
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)
library(cluster)
library(factoextra)
library(fmsb)
library(reshape2)
library(networkD3)
library(ggalluvial)
library(fastDummies)
library(parallelPlot)

pacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial) 
```

# 1 Objective

Mine **personalized learning behavior patterns** based on **learners' characteristics**.

Design and present **learners' profiles** from various perspectives, including:

**-peak answering hours,**

**-preferred question types,**

**-correct answering rates, etc.**

# 2 Importing the Data

The data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.

-   Dataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project
-   Dataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project
-   Dataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners' answering variables to the questions collated in the scope of this project

From the raw data, the file was cleaned, prepared and merged in the Data Preparation Steps.

The code chunk below imports the prepared dataset into R environment by using [*`read_csv()`*](https://readr.tidyverse.org/reference/read_delim.html) function of [`readr`](https://readr.tidyverse.org/), which is part of the tidyverse package.

```{r}
merged_data <- readRDS("merged_data_df.rds")

head (merged_data)
```

## 2.1 Converting the formatt of the column

```{r}
merged_data <- merged_data %>%
  mutate(day = wday(as.POSIXct(time, origin = "1970-01-01", tz = "UTC"), week_start = 1))
unique(merged_data$day)
```

## 2.2 Final data

Now we merged with student information and rearrange the column for the further analysis.

```{r}
stu_info <- merged_data %>%
  distinct(student_ID, .keep_all = TRUE) %>%
  select(student_ID, sex, age, major)

merged_data <- merged_data %>%
  mutate(rate = actual_score / question_score) %>%
  select(-actual_score, -question_score)

# 计算每个学生的平均rate
avg_rate <- merged_data %>%
  group_by(student_ID) %>%
  summarise(average_rate = mean(rate, na.rm = TRUE))

# 将day中的1 2 3 4 5计为'week'，6 7计为'weekend'
merged_data <- merged_data %>%
  mutate(week_category = ifelse(day %in% 1:5, "week", "weekend"))

# 计算每个学生每种knowledge的百分比
knowledge_percentage <- merged_data %>%
  group_by(student_ID, knowledge) %>%
  summarise(counts = n()) %>%
  ungroup() %>%
  group_by(student_ID) %>%
  mutate(total_counts = sum(counts),
         percentage = counts / total_counts) %>%
  select(student_ID, knowledge, percentage) %>%
  spread(key = knowledge, value = percentage, fill = 0)

# 计算每个学生在week和weekend的百分比
weekend_percentage <- merged_data %>%
  group_by(student_ID, week_category) %>%
  summarise(counts = n()) %>%
  ungroup() %>%
  group_by(student_ID) %>%
  mutate(total_counts = sum(counts),
         percentage = counts / total_counts) %>%
  select(student_ID, week_category, percentage) %>%
  spread(key = week_category, value = percentage, fill = 0)

# 合并学生信息和计算结果
final_data <- stu_info %>%
  left_join(avg_rate, by = "student_ID") %>%
  left_join(merged_data %>% select(student_ID, -day) %>% distinct(), by = "student_ID") %>%
  left_join(knowledge_percentage, by = "student_ID") %>%
  left_join(weekend_percentage, by = "student_ID")


# 查看结果
head(final_data)
```

# 3 feature

## 3.1 question

```{r}
# 提取每个学生提交记录中出现频率最高的title
title_frequency <- merged_data %>%
  group_by(student_ID, title_ID) %>%
  summarise(frequency = n(), .groups = 'drop') %>%
  arrange(student_ID, desc(frequency)) %>%
  distinct(student_ID, .keep_all = TRUE) %>%
  select(student_ID, title_ID)

# 合并数据框 time 和 title_frequency
question <- final_data %>%
  select(student_ID, week, weekend) %>%
  left_join(title_frequency, by = "student_ID")

# 查看结果
head(question)
```

```{r}
title_encode <- readRDS('title_encode.rds')
# 重命名编码表列
colnames(title_encode) <- c("title_ID", "title_pre")

# 合并title_frequency和title_encoding
title_frequency_encoded <- title_frequency %>%
  left_join(title_encode, by = "title_ID") %>%
  select(student_ID, title_pre)

# 合并数据框 time 和 title_frequency_encoded
question <- question %>%
  left_join(title_frequency_encoded, by = "student_ID")

# 查看结果
head(question)
```

## 3.3 method

```{r}
# 提取每个学生提交记录中出现频率最高的title
method_frequency <- merged_data %>%
  group_by(student_ID, method) %>%
  summarise(frequency = n(), .groups = 'drop') %>%
  arrange(student_ID, desc(frequency)) %>%
  distinct(student_ID, .keep_all = TRUE) %>%
  select(student_ID, method)

# 合并数据框 time 和 title_frequency
method <- question %>%
  left_join(method_frequency, by = "student_ID")

# 查看结果
head(method)
```

```{r}
method_encode <- readRDS('method_encode.rds')
colnames(method_encode) <- c("method", "method_pre")

# 合并title_frequency和title_encoding
method_frequency_encoded <- method_frequency %>%
  left_join(method_encode, by = "method") %>%
  select(student_ID, method, method_pre)

# 合并数据框 time 和 title_frequency_encoded
method <- question %>%
  left_join(method_frequency_encoded, by = "student_ID")

# 查看结果
head(method)
```

## 3.3 knowledge

```{r}
# 创建 knowledge dataframe 并放入 student_ID 列的内容
knowledge <- data.frame(student_ID = final_data$student_ID)

# 找到每一行 6-15 列的最高值对应的列名
knowledge$knowledge <- apply(final_data[, 6:15], 1, function(row) {
  colnames(final_data)[6:15][which.max(row)]
})

# 查看结果
head(knowledge)
```

```{r}
knowledge_encode <- readRDS('knowledge_encode.rds')
# 重命名编码表的列
colnames(knowledge_encode) <- c("knowledge", "knowledge_pre")

# 合并知识偏好和编码表
knowledge <- knowledge %>%
  left_join(knowledge_encode, by = "knowledge") 

knowledge <- method %>%
  left_join(knowledge, by = "student_ID")

# 查看结果
head(knowledge)
```

## 3.4 Correct rate trend score

```{r}
title_info <- readRDS("title_info.rds")
# 初始化结果数据框
trend_scores <- data.frame()

# 定义计算正确率趋势的函数
calculate_trend <- function(data) {
  if (nrow(data) < 2) {
    return(NA)
  }
  model <- lm(rate ~ attempt, data = data)
  return(coef(model)[2]) # 返回斜率
}

# 遍历每位学生
for (I in 1:nrow(knowledge)) {
  student_id <- knowledge$student_ID[I]
  title_id <- knowledge$title_ID[I]
  
  # 找到该学生在该题目的所有答题记录并排序
  student_data <- merged_data %>%
    filter(student_ID == student_id, title_ID == title_id) %>%
    arrange(time)
  
  # 增加attempt列
  student_data <- student_data %>%
    mutate(attempt = row_number() - 1)
  
  # 计算该学生在该题目的正确率趋势
  trend <- calculate_trend(student_data)
  
  # 将结果存储在结果数据框中
  trend_scores <- rbind(trend_scores, data.frame(student_ID = student_id, title_ID = title_id, trend = trend))
}

# 将title_info中的分数信息合并到结果数据框中
trend_scores <- trend_scores %>%
  left_join(title_info %>% select(title_ID, score), by = "title_ID") %>%
  mutate(correct_rate_trend_score = trend * score)

# 查看结果
head(trend_scores)
```

```{r}

# 确保每个 student_ID 只有一个 correct_rate_trend_score，取平均值
trend_scores_agg <- trend_scores %>%
  group_by(student_ID) %>%
  summarise(trend_score = mean(correct_rate_trend_score, na.rm = TRUE))

# 合并到 knowledge 数据框中
cluster_data <- knowledge %>%
  left_join(trend_scores_agg, by = 'student_ID')

# 查看结果
head(cluster_data)
```

```{r}
# 遍历每位学生
plot_data <- data.frame()

for (I in 1:nrow(knowledge)) {
  student_id <- knowledge$student_ID[I]
  title_id <- knowledge$title_ID[I]
  
  # 找到该学生在该题目的所有答题记录并排序
  student_data <- merged_data %>%
    filter(student_ID == student_id, title_ID == title_id) %>%
    arrange(time)
  
  # 增加attempt列
  student_data <- student_data %>%
    mutate(attempt = row_number() - 1)
  
  # 保留尝试次数大于10的学生数据
  if (nrow(student_data) > 10) {
    plot_data <- rbind(plot_data, student_data)
  }
}

# 绘制散点图和趋势线
ggplot(plot_data, aes(x = attempt, y = rate)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue", linetype = "dashed") +
  labs(title = "Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts",
       x = "Attempt",
       y = "Correct Rate") +
  theme_minimal()

# 如果需要按学生分组进行绘图
ggplot(plot_data, aes(x = attempt, y = rate, color = student_ID)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed") +
  labs(title = "Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts",
       x = "Attempt",
       y = "Correct Rate") +
  theme_minimal() +
  theme(legend.position = "none") # 如果学生太多，可以移除图例
```

## 3.5 sex, age, major

```{r}
# 合并 student_info 中的 sex, age, major 列到 cluster_data 中
cluster_data <- cluster_data %>%
  left_join(stu_info %>% select(student_ID, sex, age, major), by = "student_ID")

# 查看结果
head(cluster_data)
```

female = 1

male = 0

```{r}
major_encode <- readRDS('major_encode.rds')

# 对 sex 进行编码
cluster_data <- cluster_data %>%
  mutate(sex = ifelse(sex == "female", 1, 0))

# 合并 major 编码
major_encode <- major_encode %>%
  rename(major_name = major1, major_code = major_encode)

cluster_data <- cluster_data %>%
  left_join(major_encode, by = c("major" = "major_name")) %>%
  select(-major) %>%
  rename(major = major_code) 

# 查看结果
head(cluster_data)
```

# 4 visualization

## 4.1 k-means clustering

```{r}
# 检查数据中的NA值
colSums(is.na(cluster_data))

# 处理NA值，使用中位数填补
cluster_data_clean <- cluster_data %>%
  mutate(across(2:10, ~ifelse(is.na(.), median(., na.rm = TRUE), .)))

# 计算相关矩阵
SLM.cor <- cor(cluster_data_clean[, c(2, 3, 5, 7, 9, 10, 11, 12, 13)], use = "complete.obs")

# 绘制相关图
corrplot(SLM.cor, 
         method = "ellipse", 
         tl.pos = "lt",
         tl.col = "black",
         order = "hclust",
         hclust.method = "ward.D",
         addrect = 3)

colSums(is.na(cluster_data_clean))
```

```{r}
# Exclude non-numeric columns
cluster_numeric <- cluster_data_clean %>%
  select(-student_ID, -title_ID, -method, -knowledge)

# Function to compute silhouette widths
silhouette_analysis <- function(data, max_clusters) {
  avg_sil_widths <- numeric(max_clusters)
  
  for (k in 2:max_clusters) {
    # Perform k-means clustering
    kmeans_result <- kmeans(data, centers = k, nstart = 25)
    
    # Compute silhouette widths
    sil <- silhouette(kmeans_result$cluster, dist(data))
    
    # Calculate average silhouette width
    avg_sil_widths[k] <- mean(sil[, 3])
  }
  
  return(avg_sil_widths)
}

# Determine the maximum number of clusters to test
max_clusters <- 18

# Perform silhouette analysis
avg_sil_widths <- silhouette_analysis(cluster_numeric, max_clusters)

# Plot the average silhouette widths
plot(1:max_clusters, avg_sil_widths, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters", ylab = "Average silhouette width",
     main = "Silhouette Analysis for Determining Optimal Number of Clusters")

# Highlight the optimal number of clusters
optimal_clusters <- which.max(avg_sil_widths)
points(optimal_clusters, avg_sil_widths[optimal_clusters], col = "red", pch = 19)
```
```{r}
saveRDS(avg_sil_widths,'avg_sil_widths.rds')
```

```{r}
# Function to compute SSE for different numbers of clusters
compute_sse <- function(data, max_clusters) {
  sse <- numeric(max_clusters)
  
  for (k in 1:max_clusters) {
    # Perform k-means clustering
    kmeans_result <- kmeans(data, centers = k, nstart = 25)
    
    # Compute SSE
    sse[k] <- kmeans_result$tot.withinss
  }
  
  return(sse)
}

# Determine the maximum number of clusters to test
max_clusters <- 18

# Compute SSE for each number of clusters
sse_values <- compute_sse(cluster_numeric, max_clusters)

# Plot SSE against number of clusters
plot(1:max_clusters, sse_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters", ylab = "SSE",
     main = "Elbow Method for Optimal Number of Clusters")

# Add text for elbow point
elbow_point <- which.min(diff(sse_values)) + 1
text(elbow_point, sse_values[elbow_point], labels = paste("Elbow Point:", elbow_point), pos = 4, col = "red")
```

```{r}
# Drop the student_ID column
clustering_data <- cluster_data_clean %>%
  select(-student_ID, -title_ID, -method, -knowledge)

# Standardize the data
clustering_data_scaled <- scale(clustering_data)

# Perform k-means clustering
set.seed(123)  # For reproducibility
kmeans_result <- kmeans(clustering_data_scaled, centers = 2, nstart = 25)

# Add the cluster assignments to the original data
cluster_data_clean$cluster <- kmeans_result$cluster
```

```{r}
# Perform PCA
pca_result <- prcomp(clustering_data[-1], scale. = TRUE)

# Get PCA scores
pca_scores <- as.data.frame(predict(pca_result))

# Add cluster information to PCA scores
pca_scores$cluster <- factor(cluster_data_clean$cluster)

# Plot PCA results with cluster color coding
pca_plot <- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +
  geom_point(size = 3) +
  scale_color_discrete(name = "Cluster") +
  labs(x = "Principal Component 1", y = "Principal Component 2",
       title = "PCA Plot of Clusters") +
  theme_minimal()

# Display the plot
pca_plot
```

```{r}
#| fig-width: 15
#| fig-height: 10

cluster_factor <- cluster_data_clean
cluster_factor$cluster <- as.character(cluster_factor$cluster)

ggparcoord(data = cluster_factor, 
           columns = c(2:13), 
           groupColumn = 14,
           scale = "uniminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Parallel Coordinates Plot of Students' learning modes")+
   theme(axis.text.x = element_text(angle = 30, size = 20))
```
```{r}
saveRDS(cluster_factor,'cluster_factor.rds')
```

```{r}
#| fig-width: 30
#| fig-height: 10

ggparcoord(data = cluster_factor, 
           columns = c(2:13), 
           groupColumn = 14,
           scale = "uniminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Parallel Coordinates Plot of Students' learning modes")+
   theme(axis.text.x = element_text(angle = 30, size = 20))

ggparcoord(data = cluster_factor, 
columns = c(2:13), 
           groupColumn = 14,
           scale = "uniminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Parallel Coordinates Plot of Students' learning modes")+
  facet_wrap(~ cluster)+
  theme(axis.text.x = element_text(angle = 30, size = 20))
```

```{r}
#| fig-width: 30
#| fig-height: 30

# 将分类特征转换为因子类型
categorical_columns <- c("sex", "age", "knowledge_pre", "major", "method_pre", "title_pre", "cluster")
cluster_data_clean[categorical_columns] <- lapply(cluster_data_clean[categorical_columns], as.factor)

# 创建alluvial plot
ggplot_alluvial <- ggplot(cluster_data_clean,
       aes(axis1 = sex, axis2 = age, axis3 = knowledge_pre, axis4 = major, axis5 = method_pre, axis6 = title_pre, axis7 = cluster,
           y = ..count..)) +
  scale_x_discrete(limits = c("Sex", "Age", "Knowledge_pre", "Major", "Method_pre", "Title_pre", "Cluster"), expand = c(.1, .1)) +
  geom_alluvium(aes(fill = cluster, text = cluster), width = 0.25) +
  geom_stratum(aes(text = after_stat(stratum)), width = 0.25) +
  theme_minimal() +
  labs(title = "Alluvial Plot of Students Data Distribution and Flow",
       y = "Number of Students",
       x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,size = 10))

# 转换为互动图表并添加悬停信息
interactive_plot <- ggplotly(ggplot_alluvial, tooltip = "text") %>% layout(showlegend = FALSE)

# 显示互动图表
interactive_plot
```

```{r}

cluster_factor1 <- select(cluster_factor, c(1, 2:13))
cluster_factor_matrix <- data.matrix(cluster_factor1)

cluster_data_d <- dist(normalize(cluster_factor_matrix[, -c(1)]), method = "euclidean")
dend_expend(cluster_data_d)[[3]]
```

```{r}
cluster_data_clust <- hclust(cluster_data_d, method = "average")
num_k <- find_k(cluster_data_clust)
plot(num_k)
```

```{r}
#| fig-width: 15
#| fig-height: 12
heatmaply(normalize(cluster_factor_matrix[, -c(1)]),
          dist_method = "euclidean",
          hclust_method = "average",
          k_row = 2,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,          
          main="Students' Learning Mode Clustering \nDataTransformation using Normalise Method",
          xlab = "Student_IDs",
          ylab = "Learning Behaior Pattern"
)
```

```{r}
# 将需要的列转换为因子类型
categorical_columns <- c("sex", "age", "knowledge", "major", "method", "title_ID", "cluster")
cluster_data_clean[categorical_columns] <- lapply(cluster_data_clean[categorical_columns], as.factor)

# 计算每个分类变量中cluster 1和2的占比
get_cluster_percentage <- function(data, column) {
  data %>%
    group_by(!!sym(column), cluster) %>%
    summarise(count = n()) %>%
    mutate(percentage = count / sum(count) * 100) %>%
    filter(cluster %in% c(1, 2))
}

# 获取每个分类变量的占比数据框
sex_cluster_percentage <- get_cluster_percentage(cluster_data_clean, "sex")
age_cluster_percentage <- get_cluster_percentage(cluster_data_clean, "age")
knowledge_cluster_percentage <- get_cluster_percentage(cluster_data_clean, "knowledge")
major_cluster_percentage <- get_cluster_percentage(cluster_data_clean, "major")
method_cluster_percentage <- get_cluster_percentage(cluster_data_clean, "method")
title_cluster_percentage <- get_cluster_percentage(cluster_data_clean, "title_ID")

# 打印结果
print(sex_cluster_percentage)
print(age_cluster_percentage)
print(knowledge_cluster_percentage)
print(major_cluster_percentage)
print(method_cluster_percentage)
print(title_cluster_percentage)
```
