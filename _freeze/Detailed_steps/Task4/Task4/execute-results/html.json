{
  "hash": "4ab0af1d3287e98f5a21e33e82896114",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Task 4\"\nauthor: \"Jayexx, fudi, yuhui\"\ndate: \"June 1, 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n## Introduction\n\nThese are the detailed steps taken for Task 4 of the project.\n\n### Objective & Task Requirements\n\nThe key objective of Task 3 is:\n\n-   To analyse and provide a visual representation of the alignment between question difficulty and learners knowledge level, and hence to identify inappropriate questions where high knowledge level students had a lower correct answering rate.\n\nThis would entail the following sub-task requirements:\n\n-   To idenify high knowledge students\n-   To identify any questions that these students had a lower answering rate, and also conversely find those that low knowledge level students scored better in\n\n## Getting Started\n\n### Loading Required R Package Libraries\n\nThe code chunk below loads the following libraries:\n\n-   [`tidyverse`](https://www.tidyverse.org/packages/): an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\n-   `knitr`: for creating dynamic html tables/reports\n-   `ggridges`: extension of ggplot2 designed for plotting ridgeline plots\n-   `ggdist`: extension of ggplot2 designed for visualising distribution and uncertainty,\n-   `colorspace`: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\n-   `ggrepel`: provides geoms for ggplot2 to repel overlapping text labels.\n-   `ggthemes`: provides additional themes, geoms, and scales for ggplot package\n-   `hrbrthemes`: provides typography-centric themes and theme components for ggplot package\n-   `patchwork`: preparing composite figure created using ggplot package\n-   `lubridate`: for wrangling of date-time data\n-   `ggstatplot`: provides alternative statistical inference methods by default as an extension of the ggplot2 package\n-   `plotly`: R library for plotting interactive statistical graphs.\n-   [`rjson`](https://cran.r-project.org/web/packages/cluster/index.html): Methods for Cluster analysis.\n-   [`visNetwork`](https://cran.r-project.org/web/packages/factoextra/readme/README.html#:~:text=The%20R%20package%20factoextra%20has,data%20visualization%20with%20less%20typing.): Extract and Visualize the Results of Multivariate Data Analyses.\n-   [`BiocManager`](https://ggobi.github.io/ggally/): Extension of `ggplot2` by adding several functions to reduce the complexity of combining geometric objects with transformed data.\n-   [`igraph`](https://ggobi.github.io/ggally/): Extension of `ggplot2` by adding several functions to reduce the complexity of combining geometric objects with transformed data.\n-   cluster\n-   factoextra\n-   stats\n-   hms\n-   caret\n-   ggfortify\n-   gridExtra\n-   GGally\n-   parallelPlot\n-   seriation\n-   dendextend\n-   heatmaply\n-   corrplot\n-   ggalluvial\n-   entropy\n-   ineq\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial, entropy, ineq) \n```\n:::\n\n\n### Importing the Data\n\nThe data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.\n\n-   Dataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\n-   Dataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\n-   Dataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners' answering variables to the questions collated in the scope of this project\n\nFrom the raw data, the file was cleaned, prepared and merged in the Data Preparation Steps.\n\nThe code chunk below imports the prepared dataset into R environment by using [*`read_csv()`*](https://readr.tidyverse.org/reference/read_delim.html) function of [`readr`](https://readr.tidyverse.org/), which is part of the tidyverse package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmerged_data <- readRDS(\"merged_data_df.rds\")\n\nglimpse (merged_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 232,811\nColumns: 39\n$ title_ID       <chr> \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8EK…\n$ student_ID     <chr> \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b2292…\n$ class          <chr> \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"Cla…\n$ time           <dbl> 1696330917, 1699625054, 1697444103, 1695964704, 1697727…\n$ state          <chr> \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"Pa…\n$ actual_score   <dbl> 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1…\n$ method         <chr> \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvyGd…\n$ memory         <dbl> 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320, 3…\n$ timeconsume    <dbl> 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3, 3…\n$ time_change    <dttm> 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16 0…\n$ sex            <chr> \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"ma…\n$ age            <dbl> 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 21,…\n$ major          <chr> \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J401…\n$ b3C9s_j0v1yls8 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ question_score <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ knowledge      <chr> \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"…\n$ sub_knowledge  <chr> \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"…\n```\n\n\n:::\n:::\n\n\n## Metrics Selection and Computation\n\n### For High and Low Knowledge Level Learners\n\nBased on assessment of the given data and contextual information provided, the selected approach to identify high knowledge level learners is as follows:\n\n-   Compute mastery points of students\n\nRecap on Mastery Point metric from task 1:\n\n  -   Proportion of absolutely and partially correct attempts:\n    -   absolutely correct attempts - award 1 pt \n    -   partially correct attempts - award (actual_score / question_score) \n    -   normalise attempts across questions - uses (total point / total attempts)\\\n  -   Use of more than 1 method per question - multiply by no. of methods if absolutely correct attempt submitted for that question\n\n-   Identify the top percentile of students with the highest mastery points as high mastery students\n-   Conversely, identify the bottom percentile of students with lowest mastery points as low mastery students\n\nThis is computed in the following code chunks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assign points to attempts based on state and actual score\nadjusted_scores <- merged_data %>%\n  mutate(points = case_when(\n    state == \"Absolutely_Correct\" ~ 1,\n    state == \"Partially_Correct\" ~ actual_score / question_score,\n    TRUE ~ 0 # default case for any unexpected states\n  ))\n\n# Assign points to title_IDs per student factoring in normalisation and multiple methods used\nmastery_scores_byQns <- adjusted_scores %>%\n  group_by(student_ID, title_ID) %>%\n  summarise(\n    total_points = sum(points),\n    total_attempts = n(),\n    unique_methods = n_distinct(method),\n    absolutely_correct_methods = sum(points == 1)\n  ) %>%\n  mutate(\n    adjusted_points = total_points / total_attempts,\n    adjusted_points = adjusted_points * ifelse(absolutely_correct_methods > 0, unique_methods, 1)\n  )\n\nglimpse(mastery_scores_byQns)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50,482\nColumns: 7\nGroups: student_ID [1,364]\n$ student_ID                 <chr> \"0088dc183f73c83f763e\", \"0088dc183f73c83f76…\n$ title_ID                   <chr> \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_…\n$ total_points               <dbl> 1.000000, 1.000000, 4.666667, 7.666667, 1.0…\n$ total_attempts             <int> 23, 9, 7, 22, 1, 1, 2, 4, 8, 11, 1, 1, 5, 1…\n$ unique_methods             <int> 5, 4, 5, 5, 1, 1, 1, 2, 5, 5, 1, 1, 2, 1, 1…\n$ absolutely_correct_methods <int> 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ adjusted_points            <dbl> 0.2173913, 0.4444444, 3.3333333, 1.7424242,…\n```\n\n\n:::\n:::\n\n\n```\n# Combine the adjusted score with knowledge-transposed titleInfo dataframe\nmastery_scores2 <- df_TitleInfo_gp %>%\n  distinct(title_ID, .keep_all = TRUE) %>%\n  left_join(mastery_scores1, by = \"title_ID\") %>%\n  rename(knowledge = knowledge.x) %>%\n  select(-score,\n         -knowledge.y)\n  \nglimpse(mastery_scores2)\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summing up points to identify high and low mastery students\nmastery_scores_byStudent <- mastery_scores_byQns %>%\n  group_by(student_ID) %>%\n  summarize(\n    # Part (a): Sum of total points across all questions\n    `Sum of points Overall` = sum(adjusted_points, na.rm = TRUE)\n    \n  )\n\nglimpse(mastery_scores_byStudent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,364\nColumns: 2\n$ student_ID              <chr> \"0088dc183f73c83f763e\", \"00cbf05221bb479e66c3\"…\n$ `Sum of points Overall` <dbl> 38.75682, 35.69721, 35.28832, 42.32937, 31.982…\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(mastery_scores_byStudent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [1,364 × 2] (S3: tbl_df/tbl/data.frame)\n $ student_ID           : chr [1:1364] \"0088dc183f73c83f763e\" \"00cbf05221bb479e66c3\" \"00df647ee4bf7173642f\" \"0107f72b66cbd1a0926d\" ...\n $ Sum of points Overall: num [1:1364] 38.8 35.7 35.3 42.3 32 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(mastery_scores_byStudent)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  student_ID           `Sum of points Overall`\n  <chr>                                  <dbl>\n1 0088dc183f73c83f763e                    38.8\n2 00cbf05221bb479e66c3                    35.7\n3 00df647ee4bf7173642f                    35.3\n4 0107f72b66cbd1a0926d                    42.3\n5 011d454f199c123d44ad                    32.0\n6 01558eef77a8d39b7103                    38.2\n```\n\n\n:::\n:::\n\n\nThe high knowledge level (aka high mastery) and low knowledge level (aka low mastery) are identified with the following code chunk\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalculate_threshold_scores <- function(df, column, threshold) {\n  percentile <- as.numeric(sub(\"%\", \"\", threshold)) / 100\n  threshold_score <- quantile(df[[column]], percentile, na.rm = TRUE)\n  \n  return(threshold_score)\n}\n\n# Calculate threshold scores for 99% and 1%\nthreshold_95 <- calculate_threshold_scores(mastery_scores_byStudent, \"Sum of points Overall\", \"95%\")\nthreshold_5 <- calculate_threshold_scores(mastery_scores_byStudent, \"Sum of points Overall\", \"5%\")\n\n# Filter high and low mastery students based on the threshold scores\nhigh_mastery_students <- mastery_scores_byStudent %>%\n  filter(`Sum of points Overall` > threshold_95)\n\nlow_mastery_students <- mastery_scores_byStudent %>%\n  filter(`Sum of points Overall` < threshold_5)\n\nglimpse(high_mastery_students)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 69\nColumns: 2\n$ student_ID              <chr> \"03754dce5ec7731fb3e2\", \"06aff3e28c5db152f506\"…\n$ `Sum of points Overall` <dbl> 59.30627, 53.34722, 48.94286, 54.85595, 50.353…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(low_mastery_students)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 69\nColumns: 2\n$ student_ID              <chr> \"02068067a434adb6556a\", \"05e1c3ce1728da7e37e0\"…\n$ `Sum of points Overall` <dbl> 0.0000000, 17.1165831, 23.5655844, 21.9944444,…\n```\n\n\n:::\n:::\n\n\n### Poor correct answer rate\n\nBased on assessment of the given data and contextual information provided, the selected metrics to identify  poor correct answering rate is assessed to be as follows:\n\n-   Never Correct Questions - Questions where a learner never gotten absolutely correct across all answers submitted\n-   Error rate - percentage of wrong answer (0 score) submissions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Never Correct title_IDs for individual students\nnever_absolutely_correct <- merged_data %>%\n  group_by(student_ID, title_ID) %>%\n  summarise(\n    never_absolutely_correct = all(state != \"Absolutely_Correct\"),\n    .groups = 'drop'\n  )  %>%\n  filter(never_absolutely_correct)\n\n# Save processed dataset\nsaveRDS(never_absolutely_correct, file = \"never_absolutely_correct.RDS\")\n\nglimpse(never_absolutely_correct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 7,022\nColumns: 3\n$ student_ID               <chr> \"01qkq6w2v62cimidb3b7\", \"01qkq6w2v62cimidb3b7…\n$ title_ID                 <chr> \"Question_4nHcauCQ0Y6Pm8DgKlLo\", \"Question_62…\n$ never_absolutely_correct <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the error rate with consideration of different methods\npercentage_error <- merged_data %>%\n  group_by(student_ID, title_ID) %>%\n  summarise(\n    total_attempts = n(),\n    wrong_attempts = sum(state != \"Absolutely_Correct\"),\n    percentage_wrong = (wrong_attempts / total_attempts) * 100,\n    .groups = 'drop'\n  )\n\n# View the result\nglimpse(percentage_error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 50,482\nColumns: 5\n$ student_ID       <chr> \"0088dc183f73c83f763e\", \"0088dc183f73c83f763e\", \"0088…\n$ title_ID         <chr> \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3oPyUzDmQt…\n$ total_attempts   <int> 23, 9, 7, 22, 1, 1, 2, 4, 8, 11, 1, 1, 5, 1, 1, 14, 2…\n$ wrong_attempts   <int> 22, 8, 6, 21, 0, 0, 1, 3, 6, 10, 0, 0, 4, 0, 0, 12, 0…\n$ percentage_wrong <dbl> 95.65217, 88.88889, 85.71429, 95.45455, 0.00000, 0.00…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Save processed dataset\nsaveRDS(percentage_error, file = \"percentage_error.RDS\")\n```\n:::\n\n\n\n## Visualising In appropriate questions\n\n### Dumbell plot\n\nDumbbell Plot Analysis Utilizing a dumbbell plot, we examined the discrepancies in question performance between high and low mastery students based on percentage of never correct questions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter never absolutely correct questions based on high mastery students\nfiltered_never_correct <- never_absolutely_correct %>%\n      filter(student_ID %in% high_mastery_students$student_ID) %>%\n      distinct(student_ID, title_ID) %>%\n      group_by(title_ID) %>%\n      summarise(\n#        number_of_students = n_distinct(student_ID[never_absolutely_correct == TRUE]),\n        number_of_students = n(),\n        .groups = 'drop') %>%\n#      left_join(aggregate_title_info, by = \"title_ID\") %>%\n      mutate(percentage_of_student_wrong = (number_of_students/length(high_mastery_students$student_ID)*100)) \n    \n    \n# Check if bottom percentage students got the questions that were never correct by high mastery students correct\nbottom_percentage_never_correct <- merged_data %>%\n      filter(student_ID %in% low_mastery_students$student_ID) %>%\n      filter(title_ID %in% filtered_never_correct$title_ID) %>%\n      filter(state == \"Absolutely_Correct\") %>%\n      distinct(student_ID, title_ID) %>%\n      group_by(title_ID) %>%\n      summarise(no_of_students = n(), .groups = 'drop') %>%\n      mutate(percentage_of_student_right = (no_of_students/length(low_mastery_students$student_ID)*100))\n    \nglimpse(filtered_never_correct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 12\nColumns: 3\n$ title_ID                    <chr> \"Question_3oPyUzDmQtcMfLpGZ0jW\", \"Question…\n$ number_of_students          <int> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1\n$ percentage_of_student_wrong <dbl> 1.449275, 1.449275, 1.449275, 1.449275, 1.…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(bottom_percentage_never_correct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 12\nColumns: 3\n$ title_ID                    <chr> \"Question_3oPyUzDmQtcMfLpGZ0jW\", \"Question…\n$ no_of_students              <int> 34, 35, 34, 35, 29, 23, 23, 38, 28, 45, 24…\n$ percentage_of_student_right <dbl> 49.27536, 50.72464, 49.27536, 50.72464, 42…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n    # Calculate percentages for high mastery and low mastery students\n    high_mastery_percent <- filtered_never_correct %>%\n      mutate(high_mastery = number_of_students / nrow(high_mastery_students) * 100) %>%\n      select (title_ID, high_mastery)\n\n    low_mastery_percent <- bottom_percentage_never_correct %>%\n      mutate(low_mastery = no_of_students / nrow(low_mastery_students) * 100) %>%\n      select (title_ID, low_mastery)\n    \n    comparison_data <- high_mastery_percent %>%\n      left_join(low_mastery_percent, by = \"title_ID\") %>%\n      mutate(diff = round(low_mastery - high_mastery),digits = 2) %>%\n      pivot_longer (cols = c(low_mastery,high_mastery)) %>%\n      rename (type_of_student = name,\n              percentage = value)\n   \n    low_mastery <-  comparison_data %>%\n      filter(type_of_student == \"low_mastery\")\n    \n    high_mastery <-  comparison_data %>%\n      filter(type_of_student == \"high_mastery\")\n    \n    stats <- comparison_data %>%\n      group_by(type_of_student) %>%\n      summarise(mean = mean(percentage),\n                SE = sd(percentage)) %>%\n      mutate(meanpos = mean + 1 *SE,\n             meanneg = mean - 1 *SE)\n    \n    stats_low_mastery <- stats %>%\n      filter(type_of_student == \"low_mastery\")\n    stats_high_mastery <- stats %>%\n      filter(type_of_student == \"high_mastery\")\n    \n    diff <- comparison_data %>% \n      filter(type_of_student == \"low_mastery\") %>%\n      mutate(x_pos = percentage + (-diff/2))\n    \n    comparison_spread <- comparison_data %>%\n      spread(type_of_student, percentage)\n    \n    # Join the stats to get the mean values\n    comparison_spread <- comparison_spread %>%\n      left_join(stats %>% filter(type_of_student == \"high_mastery\") %>% select(-type_of_student), by = character()) %>%\n      rename(high_mastery_mean = mean, high_mastery_SE = SE, high_mastery_meanpos = meanpos, high_mastery_meanneg = meanneg) %>%\n      left_join(stats %>% filter(type_of_student == \"low_mastery\") %>% select(-type_of_student), by = character()) %>%\n      rename(low_mastery_mean = mean, low_mastery_SE = SE, low_mastery_meanpos = meanpos, low_mastery_meanneg = meanneg)\n    \n    # Create the color condition\n    comparison_spread <- comparison_spread %>%\n      mutate(color_condition = case_when(\n        high_mastery > high_mastery_meanpos & low_mastery > low_mastery_meanpos ~ \"red\",\n        TRUE ~ \"default\"\n      ))\n    \n    # Gather the data back to long format\n    comparison_long <- comparison_spread %>%\n      gather(type_of_student, percentage, high_mastery, low_mastery) %>%\n      mutate(type_of_student = factor(type_of_student, levels = c(\"high_mastery\", \"low_mastery\"))) %>%\n      mutate (color_condition = ifelse(color_condition == \"default\", type_of_student, color_condition))\n    \n    comparison_long$color_condition <- as.factor(comparison_long$color_condition)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n      ggplot(comparison_long) +\n        geom_rect(xmin = stats_low_mastery$meanneg, xmax = stats_low_mastery$meanpos,\n                  ymin = 0, ymax = 38, fill = \"#762a83\", alpha = .05) +\n        geom_vline(xintercept = stats_low_mastery$mean, linetype = \"solid\", size = .5, alpha = .8, color = \"#762a83\")+\n        \n        geom_rect(xmin = stats_high_mastery$meanneg, xmax = stats_high_mastery$meanpos,\n                  ymin = 0, ymax = 38, fill = \"#009688\", alpha = .05)+  \n        geom_vline(xintercept = stats_high_mastery$mean, color = \"#009688\", linetype = \"solid\",  size = .5, alpha = .8) +\n        \n        \n        geom_segment(data = low_mastery,\n                     aes(x = percentage, y = title_ID,\n                         yend = high_mastery$title_ID, xend = high_mastery$percentage),\n                     color = \"#aeb6bf\",\n                     size = 4.5,\n                     alpha = 0.5) +\n        geom_point(aes(x = percentage, y = title_ID, color = color_condition), size = 4, show.legend = TRUE) +\n        #color points\n        scale_color_manual(values = c(\"1\" = \"#009688\", \"2\" = \"#762a83\", \"red\" = \"red\"))+\n        #add annotations for mean and standard deviations\n        geom_text(x = stats_low_mastery$mean + 5, y = 38, label = \"MEAN\", angle = 90, size = 2.5, color = \"#009688\")+\n        geom_text(x = stats_low_mastery$meanpos + 5, y = 38, label = \"STDEV\", angle = 90, size = 2.5, color = \"#009688\")+\n        ggtitle(\"Comparison of High Mastery Student Never Correct Percentage vs Low Mastery Student Correct Percentage\") +\n        geom_text (data = diff,\n                   aes(label = paste(\"D:\", diff, \"%\"), x = x_pos, y = title_ID),\n                   color = \"#4a4e4d\",\n                   size = 2.5) +\n        facet_grid(title_ID ~ ., scales = \"free\", switch = \"y\") +\n        theme_minimal()+\n        theme(panel.grid.major.y = element_blank(),\n              panel.grid.minor.y = element_blank(),\n              panel.grid.major.x = element_blank(),\n              panel.grid.minor.x = element_blank(),\n              axis.title.y = element_blank(),\n              axis.text.y = element_blank(),\n              axis.ticks.y = element_blank(),\n              axis.ticks.x = element_line(color = \"#4a4e4d\"),\n              text = element_text(family = \"Segoe UI Semibold\", color = \"#4a4e4d\"),\n              strip.text.y.left  = element_text(angle = 0),\n              panel.background = element_rect(fill = \"white\", color = \"white\"),\n              strip.background = element_rect(fill = \"white\", color = \"white\"),\n              strip.text = element_text(color = \"#4a4e4d\", family = \"Segoe UI\"),\n              plot.background = element_rect(fill = \"white\", color = \"white\"),\n              panel.spacing = unit(0, \"lines\"),\n              plot.margin = margin(1,1,.5,1, \"cm\"))\n```\n\n::: {.cell-output-display}\n![](Task4_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Lollipop Chart Analysis\n\nOur subsequent analysis involved a lollipop chart to identify the error rates among high mastery students across all assessed questions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npercentage_error <- percentage_error %>%\n  filter (student_ID %in% high_mastery_students$student_ID) %>%\n  group_by(title_ID) %>%\n  summarise (avg_percentage_wrong = mean(percentage_wrong))\n\npercentile_75 <- quantile(percentage_error$avg_percentage_wrong, 0.75)\n\npercentage_error <- percentage_error %>%\n  mutate(highlight = ifelse(avg_percentage_wrong > percentile_75, \"Above 75 Percentile\", \"Below 75 Percentile\"))\n\nggplot(percentage_error, aes(x = reorder(title_ID,avg_percentage_wrong), y = avg_percentage_wrong, color = highlight)) +\n  geom_segment(aes(x = title_ID, xend = title_ID, y = 0, yend = avg_percentage_wrong), color = \"grey\") +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"Above 75 Percentile\" = \"red\", \"Below 75 Percentile\" = \"blue\"), name = \"Error Quantile\") +\n  labs(\n    title = \"Average Error Rate per Question for High Mastery Students\",\n    x = \"Title ID\",\n    y = \"Percentage of Error Rate\"\n    ) +\n  theme_light() +\n  coord_flip() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.border = element_blank(),\n    axis.ticks.y = element_blank(),    \n    axis.title.y = element_text(size = 14, color = \"black\"),\n    axis.title.x = element_text(size = 14, color = \"black\"),\n    plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\", color = \"black\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 10),\n    legend.position = \"bottom\",\n    panel.spacing.x = unit(2, \"cm\")\n    )\n```\n\n::: {.cell-output-display}\n![](Task4_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\nBy focusing on the 75th percentile of error rates, we identified several questions that frequently tripped up otherwise proficient students. Notable examples include Question_5fgqjSBwTPG7KUV3it6O and Question_YWXHr4G6Cl7bEm9iF2kQ, which consistently demonstrated higher error rates, indicating potential misalignment with the expected competencies of high mastery students. These findings highlight the need for a reassessment of these questions to ensure they accurately measure and reflect true student performance.\n\n\n### Information Characteristic Curve (ICC) Analysis\n\nFurther insights were gleaned from an Information Characteristic Curve (ICC) analysis using a two-parameter Item Response Theory model to determine the degree by which questions will differentiate in performance between high and low knowledge level students.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent_question <- merged_data %>%\n  group_by(student_ID, title_ID) %>%\n  summarise (count = n()) %>%\n  ungroup()\n\nstudent_question <- student_question %>%\n  group_by(student_ID) %>%\n  summarise (count = n()) %>%\n  filter (count >1) %>%\n  ungroup()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_attempt_data <- merged_data %>%\n  group_by(student_ID, title_ID) %>%\n  slice(1) %>%\n  mutate (true_points = case_when(\n      state == \"Absolutely_Correct\" ~ 1,\n      TRUE ~ 0\n    )) %>%\n  select (student_ID, title_ID, true_points)\n\nfirst_attempt_long <- pivot_wider(first_attempt_data, \n                                   names_from = title_ID, values_from = true_points) %>%\n  filter (student_ID %in% student_question$student_ID)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary (mirt)\nfirst_attempt_long <- first_attempt_long %>%\n  mutate_all(~ replace_na(., 0))\n\nsaveRDS(first_attempt_long, \"mirtdata.RDS\")  \n\n\nfirst_attempt_long_high <- first_attempt_long %>%\n  filter(student_ID %in% high_mastery_students$student_ID | student_ID %in% low_mastery_students$student_ID)\n\nfit3PL <- mirt(data = first_attempt_long_high[2:39], \n               model = 1,  # alternatively, we could also just specify model = 1 in this case\n               itemtype = \"2PL\",\n               SE = TRUE,\n               verbose = FALSE)\nfit3PL\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nmirt(data = first_attempt_long_high[2:39], model = 1, itemtype = \"2PL\", \n    SE = TRUE, verbose = FALSE)\n\nFull-information item factor analysis with 1 factor(s).\nConverged within 1e-04 tolerance after 19 EM iterations.\nmirt version: 1.41 \nM-step optimizer: BFGS \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 61\nLatent density type: Gaussian \n\nInformation matrix estimated with method: Oakes\nSecond-order test: model is a possible local maximum\nCondition number of information matrix =  13.58293\n\nLog-likelihood = -2899.229\nEstimated parameters: 76 \nAIC = 5950.458\nBIC = 6171.82; SABIC = 5931.398\nG2 (1e+10) = 4462.22, p = 1\nRMSEA = 0, CFI = NaN, TLI = NaN\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_IRT <- readRDS (\"mirtdata.RDS\")\n\ndf_IRT <- df_IRT %>%\n  filter(student_ID %in% high_mastery_students$student_ID | student_ID %in% low_mastery_students$student_ID)\n    \nfit3PL <- mirt(data = df_IRT[2:39],\n               model = 1,  # alternatively, we could also just specify model = 1 in this case\n               itemtype = \"2PL\",\n               SE = TRUE,\n               verbose = FALSE)\n\nplot(fit3PL, type = 'infotrace', which.item = c(1:38), facet_items = TRUE, \n     as.table = TRUE, auto.key = list(points = FALSE, lines = TRUE, columns = 1, space = 'right', cex = .8), \n     theta_lim = c(-3, 3), \n     main = \"Item Information Curves Plot of 2PL Model\",\n     layout = c(10, ceiling(38 / 10)))\n```\n\n::: {.cell-output-display}\n![](Task4_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit3PL, type = 'trace', which.item = c(1:38), facet_items = TRUE, \n     as.table = TRUE, auto.key = list(points = FALSE, lines = TRUE, columns = 1, space = 'right', cex = .8), \n     theta_lim = c(-3, 3), \n     main = \"Category Characteristic Curves Plot of 2PL Model\",\n     layout = c(10, ceiling(38 / 10)))\n```\n\n::: {.cell-output-display}\n![](Task4_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\nThis analysis highlighted items like Question_h7pXNg80nJbw1C4kAPRm and Question_Az73sM0rHfWVKuc4X2kL, which showed shallow gradients and did not peak at optimal probabilities. This indicates a low discrimination ability among various student abilities, underscoring the ineffectiveness of these questions in differentiating between differing levels of student mastery. Coupled with information from high mastery student ICCs, which ideally should show steeper slopes, it was confirmed that even top-performing students struggled with these questions, suggesting they are poorly suited for assessing high-level competencies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "Task4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}