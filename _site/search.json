[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Our Team",
    "section": "",
    "text": "Fudi XiaoLow JixiongWang Yuhui"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual-Analytics-Fly-Like-a-G6",
    "section": "",
    "text": "NorthClass, a prominent higher education training institution, offers over 100 courses across various disciplines, attracting around 300,000 registered learners. The institution collects temporal learning data to assess instructional quality. To optimize teaching resources and improve quality, NorthClass plans to establish a Smart Education Development and Innovation Group. This group will explore the use of next-generation AI technology to cultivate innovative talents.\nAs a member of the Smart Education Development and Innovation Group, your task is to design and implement a Visual Analytics solution. This solution should help the institution intuitively perceive learners’ learning status and provide actionable suggestions for adjusting teaching strategies and course designs. Here are the tasks:\n\nAnalyze the log records of learners’ question-answering behaviors, quantitatively assess the degree of knowledge mastery based on multi-dimensional attributes such as answer scores and answer status, and identify weak links in their knowledge system. (It is recommended that participants answer this question with no more than 800 words and no more than 5 pictures)\nMine personalized learning behavior patterns based on learners’ characteristics, and design and present learners’ profiles from various perspectives, including peak answering hours, preferred question types, correct answering rates, etc. (It is recommended that participants answer this question with no more than 800 words and 5 pictures)\nDifferent learning modes directly impact learners’ ability to absorb, integrate, and apply knowledge. Efficient learning modes can enhance deep understanding and long-term memory retention of knowledge. Please model the potential relationship between learning modes and knowledge acquisition, present the results in the form of a graph, and provide a brief analysis. (It is recommended that participants answer this question with no more than 800 words and 5 pictures)\nThe difficulty level of questions should align with the learner’s level of knowledge. When a learner possesses a high level of knowledge but achieves a low percentage of correct answers, it indicates that the question’s difficulty exceeds their ability. Please utilize Visual Analytics to identify these inappropriate questions. (It is recommended that participants answer this question with no more than 800 words and no more than 5 pictures)\nBased on the outcomes of the aforementioned analysis, it is crucial to offer valuable recommendations for topic designers and course managers to optimize question bank content settings and enhance the quality of teaching and learning. Please briefly explain the rationale behind these suggestions. (It is recommended that participants answer this question with no more than 800 words and 3 pictures)"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html",
    "title": "Take-home Ex3",
    "section": "",
    "text": "This take-home exercise is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. The primary focus is to support the institution’s endeavor to analyze and visualise learners’ knowledge mastery levels, monitor the patterns and trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\n\n\nTo address the above, the key objective of this exercise is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#introduction",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#introduction",
    "title": "Take-home Ex3",
    "section": "",
    "text": "This take-home exercise is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. The primary focus is to support the institution’s endeavor to analyze and visualise learners’ knowledge mastery levels, monitor the patterns and trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\n\n\nTo address the above, the key objective of this exercise is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#getting-started",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#getting-started",
    "title": "Take-home Ex3",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Required R Package Libraries\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\nggstatplot: provides alternative statistical inference methods by default as an extension of the ggplot2 package\nplotly: R library for plotting interactive statistical graphs.\nrjson: Methods for Cluster analysis.\nvisNetwork: Extract and Visualize the Results of Multivariate Data Analyses.\nBiocManager: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\nigraph: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\ncluster\nfactoextra\nstats\nhms\ncaret\nggfortify\ngridExtra\nGGally\nparallelPlot\nseriation\ndendextend\nheatmaply\ncorrplot\nggalluvial\nentropy\nineq\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial, entropy, ineq) \n\n\n\nImporting the Data\nThe data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project\n\nThe code chunk below imports the dataset into R environment by using read_csv() function of readr, which is part of the tidyverse package.\n\ndf_StudentInfo &lt;- read_csv(\"data/Data_StudentInfo.csv\")\n\n\ndf_TitleInfo &lt;- read_csv(\"data/Data_TitleInfo.csv\")\n\n\ncsv_file_list &lt;- dir('data/Data_SubmitRecord')\ncsv_file_list &lt;- paste0(\"./data/Data_SubmitRecord/\",csv_file_list)\n\ndf_StudentRecord &lt;- NULL\nfor (file in csv_file_list) { # for every file...\n  file &lt;- read_csv(file)\n    df_StudentRecord &lt;- rbind(df_StudentRecord, file) # then stick together by rows\n}"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#data-preparation",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#data-preparation",
    "title": "Take-home Ex3",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nData Cleaning\nBefore data transformation, the cleanliness of the data set is first ascertained by checking for missing and duplicate data.\n\nMissing Data\ncolSums() and is.NA() functions are used to search for missing values as a whole for the 3 data sets in the code chunks as follows.\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentInfo))\n\n     index student_ID        sex        age      major \n         0          0          0          0          0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_TitleInfo))\n\n        index      title_ID         score     knowledge sub_knowledge \n            0             0             0             0             0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentRecord))\n\n      index       class        time       state       score    title_ID \n          0           0           0           0           0           0 \n     method      memory timeconsume  student_ID \n          0           0           0           0 \n\n\nFrom the outputs above, none of the variables contain missing values.\n\n\nCheck for duplicate rows\nUsing duplicated(), duplicate rows in each of the 3 data sets are identified and extracted in the following code chunks.\n\ndf_StudentInfo[duplicated(df_StudentInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\n\ndf_TitleInfo[duplicated(df_TitleInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, title_ID &lt;chr&gt;, score &lt;dbl&gt;, knowledge &lt;chr&gt;,\n#   sub_knowledge &lt;chr&gt;\n\n\n\ndf_StudentRecord[duplicated(df_StudentRecord), ]\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nFrom the outputs above, there were no duplicate rows found.\n\n\n\nData Wrangling for Inconsistencies\nTo get a better understanding of the variables in the original dataset, the glimpse() function is used in the following code chunks.\n\nglimpse(df_StudentInfo)\n\nRows: 1,364\nColumns: 5\n$ index      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ student_ID &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"63eef37311aaac915a45\", \"5d89810b20…\n$ sex        &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"male\", \"male\", \"ma…\n$ age        &lt;dbl&gt; 24, 21, 23, 21, 22, 19, 21, 18, 21, 24, 23, 20, 18, 18, 23,…\n$ major      &lt;chr&gt; \"J23517\", \"J87654\", \"J87654\", \"J78901\", \"J40192\", \"J57489\",…\n\n\n\nglimpse(df_TitleInfo)\n\nRows: 44\nColumns: 5\n$ index         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title_ID      &lt;chr&gt; \"Question_VgKw8PjY1FR6cm2QI9XW\", \"Question_q7OpB2zCMmW9w…\n$ score         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ knowledge     &lt;chr&gt; \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"t…\n$ sub_knowledge &lt;chr&gt; \"r8S3g_l0p5viby\", \"r8S3g_n0m9rsw4\", \"r8S3g_l0p5viby\", \"r…\n\n\n\nglimpse(df_StudentRecord)\n\nRows: 232,818\nColumns: 10\n$ index       &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n\n\n\nIdentifying Other Unexpected Duplicate Values\nConsidering intuitively unique values for certain variables or dependent variables, other forms of duplicates are also identified and cleaned where relevant.\n\nDuplicate student_ID in StudentInfo\n\n\n# Find the duplicated student_IDs\nduplicates &lt;- df_StudentInfo[duplicated(df_StudentInfo$student_ID) | duplicated(df_StudentInfo$student_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate student_IDs\nduplicates\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\nFrom the output above, no duplicates found.\n\nDuplicate title_ID (aka questions) in TitleInfo\n\n\n# Find the duplicated title_IDs\nduplicates &lt;- df_TitleInfo[duplicated(df_TitleInfo$title_ID) | duplicated(df_TitleInfo$title_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate title_IDs\nduplicates\n\n# A tibble: 12 × 5\n   index title_ID                      score knowledge sub_knowledge \n   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n 1     2 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_n0m9rsw4\n 2     3 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_l0p5viby\n 3    21 Question_QRm48lXxzdP7Tn1WgNOf     3 y9W5d     y9W5d_c0w4mj5h\n 4    22 Question_QRm48lXxzdP7Tn1WgNOf     3 m3D1v     m3D1v_r1d7fr3l\n 5    23 Question_pVKXjZn0BkSwYcsa7C31     3 y9W5d     y9W5d_c0w4mj5h\n 6    24 Question_pVKXjZn0BkSwYcsa7C31     3 m3D1v     m3D1v_r1d7fr3l\n 7    26 Question_lU2wvHSZq7m43xiVroBc     3 y9W5d     y9W5d_c0w4mj5h\n 8    27 Question_lU2wvHSZq7m43xiVroBc     3 k4W1c     k4W1c_h5r6nux7\n 9    30 Question_x2Fy7rZ3SwYl9jMQkpOD     3 y9W5d     y9W5d_c0w4mj5h\n10    31 Question_x2Fy7rZ3SwYl9jMQkpOD     3 s8Y2f     s8Y2f_v4x8by9j\n11    36 Question_oCjnFLbIs4Uxwek9rBpu     3 g7R2j     g7R2j_e0v1yls8\n12    37 Question_oCjnFLbIs4Uxwek9rBpu     3 m3D1v     m3D1v_r1d7fr3l\n\n\n\nunique(duplicates$knowledge)\n\n[1] \"r8S3g\" \"y9W5d\" \"m3D1v\" \"k4W1c\" \"s8Y2f\" \"g7R2j\"\n\nunique(duplicates$sub_knowledge)\n\n[1] \"r8S3g_n0m9rsw4\" \"r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h\" \"m3D1v_r1d7fr3l\"\n[5] \"k4W1c_h5r6nux7\" \"s8Y2f_v4x8by9j\" \"g7R2j_e0v1yls8\"\n\n\nFrom the outputs above, some questions (title_ID) belong to up to 2 knowledge areas or 2 sub-knowledge areas, where the scores for the former are consistently 3, and for the latter, 1. This overlap in title_ID affects 6 title_IDs, spreads across 6 knowledge areas and 7 sub-knowledge areas.\nThe unique values for knowledge and sub-knowledge areas are obtained in the following code chunk to better understand the complexity of these 2 variables.\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nBased on the output above, there is a total of 8 knowledge areas and 15 sub-knowledge areas. This suggests that majority of the knowledge areas and approximately half of sub-knowledge areas have overlapping title_ID. From the nomenclature, each sub-knowledge area is tagged to only 1 knowledge area.\nTo meaningfully analyse the relationship between knowledge areas & sub knowledge areas and other variables, additional columns are introduced where the values in these 2 columns are transposed as column labels with binary values to indicate the tagging of each question to that value. This is done in the following code chunk.\n\n# Transpose the knowledge column to create new columns for each unique value\ndf_TitleInfo1 &lt;- df_TitleInfo %&gt;%\n  mutate(knowledge_presence = 1) %&gt;%\n  spread(key = knowledge, value = knowledge_presence, fill = 0)\n\n# Transpose the sub_knowledge column to create new columns for each unique value\ndf_TitleInfo2 &lt;- df_TitleInfo %&gt;%\n  mutate(sub_knowledge_presence = 1) %&gt;%\n  spread(key = sub_knowledge, value = sub_knowledge_presence, fill = 0)\n\n# Combine the new columns with the original dataframe\ndf_TitleInfo3 &lt;- df_TitleInfo2 %&gt;%\n  distinct(index, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo1, by = \"index\") %&gt;%\n  distinct(index, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo, by = \"index\")\n  \n\n# Reassign values to the knowledge & sub_knowledge columns for repeated title_ID rows\ndf_TitleInfo_gp &lt;- df_TitleInfo3 %&gt;%\n  group_by(title_ID) %&gt;%\n  summarise(across(where(is.numeric), max, na.rm = TRUE),\n            knowledge = paste(unique(knowledge.x), collapse = \"_\"),\n            sub_knowledge = paste(unique(sub_knowledge.x), collapse = \"_\")) %&gt;%\n  select(-score.y,\n         -score.x,\n         -index)\n\nglimpse(df_TitleInfo_gp)\n\nRows: 38\nColumns: 27\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3oPyUzDmQtcM…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ score          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"m3D1v\", \"g7R2j\", \"y9W5d\", \"m3D1v\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"m3D1v_r1d7fr3l\", \"…\n\nunique(df_TitleInfo_gp$knowledge)\n\n [1] \"t5V9e\"       \"m3D1v\"       \"g7R2j\"       \"y9W5d\"       \"r8S3g\"      \n [6] \"b3C9s\"       \"y9W5d_m3D1v\" \"y9W5d_k4W1c\" \"g7R2j_m3D1v\" \"y9W5d_s8Y2f\"\n\nunique(df_TitleInfo_gp$sub_knowledge)\n\n [1] \"t5V9e_e1k6cixp\"                \"m3D1v_r1d7fr3l\"               \n [3] \"g7R2j_e0v1yls8\"                \"y9W5d_c0w4mj5h\"               \n [5] \"m3D1v_v3d9is1x\"                \"y9W5d_p8g6dgtv\"               \n [7] \"r8S3g_n0m9rsw4\"                \"y9W5d_e2j7p95s\"               \n [9] \"b3C9s_j0v1yls8\"                \"m3D1v_t0v5ts9h\"               \n[11] \"y9W5d_c0w4mj5h_m3D1v_r1d7fr3l\" \"r8S3g_l0p5viby\"               \n[13] \"g7R2j_j1g8gd3v\"                \"b3C9s_l4z6od7y\"               \n[15] \"y9W5d_c0w4mj5h_k4W1c_h5r6nux7\" \"g7R2j_e0v1yls8_m3D1v_r1d7fr3l\"\n[17] \"r8S3g_n0m9rsw4_r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h_s8Y2f_v4x8by9j\"\n\n\n\nDuplicate class for each Individual Students in StudentRecord\n\n\n# Identify students with multiple classes\nstudents_multiple_classes &lt;- df_StudentRecord %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(unique_classes = n_distinct(class)) %&gt;%\n  filter(unique_classes &gt; 1)\n\nstudents_multiple_classes_entries &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  arrange(desc(student_ID))\n\n# Display the results\nprint(students_multiple_classes_entries)\n\n# A tibble: 12 × 3\n# Groups:   student_ID [6]\n   student_ID           class   count\n   &lt;chr&gt;                &lt;chr&gt;   &lt;int&gt;\n 1 r9m46ndmmmzeeehft96z Class15   140\n 2 r9m46ndmmmzeeehft96z class       1\n 3 qz6jjynwbd3szlp0rj04 Class1    136\n 4 qz6jjynwbd3szlp0rj04 class       1\n 5 nd9xpohv0s4ttw0o7fts Class8    143\n 6 nd9xpohv0s4ttw0o7fts class       1\n 7 lqm8jh0uggps7yd0lx2x Class8    132\n 8 lqm8jh0uggps7yd0lx2x class       1\n 9 isa355t9q5rut5fm8aml Class1    142\n10 isa355t9q5rut5fm8aml class       1\n11 ezdogkk0jqt4nvvvbnxp Class7    125\n12 ezdogkk0jqt4nvvvbnxp class       1\n\n\nBased on the output above, it is apparent that the 2nd class for each of the student above is an erroneous value. Hence this inconsistency will be cleaned in the following code chunk\n\n# Step 1: Identify the correct class for each student (the class with the highest frequency)\ncorrect_classes &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice(1) %&gt;%\n  select(student_ID, correct_class = class)\n\n# Step 2: Replace wrong class values\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  left_join(correct_classes, by = \"student_ID\") %&gt;%\n  mutate(class = ifelse(!is.na(correct_class), correct_class, class)) %&gt;%\n  select(-correct_class)\n\nFor completeness, a check is done for existence of other students with class that has no class number in the following code chunk.\n\nMissingClassNo &lt;- df_StudentRecord %&gt;%\n  filter(class == \"class\")\nMissingClassNo\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nBased on the output above, there are no further students with class without number.\n\n\nIdentifying Other Unexpected and/or Missing Values\n\nMissing Student_ID and title_ID in StudentRecord are also identified.\n\n\nmissing_students &lt;- anti_join(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Display the missing student IDs\nmissing_student_ids &lt;- missing_students %&gt;% select(student_ID) %&gt;% distinct()\nprint(missing_student_ids)\n\n# A tibble: 1 × 1\n  student_ID           \n  &lt;chr&gt;                \n1 44c7cf3881ae07f7fb3eD\n\n\n\nmissing_questions &lt;- anti_join(df_StudentRecord, df_TitleInfo, by = \"title_ID\")\n\n# Display the missing title IDs\nmissing_questions &lt;- missing_questions %&gt;% select(title_ID) %&gt;% distinct()\nprint(missing_questions)\n\n# A tibble: 0 × 1\n# ℹ 1 variable: title_ID &lt;chr&gt;\n\n\nThere is 1 missing student between either StudentRecord or StudentInfo, but no missing questions. Since there is partial missing info on this student, it isn’t meaningful to include in this analysis, hence the student_ID will be excluded in the following code chunk.\n\ndf_StudentInfo &lt;- df_StudentInfo %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\n\n\nOther unexpected values\n\nThe unique values for each column is queried to check for unexpected values in the following code chunk, wherein Index, time, class, title_ID and student_ID are excluded since they will be dealt with separately\n\nunique(df_StudentRecord$state)\n\n [1] \"Absolutely_Correct\" \"Error1\"             \"Absolutely_Error\"  \n [4] \"Error6\"             \"Error4\"             \"Partially_Correct\" \n [7] \"Error2\"             \"Error3\"             \"Error5\"            \n[10] \"Error7\"             \"Error8\"             \"Error9\"            \n[13] \"�������\"           \n\nunique(df_StudentRecord$score)\n\n[1] 3 4 0 1 2\n\nunique(df_StudentRecord$method)\n\n[1] \"Method_Cj9Ya2R7fZd6xs1q5mNQ\" \"Method_gj1NLb4Jn7URf9K2kQPd\"\n[3] \"Method_5Q4KoXthUuYz3bvrTDFm\" \"Method_m8vwGkEZc3TSW2xqYUoR\"\n[5] \"Method_BXr9AIsPQhwNvyGdZL57\"\n\nunique(df_StudentRecord$memory)\n\n  [1]   320   356   196   308     0   312   328   512   324   188   316   344\n [13]   444   192   332   484   360   200   340   184   476   492   180   448\n [25]   464  8544   204   496   364   460   508   456   352   480   348   488\n [37]   468   400   616   472   384   376   452   336   588   604   440   600\n [49]   580   500   640   520   436   368   612   504   736   632  8448   220\n [61]   372   208   828   256   568   576   628   756   620   700   212   592\n [73]   380   396   432   404   644   564   748   216   264   708   768   304\n [85]   420   624  8516  8644   288  8632  8640  8512   408   260   292   608\n [97]  8580   636   536   424   596   272   388   300   280   268   176   160\n[109]   296   416   240   284   248   172  8388   832  4164  4284   428   168\n[121]   572   164   276   528   392   412  8668  8500  8540  8664  8536  8576\n[133]  8628  8504  8800  8524  8392  8548   692   952  8508  8648  9664  9536\n[145]  9564 49852 59616  1332   948   824   724  2876  3024 24668 25208 26712\n[157] 23968   732 25248 22740   712  8520   720 18264   224  4984  8696 20272\n[169] 19576   516  8976  9028  9532   544   584   552   524  5624 29688   688\n[181] 30940 44020   740   556 51376 14656 65536   680 30440 30284 23128 28112\n[193]   760 15060 25660 23356 31796   804 24768 24232 12792 14720 26172 29020\n[205] 32992 28492 10568  8460  8404   908   652   540  8620 34268 11348 11640\n[217] 13124   532 12608 15028  1400 32544 39612 27272 28852 29248  8452  8616\n[229]  8480  8528   560 13576  8436   548  2012 24896   232 21728 21148  4424\n[241]  7640 43512 39912 19936 12580  2412  2436 24224  4296  4332  6392 25912\n[253] 21332 20128   668 35948  2360  8612  8384  5560 26548 25532 13112 15288\n[265] 13992 49336 53216 15040 13780  8496  8424 37184  8476  8400  8408 30656\n[277]  8156  8140  8064  8136 11236  5616  8160  4192 23116 19784 22908 21176\n[289] 18276 20708 19868 16348 18716 17208 19588 14824 20780 20204 24932 21084\n[301] 24992 21884 18764 26624 24368 13240 22988  3740 43532 26084 26320 13340\n[313] 11372 46460 49464 13356  8144  8564  1720 13892 14488 10580 23576  8396\n[325] 15212 15340   872 25648 25920 27028 24356 23544  7416  6560  4852  8556\n[337] 32088 32716 44216  4292   228 33212 33736 27228 27288 11764 10540 11560\n[349] 10456 11384 10708 32932 25940 17800 16764 46908 30512  9368  9472 19156\n[361]  2348 36136  8132  4708 39048 21152 30632 27200   656   252 47096  8552\n[373]  8464 14040 36984  2384  1792  6084  5844  2456  2440 26452 27364   648\n[385]   244 23168 24324  8420 41460 40568 34316   896  1472  7156 23740  6444\n[397]  6972  6200  6060  7488  6700  6580  5184  4948  5052  5820  6120  5404\n[409]  5028  5180  5100  5068  5020  5204  5976  5176  5048  5884  5824  5828\n[421]  5060  5072  5056  6076  6328  5076  8492  8428   236  7340  6668  7492\n[433]  8412  8652 17176  6852  6616  6032 45288 50140 40348 16848 21820 20856\n[445] 26296 28128 31560 17272 17656 37548 34476 38428 30456 41624 34224 18148\n[457] 20816   128   808   156   844   728   716   696   836   676  4324   860\n[469]  1980  8812   660  8636   684  8756   704  8532  8572  1920  1972  2332\n[481]  2172  2296  2280 13908 63088 15432 15680 15624 15824 15956 15724 15292\n[493]  8796  1880  1996  1992 11256 11268 11264 29240 29144 28752 27988  6068\n[505]  1180 28536 11032 39216 35632 28600  2104  8656 36028 38432 12456 30164\n[517]  1268  1328  1316  1240 50220  4540 35888  1976  4440 14336 14384 45680\n[529] 39080 28484 39104 53732  8680  8692  8660 14136  4564  4480 28848 29112\n[541] 18856  8792  8600  8592 41404 37052 36532 37804 33084 37368 30820 50620\n[553] 26248 22264 26616 25900   752 47040 14644 40636 43128 33568 36248 33088\n[565] 28140 28084 30532 30572 48376 47640 17400 20288 28724 20216 12664 12204\n[577] 11960 27188 15700 15664  4580  4584 28036 28732 34004 33508 31808  1528\n[589]  1716 13752  9592  9520  9784  9208  8828 28716 27536 28584  1704  1620\n[601] 13096 14132 14584 57528 45500  7096  2168  2236 12984 20412 31172 29296\n[613] 54356 54336 47548 41664 41812 13624  1336  1348 13496 55524  1352  1356\n[625] 42052   744   996   984   940  1016 29012 28080 26036  7344  7232  7476\n[637]  7828 13956 43452  1456  1324  1364 43196 27964 10812   972  1340  4692\n[649] 27248 44592 44860 46576 20464 52656 52996 48964 49516  6904  6592  6584\n[661]  8672 46852 40364 14500 14712 17740 17620 52584  8488 36488 44204 44500\n[673] 42300 45228 17980 37460 28240 28988 53288 58424  9540  9524  6936  6204\n[685] 54596 28604 29528 42804 12856 13776 15720  4156 12472  8704  8688 29300\n[697] 18612 12976 32376  8776 13548 26456  1884  1752   764  4172 53316 52160\n[709] 47036 45632 53396 51320 12468 11496 53604\n\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"--\"  \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"309\" \"331\"\n [61] \"36\"  \"65\"  \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\" \n [73] \"190\" \"163\" \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\" \n [85] \"66\"  \"326\" \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\" \n [97] \"394\" \"385\" \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\"\n[109] \"55\"  \"84\"  \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"-\"   \"135\" \"103\" \"114\"\n[121] \"258\" \"254\" \"85\"  \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\"\n[133] \"246\" \"146\" \"89\"  \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\"\n[145] \"160\" \"159\" \"182\" \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"280\" \"274\" \"269\"\n[157] \"288\" \"271\" \"136\" \"276\" \"277\" \"356\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\nunique(df_StudentInfo$sex)\n\n[1] \"female\" \"male\"  \n\nunique(df_StudentInfo$age)\n\n[1] 24 21 23 22 19 18 20\n\nunique(df_StudentInfo$major)\n\n[1] \"J23517\" \"J87654\" \"J78901\" \"J40192\" \"J57489\"\n\n\n\nunique(df_TitleInfo$score)\n\n[1] 1 2 3 4\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nFrom the outputs above, there is an unexpected value for state and timeconsume in StudentRecord.\nStarting with state, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_state &lt;- df_StudentRecord %&gt;%\n  filter (state == '�������')\nOutlier_state\n\n# A tibble: 6 × 10\n  index class     time state score title_ID method memory timeconsume student_ID\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n1  6344 Class10 1.70e9 ����…     0 Questio… Metho…  65536 309         c681117f7…\n2  6346 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n3  6347 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n4 10138 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         1883af270…\n5 16420 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         hpb03ydul…\n6 16458 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         ljylby8in…\n\n\nFrom the output above, there are only 6 rows that are affected. Further cross-validation with the data description document found that there should only be 12 unique values for this variable, and including this outlier state value will give 13. Hence this is likely a wrong entry, and so it will be excluded from the analysis in the following code chunk.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (state != '�������')\n\nFor timeconsume, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_timeconsume &lt;- df_StudentRecord %&gt;%\n  filter (timeconsume %in% c('-', '--'))\nOutlier_timeconsume\n\n# A tibble: 2,612 × 10\n   index class    time state score title_ID method memory timeconsume student_ID\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1   191 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9417c1b4c…\n 2   321 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 3   322 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 4   366 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 5   396 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 6   397 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 7   422 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 8   423 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 9   424 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n10   425 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n# ℹ 2,602 more rows\n\n\nBased on the output, there is a sizable number of 2,612 rows with the unexpected value. Hence these rows will be kept in the analysis and replaced with 0 (since there is no existing values of 0 too), however subsequent analysis in this exercise involving the timeconsume variable will note these values as missing values. This is done in the following code chunk\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = ifelse(timeconsume %in% c(\"-\", \"--\"), 0, timeconsume))\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"0\"   \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"36\"  \"65\" \n [61] \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\"  \"190\" \"163\"\n [73] \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\"  \"66\"  \"326\"\n [85] \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\"  \"394\" \"385\"\n [97] \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\" \"55\"  \"84\" \n[109] \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"135\" \"103\" \"114\" \"258\" \"254\" \"85\" \n[121] \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\" \"246\" \"146\" \"89\" \n[133] \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\" \"160\" \"159\" \"182\"\n[145] \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"331\" \"280\" \"274\" \"269\" \"288\" \"271\"\n[157] \"136\" \"276\" \"277\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\n\nRemoving Index Col\nEach data set contains an index column, which is possibly to keep track of the original order and the total number of rows. This is no longer required and relevant in the analysis, hence it will be excluded.\n\n#remove index column\ndf_StudentRecord &lt;- df_StudentRecord %&gt;% select(-1)\ndf_TitleInfo &lt;- df_TitleInfo %&gt;% select(-1)\ndf_StudentInfo &lt;- df_StudentInfo %&gt;% select(-1)\n\n\n\nCorrecting Data Types\nBased on the glimpse() function, the time variable of the StudentRecord is currently in numerical format. This will be corrected to date time format with the following steps.\nStep 1: From the data description document, the data collection period spans 148 days from 31/8/2023 to 25/1/2024, and the time variable of the StudentRecord in this data set is in seconds. This is compared against the min and max values of the time variable converted to days and deducted from the given start and end date of the collection period given, in the following code chunk.\n\n# Get the min and max values of the time column\nmin_time &lt;- min(df_StudentRecord$time, na.rm = TRUE)\nmax_time &lt;- max(df_StudentRecord$time, na.rm = TRUE)\n\n# Display the min & max values\ndate_adjustment1 &lt;- as.numeric(as.Date(\"2023-08-31\")) - (min_time / 24 / 60 / 60)\ndate_adjustment2 &lt;- as.numeric(as.Date(\"2024-01-25\")) - (max_time / 24 / 60 / 60)\ndate_adjustmentavg &lt;- as.Date((date_adjustment1 + date_adjustment2)/2, origin = \"1970-01-01\")\ndate_adjustmentavg\n\n[1] \"1969-12-31\"\n\n\nStep 2: Apply date_adjustmentavg to the time variable to amend the data type to date time format in the folloiwing code chunk\n\n# Convert time from timestamp to POSIXct\ndf_StudentRecord$time_change &lt;- as.POSIXct(df_StudentRecord$time, origin=date_adjustmentavg, tz=\"UTC\")\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\nFurther, the timeconsume variable will be converted to numeric, wherein since the ‘-’ and ‘–’ values found earlier had taken the value of 0, there will not be an issue of NA values affecting subsequent analysis.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = as.numeric(timeconsume))\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;dbl&gt; 3, 3, 2, 2, 3, 5, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 5…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\n\n\n\nCreate Merged Dataset\nTo prepare for cross-dataset visualisation and analysis of variables, the 3 data sets are joined on title_id and student_id variables in the following code chunks.\n\n# Merge StudentInfo with SubmitRecord based on student_ID\nmerged_data &lt;- merge(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Merge TitleInfo with the already merged data based on title_ID\nmerged_data &lt;- merge(merged_data, df_TitleInfo_gp, by = \"title_ID\")\n\nmerged_data &lt;- merged_data %&gt;%\n  rename(\n    actual_score = score.x,\n    question_score = score.y\n  )\n\n\nsaveRDS(merged_data, \"merged_data_df.rds\")\n\n\nglimpse (merged_data)\n\nRows: 232,811\nColumns: 39\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8EK…\n$ student_ID     &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b2292…\n$ class          &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"Cla…\n$ time           &lt;dbl&gt; 1696330917, 1699625054, 1697444103, 1695964704, 1697727…\n$ state          &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"Pa…\n$ actual_score   &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1…\n$ method         &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvyGd…\n$ memory         &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320, 3…\n$ timeconsume    &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3, 3…\n$ time_change    &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16 0…\n$ sex            &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"ma…\n$ age            &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 21,…\n$ major          &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J401…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ question_score &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"…"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#learner-modes",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#learner-modes",
    "title": "Take-home Ex3",
    "section": "Learner modes",
    "text": "Learner modes\n\nFeature engineering\nSplitting Date and time up from the earlier created time_change date-time variable with the following code chunk\n\nmerged_data &lt;- merged_data %&gt;%\n  mutate(\n    date = as.Date(time_change),\n    time = as_hms(format(time_change, \"%H:%M:%S\"))\n)\nglimpse(merged_data)\n\nRows: 232,811\nColumns: 40\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8EK…\n$ student_ID     &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b2292…\n$ class          &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"Cla…\n$ time           &lt;time&gt; 04:09:22, 07:11:39, 01:22:28, 22:25:49, 08:11:04, 02:4…\n$ state          &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"Pa…\n$ actual_score   &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1…\n$ method         &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvyGd…\n$ memory         &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320, 3…\n$ timeconsume    &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3, 3…\n$ time_change    &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16 0…\n$ sex            &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"ma…\n$ age            &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 21,…\n$ major          &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J401…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ question_score &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ date           &lt;date&gt; 2023-10-03, 2023-11-10, 2023-10-16, 2023-09-28, 2023-1…\n\n\n\n\nDetermine number of cluster for KMeans\n\nSilhouette analysis\n\n# Function to compute silhouette width for different numbers of clusters\nsilhouette_analysis &lt;- function(merged_data, max_clusters) {\n  # Convert character columns to factors\n  merged_data &lt;- merged_data %&gt;%\n    mutate_if(is.character, as.factor)\n  \n    # Compute Gower distance matrix\n  gower_dist &lt;- daisy(merged_data, metric = \"gower\")\n  \n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform clustering using PAM\n    pam_result &lt;- pam(gower_dist, diss = TRUE, k = k)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(pam_result$clustering, gower_dist)\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Sample data (replace this with your actual data)\nset.seed(123)\nmerged_data &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100),\n  category = sample(LETTERS[1:3], 100, replace = TRUE) # Categorical column\n)\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(merged_data, max_clusters)\n\n# Plot the average silhouette widths\nplot(2:max_clusters, avg_sil_widths[2:max_clusters], type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\nSSE-Elbow method\n\n# Function to one-hot encode categorical variables\none_hot_encode &lt;- function(df) {\n  # Convert character columns to factors\n  df &lt;- df %&gt;%\n    mutate_if(is.character, as.factor)\n  \n  # Apply one-hot encoding\n  dummies &lt;- dummyVars(~ ., data = df)\n  df_encoded &lt;- data.frame(predict(dummies, newdata = df))\n  \n  return(df_encoded)\n}\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# One-hot encode the merged_data\nmerged_data_encoded &lt;- one_hot_encode(merged_data)\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(merged_data_encoded, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#learning-modes",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#learning-modes",
    "title": "Take-home Ex3",
    "section": "Learning modes",
    "text": "Learning modes\nBased on the given data, the relevant features that best defines a learner’s learning mode is assessed to be as follows:\n\nPeak answering hours determined by (a) day of the week and (b) time of the day\nVariety of question types attempted determined by (a) total number of different questions attempted, (b) total number of different knowledge and sub knowledge areas covered,\nDepth of question types and answers determined by (a) mean question scores, (b) mean memory size of file submissions across questions\nLevel of learning effort determined by (a) total number of answering attempts, (b) average number of different answering methods used across questions, (C) total memory size of file submission\nCategorical preferences\n\n\nFeature engineering\n\nPeak answering hours Boolean Integer Variables\nSplitting Date and time up from the earlier created time_change date-time variable, and adding 2 derived variables for boolean integer values for weekday (Mon to Fri) and working hours (8am to 8pm) with the following code chunk.\n\nmerged_data_lm &lt;- merged_data %&gt;%\n  mutate(\n    date = as.Date(time_change),\n    time = as_hms(format(time_change, \"%H:%M:%S\")),\n    is_weekday = as.numeric(wday(date) %in% 2:6),  # Monday to Friday 1, else 0\n    is_working_hours = as.numeric(hour(time) &gt;= 8 & hour(time) &lt; 20)  # 8am to 8pm 1, else 0\n  )\n\nglimpse(merged_data_lm)\n\nRows: 232,811\nColumns: 42\n$ title_ID         &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8…\n$ student_ID       &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b22…\n$ class            &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"C…\n$ time             &lt;time&gt; 04:09:22, 07:11:39, 01:22:28, 22:25:49, 08:11:04, 02…\n$ state            &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"…\n$ actual_score     &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1,…\n$ method           &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvy…\n$ memory           &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320,…\n$ timeconsume      &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3,…\n$ time_change      &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16…\n$ sex              &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"…\n$ age              &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 2…\n$ major            &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J4…\n$ b3C9s_j0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ b3C9s_l4z6od7y   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_e0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_j1g8gd3v   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c_h5r6nux7   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_r1d7fr3l   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_t0v5ts9h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_v3d9is1x   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_l0p5viby   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_n0m9rsw4   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f_v4x8by9j   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e_e1k6cixp   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d_c0w4mj5h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_e2j7p95s   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_p8g6dgtv   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ b3C9s            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ question_score   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ knowledge        &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\",…\n$ sub_knowledge    &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\",…\n$ date             &lt;date&gt; 2023-10-03, 2023-11-10, 2023-10-16, 2023-09-28, 2023…\n$ is_weekday       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ is_working_hours &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nGroup By Student ID\nThe following variables will be obtained with the code chunk below in preparation for clustering analysis\n\nPeak answering hours\n\n\npercentage of answers on weekdays,\npercentage of answers during working hours\n\n\nVariety of question types attempted\n\n\ntotal number of different questions attempted,\ntotal number of different knowledge and sub knowledge areas covered,\n\n\nDepth of question types\n\n\nmean question scores,\nmean memory size of file submissions across questions\nmean time consume across questions\n\n\nLevel of learning effort in answers submitted\n\n\ntotal number of answering attempts,\naverage number of different answering methods used across questions,\ntotal memory size of file submission\ntotal time consume for answers submitted\n\n\nStudentLM_data &lt;- merged_data_lm %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    `Percent of submissions on weekdays` = sum(is_weekday, na.rm = TRUE) / n() * 100,\n    `Percent of submissions during working hrs` = sum(is_working_hours, na.rm = TRUE) / n() * 100,\n    `Total no. of different qns_attempted` = n_distinct(title_ID, na.rm = TRUE),\n    `Gini Index for qns in submission` = Gini(table(title_ID)),\n    `Span of different knowledge in qns` = sum(colSums(across(29:36, as.numeric)) &gt; 0),\n    `Span of different sub knowledge in qns` = sum(colSums(across(14:28, as.numeric)) &gt; 0),\n    `Mean selected question scores` = mean(question_score, na.rm = TRUE),\n    `Mean submission memory size by qns` = mean(sapply(split(memory, title_ID), mean, na.rm = TRUE), na.rm = TRUE),\n    `Mean timeconsume by qns` = mean(sapply(split(timeconsume, title_ID), mean, na.rm = TRUE), na.rm = TRUE),\n    `Total no. of submissions` = n(),\n    `Mean no. of different answering methods per qns` = mean(sapply(split(method, title_ID), function(x) n_distinct(x, na.rm = TRUE)), na.rm = TRUE),\n    `Gini index for answering methods used per qns` = Gini(table(method)),\n    `Total memory size of submissions` = sum(memory, na.rm = TRUE),\n    `Total timeconsume of submissions` = sum(timeconsume, na.rm = TRUE)\n  )\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 15\n$ student_ID                                        &lt;chr&gt; \"0088dc183f73c83f763…\n$ `Percent of submissions on weekdays`              &lt;dbl&gt; 94.88372, 88.75000, …\n$ `Percent of submissions during working hrs`       &lt;dbl&gt; 14.883721, 10.416667…\n$ `Total no. of different qns_attempted`            &lt;int&gt; 38, 38, 38, 38, 38, …\n$ `Gini Index for qns in submission`                &lt;dbl&gt; 0.47380661, 0.395175…\n$ `Span of different knowledge in qns`              &lt;int&gt; 8, 8, 8, 8, 8, 8, 8,…\n$ `Span of different sub knowledge in qns`          &lt;int&gt; 15, 15, 15, 15, 15, …\n$ `Mean selected question scores`                   &lt;dbl&gt; 2.339535, 2.266667, …\n$ `Mean submission memory size by qns`              &lt;dbl&gt; 249.1707, 419.3248, …\n$ `Mean timeconsume by qns`                         &lt;dbl&gt; 3.543709, 12.390546,…\n$ `Total no. of submissions`                        &lt;int&gt; 215, 240, 478, 119, …\n$ `Mean no. of different answering methods per qns` &lt;dbl&gt; 2.815789, 3.289474, …\n$ `Gini index for answering methods used per qns`   &lt;dbl&gt; 0.04279070, 0.070000…\n$ `Total memory size of submissions`                &lt;dbl&gt; 48164, 97820, 136496…\n$ `Total timeconsume of submissions`                &lt;dbl&gt; 802, 2583, 9814, 415…\n\n\n\n\nUnivariate Analysis of features\n\n# Define the function to create combined box plot and histogram\ncreate_combined_plot &lt;- function(data, variable) {\n  ggplot(data, aes_string(x = paste0(\"`\", variable, \"`\"))) +\n    # Histogram\n    geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    geom_density(alpha = 0.3, fill = \"orange\") +\n    # Box plot\n    geom_boxplot(aes(y = 0), width = 0.1, color = \"red\", position = position_nudge(y = -0.1)) +\n    theme_minimal() +\n    labs(x = variable, y = \"Density\") +\n    ggtitle(paste(\"Combined Histogram and Box Plot for\", variable))\n}\n\n\n# Variables to plot\nvariables &lt;- names(StudentLM_data)[2:15]\n\n# Create combined plots for each variable\nplots &lt;- lapply(variables, function(var) create_combined_plot(StudentLM_data, var))\n\n# Display the plots\nfor (p in plots) {\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck for high colinearity\n\nSLM.cor &lt;- cor(StudentLM_data[, 2:15])\n\ncorrplot(SLM.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\nggstatsplot::ggcorrmat(\n  data = StudentLM_data, \n  cor.vars = 2:15)\n\n\n\n\n\n\nRemoving highly skewed and correlated columns\nBased on the output from the univariate and correlation analysis, 2 variables were found to be highly skewed and concentrated within 1 or 2 values, hence they are removed for more meaningful analysis, with the following code chunk. For high correlation with a threshold of &gt;0.8, 3 variables were found to be highly correlated, of which, 2 have been removed as highly skewed, leaving total_different_questions_attempted in the data frame.\n\nStudentLM_data &lt;- StudentLM_data %&gt;%\n  select(-`Span of different knowledge in qns`, \n#         -`Total timeconsume of submissions`, \n#         -`Total memory size of submissions`, \n#         -`Total no. of submissions`, \n#         -`Mean no. of different answering methods per qns`, \n#         -`Gini index for answering methods used per qns`, \n#         -`Total no. of different qns_attempted`, \n         -`Span of different sub knowledge in qns`)\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 13\n$ student_ID                                        &lt;chr&gt; \"0088dc183f73c83f763…\n$ `Percent of submissions on weekdays`              &lt;dbl&gt; 94.88372, 88.75000, …\n$ `Percent of submissions during working hrs`       &lt;dbl&gt; 14.883721, 10.416667…\n$ `Total no. of different qns_attempted`            &lt;int&gt; 38, 38, 38, 38, 38, …\n$ `Gini Index for qns in submission`                &lt;dbl&gt; 0.47380661, 0.395175…\n$ `Mean selected question scores`                   &lt;dbl&gt; 2.339535, 2.266667, …\n$ `Mean submission memory size by qns`              &lt;dbl&gt; 249.1707, 419.3248, …\n$ `Mean timeconsume by qns`                         &lt;dbl&gt; 3.543709, 12.390546,…\n$ `Total no. of submissions`                        &lt;int&gt; 215, 240, 478, 119, …\n$ `Mean no. of different answering methods per qns` &lt;dbl&gt; 2.815789, 3.289474, …\n$ `Gini index for answering methods used per qns`   &lt;dbl&gt; 0.04279070, 0.070000…\n$ `Total memory size of submissions`                &lt;dbl&gt; 48164, 97820, 136496…\n$ `Total timeconsume of submissions`                &lt;dbl&gt; 802, 2583, 9814, 415…\n\n\n\n\n\nDetermine number of K-Means clusters\nTo determine the ideal number of clusters for K-means clustering on the recompiled learners’ learning mode features, a silhouette analysis and SSE elbow method are performed in the following code chunks.\n\nSilhouette analysis\n\n# Exclude non-numeric columns\nStudentLM_data_numeric &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Function to compute silhouette widths\nsilhouette_analysis &lt;- function(data, max_clusters) {\n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(kmeans_result$cluster, dist(data))\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 12\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(StudentLM_data_numeric, max_clusters)\n\n# Plot the average silhouette widths\nplot(1:max_clusters, avg_sil_widths, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\nSSE-Elbow method\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 18\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(StudentLM_data_numeric, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")\n\n\n\n\n\n\n\nK Means clustering and Visualisation\nK Means clustering is then performed on the recompiled learners’ learning mode features with the number of clusters set as 2 based on the above results, in the following code chunk\n\n# Drop the student_ID column\nclustering_data &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Standardize the data\nclustering_data_scaled &lt;- scale(clustering_data)\n\n# Perform k-means clustering\nset.seed(123)  # For reproducibility\nkmeans_result &lt;- kmeans(clustering_data_scaled, centers = 2, nstart = 25)\n\n# Add the cluster assignments to the original data\nStudentLM_data$cluster &lt;- kmeans_result$cluster\n\nThe first plot for visualisation of the K means cluster is the Principal Component Analysis (PCA) Plot, which gives an initial sensing of the separation of the clusters based on first 2 PCA components that rank the highest in distinctness amongst the features used. This is plotted with the following code chunk.\n\n# Perform PCA\npca_result &lt;- prcomp(StudentLM_data[-1], scale. = TRUE)\n\n# Get PCA scores\npca_scores &lt;- as.data.frame(predict(pca_result))\n\n# Add cluster information to PCA scores\npca_scores$cluster &lt;- factor(StudentLM_data$cluster)\n\n# Plot PCA results with cluster color coding\npca_plot &lt;- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_discrete(name = \"Cluster\") +\n  labs(x = \"Principal Component 1\", y = \"Principal Component 2\",\n       title = \"PCA Plot of Clusters\") +\n  theme_minimal()\n\n# Display the plot\npca_plot\n\n\n\n\nBased on the PCA plot, the clusters are visually clearly separated, suggesting that the clusters are distinct, especially in relation to the top 2 PCA components in the x and y-axis.\nNext to visualise the distribution of the 2 clusters across all the features used for the K Means clustering, a parallel coordinate plot is used, with the following code chunk.\n\nStudentLM_data_factor &lt;- StudentLM_data\nStudentLM_data_factor$cluster &lt;- as.character(StudentLM_data_factor$cluster)\n\nggparcoord(data = StudentLM_data_factor,\n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n  theme(\n    plot.title = element_text(size = 20),\n    axis.text.x = element_text(angle = 30, hjust = 0.8, size = 18),\n    axis.text.y = element_text(size = 18),\n    axis.title.x = element_text(size = 18),\n    axis.title.y = element_text(size = 18),\n    legend.title = element_text(size = 18),\n    legend.text = element_text(size = 18)\n    )\n\n\n\nggparcoord(data = StudentLM_data_factor,\n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Split Plot of Students' learning modes\")+\n  facet_wrap(~ cluster)+\n  theme(\n    plot.title = element_text(size = 20),\n    axis.text.x = element_text(angle = 30, hjust = 0.8, size = 18),\n    axis.text.y = element_text(size = 18),\n    axis.title.x = element_text(size = 18),\n    axis.title.y = element_text(size = 18),\n    legend.title = element_text(size = 18),\n    legend.text = element_text(size = 18)\n    )\n\n\n\n\nBased on the plot, there is varying degree of distinction in separation between the 2 clusters across different variables. The more distinct separation are in variables such as total timeconsume of answers, total memory size of answers, mean different answering methods per question, total answering attempts and question selection gini index, where cluster 2 tends to fare better in these metrics suggesting that perhaps cluster 2 may be the more hardworking learning mode among the 2.\nAn Alluvial plot is also used in the following code chunk for an alternative visualisation of the clustering, where variables are binned into 5 equally sized bins based on distribution of student_ID.\n\n# Define a function to bin numerical variables based on the distribution of student_IDs\nbin_variable_equal_ids &lt;- function(x, bins = 5) {\n  n &lt;- length(x)\n  quantile_ranks &lt;- ceiling(rank(x, ties.method = \"first\") / (n / bins))\n  as.factor(quantile_ranks)\n}\n\n# Apply the binning function to numerical columns, excluding student_ID and cluster\nStudentLM_data_binned &lt;- StudentLM_data %&gt;%\n  mutate(across(-c(student_ID, cluster), ~ bin_variable_equal_ids(., bins = 5)))\n\n# Convert data to long format\nStudentLM_data_long &lt;- StudentLM_data_binned %&gt;%\n  pivot_longer(cols = -c(student_ID, cluster), names_to = \"variable\", values_to = \"value\")\n\n# Check rows with NA values\nStudentLM_data_checkNA &lt;- StudentLM_data_long %&gt;%\n  filter(if_any(everything(), ~ is.na(.)))\n\nglimpse(StudentLM_data_checkNA)\n\nRows: 0\nColumns: 4\n$ student_ID &lt;chr&gt; \n$ cluster    &lt;int&gt; \n$ variable   &lt;chr&gt; \n$ value      &lt;fct&gt; \n\n\n\n# Ensure the 'cluster' variable is in discrete values (1 and 2)\nStudentLM_data_long &lt;- StudentLM_data_long %&gt;%\n  mutate(cluster = as.factor(cluster))\n\n# Ensure there are no NA values in the cluster column\nStudentLM_data_long &lt;- StudentLM_data_long %&gt;%\n  filter(!is.na(cluster))\n\n# Create the alluvial plot\nggplot(StudentLM_data_long,\n       aes(x = variable, stratum = value, alluvium = student_ID)) +\n#  geom_flow(stat = \"alluvium\", lode.guidance = \"forward\", color = \"white\") +\n  geom_alluvium(aes(fill = cluster)) +\n  geom_stratum() +\n  scale_x_discrete(limits = unique(StudentLM_data_long$variable), expand = c(0.5, 0.1)) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Learning Mode Clusters\",\n       x = \"Variables\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))        \n\n\n\n#        axis.text.y = element_text(size = 8),        plot.title = element_text(size = 12),        legend.title = element_text(size = 10),        legend.text = element_text(size = 8),        axis.title.x = element_text(size = 10),        axis.title.y = element_text(size = 10),        plot.margin = unit(c(1, 1, 1, 1), \"cm\"))\n\n\n\nHierarchical Clustering and Visualisation\nAs an alternative to K means, hierarchical clustering is also considered, and initiates with mapping the data frame into a data matrix, and thereby using the dend_expend function to determine the best clustering method.\n\nStudentLM_data1 &lt;- StudentLM_data  %&gt;%\n  select(-cluster) \n#  mutate(across(everything(), scale))\n\nrow.names(StudentLM_data1) &lt;- StudentLM_data$student_ID\n#StudentLM_data1 &lt;- select(StudentLM_data1, c(1, 2:13))\nStudentLM_data_matrix1 &lt;- data.matrix(StudentLM_data1)\n\nStudentLM_data_d1 &lt;- dist(\n  normalize(StudentLM_data_matrix1[, -c(1)]), \n  method = \"euclidean\")\ndend_expend(StudentLM_data_d1)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.2669653\n2      unknown        ward.D2 0.3637016\n3      unknown         single 0.7571795\n4      unknown       complete 0.5926958\n5      unknown        average 0.8032403\n6      unknown       mcquitty 0.6096951\n7      unknown         median 0.6870881\n8      unknown       centroid 0.7422864\n\n\nBased on the output above, the average method will be the most optimal.\nA silhoutte plot in the same approach as before is also done with the following code chunk to determine the optimal number of clusters to achieve higher distinction in cluster separation for hierarchical clustering.\n\nStudentLM_data_clust &lt;- hclust(StudentLM_data_d1, method = \"average\")\nnum_k &lt;- find_k(StudentLM_data_clust)\nplot(num_k)\n\n\n\n\nBased on the output above, 9 clusters were identified to be optimal in terms of distinction in separation.\nNow using the 2 parameters, the hierarchical clustering using an interactive heatmap for visualisation is plot with the following code chunk.\nheatmaply(normalize(StudentLM_data_matrix[, -c(1)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 10,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,          \n          main=\"Students' Learning Mode Clustering \\nDataTransformation using Normalise Method\",\n          xlab = \"Student_IDs\",\n          ylab = \"Learning Mode Features\"\n)\n\n# Cut the tree into a 9 clusters\ncluster_cut &lt;- cutree(StudentLM_data_clust, k = 9)\n\n# Add the cluster assignment to the data\nStudentLM_data1_n &lt;- normalize(StudentLM_data1)\n\nStudentLM_data1_n$cluster_hc &lt;- as.factor(cluster_cut)\n\nglimpse(StudentLM_data1_n)\n\nRows: 1,364\nColumns: 14\n$ student_ID                                        &lt;chr&gt; \"0088dc183f73c83f763…\n$ `Percent of submissions on weekdays`              &lt;dbl&gt; 0.9488372, 0.8875000…\n$ `Percent of submissions during working hrs`       &lt;dbl&gt; 0.14883721, 0.104166…\n$ `Total no. of different qns_attempted`            &lt;dbl&gt; 1.0000000, 1.0000000…\n$ `Gini Index for qns in submission`                &lt;dbl&gt; 0.66600804, 0.555479…\n$ `Mean selected question scores`                   &lt;dbl&gt; 0.6515011, 0.6160606…\n$ `Mean submission memory size by qns`              &lt;dbl&gt; 0.15476836, 0.288330…\n$ `Mean timeconsume by qns`                         &lt;dbl&gt; 0.05099268, 0.406939…\n$ `Total no. of submissions`                        &lt;dbl&gt; 0.25330132, 0.283313…\n$ `Mean no. of different answering methods per qns` &lt;dbl&gt; 0.51492537, 0.649253…\n$ `Gini index for answering methods used per qns`   &lt;dbl&gt; 0.16432955, 0.294312…\n$ `Total memory size of submissions`                &lt;dbl&gt; 0.06864675, 0.139881…\n$ `Total timeconsume of submissions`                &lt;dbl&gt; 0.043706487, 0.14211…\n$ cluster_hc                                        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n# Perform PCA\npca_result &lt;- prcomp(StudentLM_data1_n[, -c(1, 14)], scale = FALSE)\npca_df &lt;- as.data.frame(pca_result$x[, 1:2])  # Example: Extracting the first two principal components\n\n# Add cluster information to PCA scores\npca_scores$cluster_hc &lt;- factor(StudentLM_data1_n$cluster_hc)\n\n# Plot PCA with cluster_hc\nggplot(pca_df, aes(x = PC1, y = PC2, color = factor(StudentLM_data1_n$cluster_hc))) +\n  geom_point() +\n  labs(title = \"PCA Plot with Clustering\", x = \"Principal Component 1\", y = \"Principal Component 2\") +\n  theme_minimal()\n\n\n\n\n\nggparcoord(data = StudentLM_data1_n, \n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n# Define a function to bin numerical variables based on the distribution of student_IDs\nbin_variable_equal_ids &lt;- function(x, bins = 5) {\n  n &lt;- length(x)\n  quantile_ranks &lt;- ceiling(rank(x, ties.method = \"first\") / (n / bins))\n  as.factor(quantile_ranks)\n}\n\n# Apply the binning function to numerical columns, excluding student_ID and cluster\nStudentLM_data_binned1 &lt;- StudentLM_data1_n %&gt;%\n  mutate(across(-c(student_ID, cluster_hc), ~ bin_variable_equal_ids(., bins = 5)))\n\n# Convert data to long format\nStudentLM_data_long1 &lt;- StudentLM_data_binned1 %&gt;%\n  pivot_longer(cols = -c(student_ID, cluster_hc), names_to = \"variable\", values_to = \"value\")\n\n# Check rows with NA values\nStudentLM_data_checkNA &lt;- StudentLM_data_long1 %&gt;%\n  filter(if_any(everything(), ~ is.na(.)))\n\nglimpse(StudentLM_data_checkNA)\n\nRows: 0\nColumns: 4\n$ student_ID &lt;chr&gt; \n$ cluster_hc &lt;fct&gt; \n$ variable   &lt;chr&gt; \n$ value      &lt;fct&gt; \n\n\n\n# Ensure the 'cluster' variable is in discrete values (1 and 2)\nStudentLM_data_long1 &lt;- StudentLM_data_long1 %&gt;%\n  mutate(cluster_hc = as.factor(cluster_hc))\n\n# Ensure there are no NA values in the cluster column\nStudentLM_data_long1 &lt;- StudentLM_data_long1 %&gt;%\n  filter(!is.na(cluster_hc))\n\n# Create the alluvial plot\nggplot(StudentLM_data_long1,\n       aes(x = variable, stratum = value, alluvium = student_ID)) +\n#  geom_flow(stat = \"alluvium\", lode.guidance = \"forward\", color = \"white\") +\n  geom_alluvium(aes(fill = cluster_hc)) +\n  geom_stratum() +\n  scale_x_discrete(limits = unique(StudentLM_data_long$variable), expand = c(0.5, 0.1)) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Learning Mode Clusters\",\n       x = \"Variables\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n        axis.text.y = element_text(size = 8),\n        plot.title = element_text(size = 12),\n        legend.title = element_text(size = 10),\n        legend.text = element_text(size = 8),\n        axis.title.x = element_text(size = 10),\n        axis.title.y = element_text(size = 10),\n        plot.margin = unit(c(1, 1, 1, 1), \"cm\"))"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#knowledge-acquisition",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#knowledge-acquisition",
    "title": "Take-home Ex3",
    "section": "Knowledge Acquisition",
    "text": "Knowledge Acquisition\nBased on the given data, the relevant features that best defines a learner’s knowledge acquisition is assessed to be as follows:\n\nknowledge mastery determined by (a) overall sum of highest actual score for each question attempted and (b) sum of highest actual score of each question by knowledge area\ncorrect answering rate determined by (a) percentage of answers absolutely correct, (b) total number of questions with answers absolutely correct and partially correct\n\n\n\nFeature engineering\n\nGroup By Student ID\nThe following variables will be obtained with the code chunks below in preparation for visualisation and analysis of Knowledge Acquisition with respect to the various learning modes.\n\nknowledge mastery\n\n\noverall sum of highest actual score for each question attempted and\nsum of highest actual score of each question by knowledge area\n\n\ncorrect answering rate\n\n\npercentage of answers absolutely correct,\ntotal number of questions with answers absolutely correct and partially correct\n\n\nCombined Metric\n\nPoint score based on:\n\nProportion of absolutely and partially correct attempts: - absolutely correct attempts - award 1 pt - partially correct attempts - award (actual_score / question_score) - normalise attempts across questions - uses (total point / total attempts)\n\nuse more than 1 method per question - multiply by no. of methods\n\n\noverall total point score\nsum of point score by knowledge group\n\n\nStudentKA_data &lt;- merged_data %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    # Part (a): Sum of highest actual score for each question attempted\n    `Sum of overall highest submission scores` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"])\n    })),\n    \n    # Part (b): Sum of highest actual score for each knowledge area\n    `Sum of overall highest submission scores for b3C9s knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 29])\n    })),\n    `Sum of overall highest submission scores for g7R2j knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 30])\n    })),\n    `Sum of overall highest submission scores for k4W1c knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 31])\n    })),\n    `Sum of overall highest submission scores for m3D1v knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 32])\n    })),\n    `Sum of overall highest submission scores for r8S3g knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 33])\n    })),\n    `Sum of overall highest submission scores for s8Y2f knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 34])\n    })),\n    `Sum of overall highest submission scores for t5V9e knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 35])\n    })),\n    `Sum of overall highest submission scores for y9W5d knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 36])\n    })),\n    \n    # Part (c): Percentage of answers absolutely correct\n    `Percent of submissions absolutely correct` = (sum(state == \"Absolutely_Correct\") / n()) * 100,\n    \n    # Part (d): Total number of questions with answers absolutely correct and partially correct\n    `No. of questions answered fully or partially correct` = length(unique(title_ID[state %in% c(\"Partially_Correct\", \"Absolutely_Correct\")]))\n    \n  )\n\n\nglimpse(StudentKA_data)\n\nRows: 1,364\nColumns: 12\n$ student_ID                                                     &lt;chr&gt; \"0088dc…\n$ `Sum of overall highest submission scores`                     &lt;dbl&gt; 100, 10…\n$ `Sum of overall highest submission scores for b3C9s knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for g7R2j knowledge` &lt;dbl&gt; 15, 15,…\n$ `Sum of overall highest submission scores for k4W1c knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for m3D1v knowledge` &lt;dbl&gt; 36, 36,…\n$ `Sum of overall highest submission scores for r8S3g knowledge` &lt;dbl&gt; 5, 5, 5…\n$ `Sum of overall highest submission scores for s8Y2f knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for t5V9e knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for y9W5d knowledge` &lt;dbl&gt; 33, 33,…\n$ `Percent of submissions absolutely correct`                    &lt;dbl&gt; 20.9302…\n$ `No. of questions answered fully or partially correct`         &lt;int&gt; 38, 38,…\n\n\n\n# Assign points to attempts based on state\nadjusted_scores &lt;- merged_data %&gt;%\n  mutate(points = case_when(\n    state == \"Absolutely_Correct\" ~ 1,\n    state == \"Partially_Correct\" ~ actual_score / question_score,\n    TRUE ~ 0 # default case for any unexpected states\n  ))\n\n# Assign points to title_IDs per student factoring in normalisation and multiple methods used\nmastery_scores1 &lt;- adjusted_scores %&gt;%\n  group_by(student_ID, title_ID, knowledge, class) %&gt;%\n  summarise(\n    total_points = sum(points),\n    total_attempts = n(),\n    unique_methods = n_distinct(method),\n    absolutely_correct_methods = sum(points == 1)\n  ) %&gt;%\n  mutate(\n    adjusted_points = total_points / total_attempts,\n    adjusted_points = adjusted_points * ifelse(absolutely_correct_methods &gt; 0, unique_methods, 1)\n  )\n\nglimpse(mastery_scores1)\n\nRows: 50,482\nColumns: 9\nGroups: student_ID, title_ID, knowledge [50,482]\n$ student_ID                 &lt;chr&gt; \"0088dc183f73c83f763e\", \"0088dc183f73c83f76…\n$ title_ID                   &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_…\n$ knowledge                  &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"m3D1v\", \"g7R2j\", \"y9W5d\"…\n$ class                      &lt;chr&gt; \"Class2\", \"Class2\", \"Class2\", \"Class2\", \"Cl…\n$ total_points               &lt;dbl&gt; 1.000000, 1.000000, 4.666667, 7.666667, 1.0…\n$ total_attempts             &lt;int&gt; 23, 9, 7, 22, 1, 1, 2, 4, 8, 11, 1, 1, 5, 1…\n$ unique_methods             &lt;int&gt; 5, 4, 5, 5, 1, 1, 1, 2, 5, 5, 1, 1, 2, 1, 1…\n$ absolutely_correct_methods &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1…\n$ adjusted_points            &lt;dbl&gt; 0.2173913, 0.4444444, 3.3333333, 1.7424242,…\n\n\n\nunique(mastery_scores1$knowledge)\n\n [1] \"t5V9e\"       \"m3D1v\"       \"g7R2j\"       \"y9W5d\"       \"r8S3g\"      \n [6] \"b3C9s\"       \"y9W5d_m3D1v\" \"y9W5d_k4W1c\" \"g7R2j_m3D1v\" \"y9W5d_s8Y2f\"\n\n\nrename(knowledge = knowledge.x) %&gt;% select(-score, -knowledge.y)\n\n# Combine the adjusted score with knowledge-transposed titleInfo dataframe\nmastery_scores2 &lt;- df_TitleInfo_gp %&gt;%\n  distinct(title_ID, .keep_all = TRUE) %&gt;%\n  left_join(mastery_scores1, by = \"title_ID\") %&gt;%\n  rename(knowledge = knowledge.x) %&gt;%\n  select(-score,\n         -knowledge.y)\n  \nglimpse(mastery_scores2)\n\nRows: 50,482\nColumns: 33\n$ title_ID                   &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_…\n$ b3C9s_j0v1yls8             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e                      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ knowledge                  &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\"…\n$ sub_knowledge              &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_…\n$ student_ID                 &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221bb479e66…\n$ class                      &lt;chr&gt; \"Class2\", \"Class10\", \"Class14\", \"Class5\", \"…\n$ total_points               &lt;dbl&gt; 1.0, 3.0, 6.5, 3.5, 1.0, 8.5, 3.0, 13.5, 7.…\n$ total_attempts             &lt;int&gt; 23, 9, 23, 6, 11, 19, 14, 37, 15, 8, 58, 23…\n$ unique_methods             &lt;int&gt; 5, 5, 5, 3, 5, 5, 5, 5, 4, 4, 5, 5, 1, 5, 2…\n$ absolutely_correct_methods &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 4, 2, 1, 8, 3, 1, 2, 1…\n$ adjusted_points            &lt;dbl&gt; 0.2173913, 1.6666667, 1.4130435, 1.7500000,…\n\n\n\n# Summing up points for Overall and Specific Knowledge Mastery for each Student\nmastery_scores &lt;- mastery_scores2 %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    # Part (a): Sum of total points across all questions\n    `Sum of points Overall` = sum(adjusted_points),\n    \n    # Part (b): Sum of highest actual score for each knowledge area\n    `Sum of points for b3C9s knowledge` = sum(case_when(\n      b3C9s == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for g7R2j knowledge` = sum(case_when(\n      g7R2j == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for k4W1c knowledge` = sum(case_when(\n      k4W1c == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for m3D1v knowledge` = sum(case_when(\n      m3D1v == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for r8S3g knowledge` = sum(case_when(\n      r8S3g == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for s8Y2f knowledge` = sum(case_when(\n      s8Y2f == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for t5V9e knowledge` = sum(case_when(\n      t5V9e == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for y9W5d knowledge` = sum(case_when(\n      y9W5d == 1 ~ adjusted_points,\n      TRUE ~ 0\n    ))\n    \n  )\n\nglimpse(mastery_scores)\n\nRows: 1,364\nColumns: 10\n$ student_ID                          &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf0522…\n$ `Sum of points Overall`             &lt;dbl&gt; 38.75682, 35.69721, 35.28832, 42.3…\n$ `Sum of points for b3C9s knowledge` &lt;dbl&gt; 3.0000000, 2.6000000, 1.3629877, 3…\n$ `Sum of points for g7R2j knowledge` &lt;dbl&gt; 6.742424, 4.350000, 6.414352, 5.85…\n$ `Sum of points for k4W1c knowledge` &lt;dbl&gt; 1.1666667, 2.0000000, 0.6666667, 1…\n$ `Sum of points for m3D1v knowledge` &lt;dbl&gt; 14.000000, 11.457143, 13.645238, 1…\n$ `Sum of points for r8S3g knowledge` &lt;dbl&gt; 4.047009, 3.172682, 2.829832, 3.90…\n$ `Sum of points for s8Y2f knowledge` &lt;dbl&gt; 0.9523810, 1.0666667, 0.5000000, 0…\n$ `Sum of points for t5V9e knowledge` &lt;dbl&gt; 3.967391, 5.545672, 3.220736, 4.84…\n$ `Sum of points for y9W5d knowledge` &lt;dbl&gt; 11.666667, 11.871717, 11.958028, 1…\n\n\n\n# Compiling the knowledge acquisition metrics\nStudentKA_data_merged &lt;- left_join(StudentKA_data, mastery_scores, by = \"student_ID\")\n\nglimpse(StudentKA_data_merged)\n\nRows: 1,364\nColumns: 21\n$ student_ID                                                     &lt;chr&gt; \"0088dc…\n$ `Sum of overall highest submission scores`                     &lt;dbl&gt; 100, 10…\n$ `Sum of overall highest submission scores for b3C9s knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for g7R2j knowledge` &lt;dbl&gt; 15, 15,…\n$ `Sum of overall highest submission scores for k4W1c knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for m3D1v knowledge` &lt;dbl&gt; 36, 36,…\n$ `Sum of overall highest submission scores for r8S3g knowledge` &lt;dbl&gt; 5, 5, 5…\n$ `Sum of overall highest submission scores for s8Y2f knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for t5V9e knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for y9W5d knowledge` &lt;dbl&gt; 33, 33,…\n$ `Percent of submissions absolutely correct`                    &lt;dbl&gt; 20.9302…\n$ `No. of questions answered fully or partially correct`         &lt;int&gt; 38, 38,…\n$ `Sum of points Overall`                                        &lt;dbl&gt; 38.7568…\n$ `Sum of points for b3C9s knowledge`                            &lt;dbl&gt; 3.00000…\n$ `Sum of points for g7R2j knowledge`                            &lt;dbl&gt; 6.74242…\n$ `Sum of points for k4W1c knowledge`                            &lt;dbl&gt; 1.16666…\n$ `Sum of points for m3D1v knowledge`                            &lt;dbl&gt; 14.0000…\n$ `Sum of points for r8S3g knowledge`                            &lt;dbl&gt; 4.04700…\n$ `Sum of points for s8Y2f knowledge`                            &lt;dbl&gt; 0.95238…\n$ `Sum of points for t5V9e knowledge`                            &lt;dbl&gt; 3.96739…\n$ `Sum of points for y9W5d knowledge`                            &lt;dbl&gt; 11.6666…\n\n\n\n# Define the function to create combined box plot and histogram\ncreate_combined_plot &lt;- function(data, variable) {\n  ggplot(data, aes_string(x = paste0(\"`\", variable, \"`\"))) +\n    # Histogram\n    geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    geom_density(alpha = 0.3, fill = \"orange\") +\n    # Box plot\n    geom_boxplot(aes(y = 0), width = 0.1, color = \"red\", position = position_nudge(y = -0.1)) +\n    theme_minimal() +\n    labs(x = variable, y = \"Density\") +\n    ggtitle(paste(\"Combined Histogram and Box Plot for\", variable))\n}\n\n\n# Variables to plot\nvariables &lt;- names(StudentKA_data_merged)[2:21]\n\n# Create combined plots for each variable\nplots &lt;- lapply(variables, function(var) create_combined_plot(StudentKA_data_merged, var))\n\n# Display the plots\nfor (p in plots) {\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving highly skewed columns\nInspecting the data frame, 4 variables were found to be highly skewed and concentrated within a small range of values, hence they are removed for more meaningful analysis, with the following code chunk.\nStudentKA_data_merged &lt;- StudentKA_data_merged %&gt;%\n  select(-sum_highest_actual_score_k4W1c, -sum_highest_actual_score_s8Y2f, -sum_points_k4W1c, -sum_points_s8Y2f)\n\nglimpse(StudentKA_data_merged)\n\n\n\nMerging Students’ Learning Modes with Knowledge Acqusition features\nWith the both data frames prepared, they will now be merged for the next sub-task which involves comparison of learners’ knowledge acqusition in respect to learning mode, and subsequently to identify patterns and relationship\n\n# Join the two dataframes on the column student_ID\nStudentLMKA_data &lt;- left_join(StudentLM_data, StudentKA_data_merged, by = \"student_ID\")\n\nglimpse(StudentLMKA_data)\n\nRows: 1,364\nColumns: 34\n$ student_ID                                                     &lt;chr&gt; \"0088dc…\n$ `Percent of submissions on weekdays`                           &lt;dbl&gt; 94.8837…\n$ `Percent of submissions during working hrs`                    &lt;dbl&gt; 14.8837…\n$ `Total no. of different qns_attempted`                         &lt;int&gt; 38, 38,…\n$ `Gini Index for qns in submission`                             &lt;dbl&gt; 0.47380…\n$ `Mean selected question scores`                                &lt;dbl&gt; 2.33953…\n$ `Mean submission memory size by qns`                           &lt;dbl&gt; 249.170…\n$ `Mean timeconsume by qns`                                      &lt;dbl&gt; 3.54370…\n$ `Total no. of submissions`                                     &lt;int&gt; 215, 24…\n$ `Mean no. of different answering methods per qns`              &lt;dbl&gt; 2.81578…\n$ `Gini index for answering methods used per qns`                &lt;dbl&gt; 0.04279…\n$ `Total memory size of submissions`                             &lt;dbl&gt; 48164, …\n$ `Total timeconsume of submissions`                             &lt;dbl&gt; 802, 25…\n$ cluster                                                        &lt;int&gt; 1, 2, 2…\n$ `Sum of overall highest submission scores`                     &lt;dbl&gt; 100, 10…\n$ `Sum of overall highest submission scores for b3C9s knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for g7R2j knowledge` &lt;dbl&gt; 15, 15,…\n$ `Sum of overall highest submission scores for k4W1c knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for m3D1v knowledge` &lt;dbl&gt; 36, 36,…\n$ `Sum of overall highest submission scores for r8S3g knowledge` &lt;dbl&gt; 5, 5, 5…\n$ `Sum of overall highest submission scores for s8Y2f knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for t5V9e knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for y9W5d knowledge` &lt;dbl&gt; 33, 33,…\n$ `Percent of submissions absolutely correct`                    &lt;dbl&gt; 20.9302…\n$ `No. of questions answered fully or partially correct`         &lt;int&gt; 38, 38,…\n$ `Sum of points Overall`                                        &lt;dbl&gt; 38.7568…\n$ `Sum of points for b3C9s knowledge`                            &lt;dbl&gt; 3.00000…\n$ `Sum of points for g7R2j knowledge`                            &lt;dbl&gt; 6.74242…\n$ `Sum of points for k4W1c knowledge`                            &lt;dbl&gt; 1.16666…\n$ `Sum of points for m3D1v knowledge`                            &lt;dbl&gt; 14.0000…\n$ `Sum of points for r8S3g knowledge`                            &lt;dbl&gt; 4.04700…\n$ `Sum of points for s8Y2f knowledge`                            &lt;dbl&gt; 0.95238…\n$ `Sum of points for t5V9e knowledge`                            &lt;dbl&gt; 3.96739…\n$ `Sum of points for y9W5d knowledge`                            &lt;dbl&gt; 11.6666…\n\n\n\n\nVisualisation of Knowledge Aquisition by learning mode clusters\nTo visualise differences in the performance in total number of questions that had correct or partially correct answers, a ridgeline plot to compare the shape of distribution of students in both clusters in the same axis, using the following code chunk.\n\nStudentLMKA_data$cluster &lt;- as.factor(StudentLMKA_data$cluster)\n\n# Plot\nggplot(StudentLMKA_data, \n       aes(x = `No. of questions answered fully or partially correct`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\nggplot(StudentLMKA_data, \n       aes(x = `Sum of points Overall`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nCluster 2 has a sharper peak and more packed to the right which suggests that students in this cluster generally performed better, while for cluster 1 there is a 2nd smaller group of that performs even worse.\nA multi faceted plot to compare the distribution of answering performance in respect to the 2 clusters across 6 knowledge areas is plot with the following code chunk.\n\n#| fig-width: 12\n#| fig-height: 7\n\n#a &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for b3C9s knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#b &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for g7R2j knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#c &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for m3D1v knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#d &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for r8S3g knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#e &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for t5V9e knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#f &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for y9W5d knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#g &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for k4W1c knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#h &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for s8Y2f knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#(a + b) / (c + d) / (e + f) / (g + h)\n\nThe findings are highly congruent with the earlier ridge plot, which found that cluster 2 had performed better with a sharper peak and more concentration of learners to the right, where as cluster 1 had small pockets of learners to the left instead.\n\n#a &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for b3C9s knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#b &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for g7R2j knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#c &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for m3D1v knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#d &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for r8S3g knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#e &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for t5V9e knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#f &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for y9W5d knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#g &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for k4W1c knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#h &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for s8Y2f knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n#(a + b) / (c + d) / (e + f) / (g + h)\n\nA statistical violin plot to perform both a mathematical 2 sample mean test in tandem with a visual analysis of the difference in the distribution of the students’ total actual score in the answering records in respect of the 2 clusters is plot with the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = `Sum of overall highest submission scores`,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein cluster 2 fared better than cluster 1, it also shows that cluster 2 is much smaller than cluster 1.\nLastly a similar statistical violin plot to analyse the differences in percentage of answers that were absolutely correct in respect of the 2 clusters is plot in the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = `Percent of submissions absolutely correct`,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein surprisingly, cluster 1 had fared better than cluster 2, cluster 2 had a smaller spread and more concentrated compared to cluster 1.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = `Sum of points Overall`,\n  type = \"np\",\n  messages = FALSE\n)"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#conclusion",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#conclusion",
    "title": "Take-home Ex3",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the visual analysis of learning modes clustering found that 2 substantially distinct clusters can be formed using the selected students’ learning mode features, whereby cluster 2 tends to be the more earnest learning mode cluster.\nUsing these clusters to draw a relationship with indicators of students’ knowledge acquistion found that cluster 2 also had a better knowledge acquisition although cluster 1 students seems to have submit lesser answers in general, hence having a higher percentage of correct answers.\nAnd therefore, the consensus in the analysis found that there is a statistically significant relationship between the selected indicators for more hardworking learning modes with better knowledge aquisition."
  },
  {
    "objectID": "Project Proposal/Group6_project_proposal.html",
    "href": "Project Proposal/Group6_project_proposal.html",
    "title": "Group 6 VA Project Proposal",
    "section": "",
    "text": "With the given choices based on the VAST Challenge 2024 and CHINAVIS 2024 Data Visualisation Competition, the selected topic for this group project is Challenge I from the latter, titled「Data Analysis Inspires Wisdom」Time-Series Multivariate Education Data Visual Analytics Challenge. This topic was chosen due to the Project Group members’ contextual familiarity with the topic, as well as its alignment to the members’ learning interests in leveraging on key visual analytics tools, to explore and design data-driven solutions for seemingly complex problems.\nThe selected project is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. Fundamentally, the project’s raison d’etre is to provide actionable insights towards the institution’s endeavor to diagnose and analyze learners’ knowledge mastery levels, monitor the trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\nTo this end, data was collected from a select group of learners over a specified set of programming tasks over a particular learning period to facilitate the design and implementation of a Visual Analytics solution as a key outcome in this project.\n\n\n\nFrom the Challenge, the key problem statement was to perform a comprehensive analysis of multiple datasets that describe various aspects of the learner’s profile, learning patterns and status, to derive key insights to enhance teaching strategies and course design.\nConsequently the key requirements based on the 5 stipulated tasks in the challenge were as follows.\n\nTask 1: To provide a quantitative assessment of the learners’ knowledge mastery and identify weak links in their knowledge system, based on the multi-dimensional attributes such as answer scores and answer status in the learners’ log records of the learners’ question-answering behaviors.\n\nThis would entail an analysis of the learners’ aggregate performance in their programming tasks (a.k.a. questions in the dataset), including measures of central tendency, or any notable patterns that can glean insights towards knowledge mastery and weaknesses from the given datasets\n\nTask 2: To design and present learners’ profiles, based on learners’ personalized learning behavior patterns (including peak answering hours, preferred question types, correct answering rates, etc.), and various other attributes and characteristics\n\nThis would entail an analysis and profiling of of learners’ behaviours and attributes based on statistically significant patterns that are observed across the variables in the given datasets\n\nTask 3: To analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail an analysis to uncover the various learning modes and the patterns and/or correlations that it may have with the learners’ performance in the questions from the given datasets\n\nTask 4: To analyse the difficulty level of questions and learner’s level of knowledge (which should mostly align) and also to identify ‘inappropriate questions’ that may not align to this relationship.\n\nThis would entail an analysis and comparison between the learners’ performance in each question and the learners’ performance ranking, and hence to identify any potential outliers that violate the alignment of question difficulty and learner’s level of knowledge\n\nTask 5: To offer valuable recommendations to optimize question bank content settings and enhance the quality of teaching and learning, based on the outcomes of the aforementioned analysis of Tasks 1 to 4 above.\n\nThis would entail deriving actionable recommendations based on the insights gleaned in the visual analytics of the datasets from tasks 1 to 4 above"
  },
  {
    "objectID": "Project Proposal/Group6_project_proposal.html#project-overview",
    "href": "Project Proposal/Group6_project_proposal.html#project-overview",
    "title": "Group 6 VA Project Proposal",
    "section": "",
    "text": "With the given choices based on the VAST Challenge 2024 and CHINAVIS 2024 Data Visualisation Competition, the selected topic for this group project is Challenge I from the latter, titled「Data Analysis Inspires Wisdom」Time-Series Multivariate Education Data Visual Analytics Challenge. This topic was chosen due to the Project Group members’ contextual familiarity with the topic, as well as its alignment to the members’ learning interests in leveraging on key visual analytics tools, to explore and design data-driven solutions for seemingly complex problems.\nThe selected project is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. Fundamentally, the project’s raison d’etre is to provide actionable insights towards the institution’s endeavor to diagnose and analyze learners’ knowledge mastery levels, monitor the trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\nTo this end, data was collected from a select group of learners over a specified set of programming tasks over a particular learning period to facilitate the design and implementation of a Visual Analytics solution as a key outcome in this project.\n\n\n\nFrom the Challenge, the key problem statement was to perform a comprehensive analysis of multiple datasets that describe various aspects of the learner’s profile, learning patterns and status, to derive key insights to enhance teaching strategies and course design.\nConsequently the key requirements based on the 5 stipulated tasks in the challenge were as follows.\n\nTask 1: To provide a quantitative assessment of the learners’ knowledge mastery and identify weak links in their knowledge system, based on the multi-dimensional attributes such as answer scores and answer status in the learners’ log records of the learners’ question-answering behaviors.\n\nThis would entail an analysis of the learners’ aggregate performance in their programming tasks (a.k.a. questions in the dataset), including measures of central tendency, or any notable patterns that can glean insights towards knowledge mastery and weaknesses from the given datasets\n\nTask 2: To design and present learners’ profiles, based on learners’ personalized learning behavior patterns (including peak answering hours, preferred question types, correct answering rates, etc.), and various other attributes and characteristics\n\nThis would entail an analysis and profiling of of learners’ behaviours and attributes based on statistically significant patterns that are observed across the variables in the given datasets\n\nTask 3: To analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail an analysis to uncover the various learning modes and the patterns and/or correlations that it may have with the learners’ performance in the questions from the given datasets\n\nTask 4: To analyse the difficulty level of questions and learner’s level of knowledge (which should mostly align) and also to identify ‘inappropriate questions’ that may not align to this relationship.\n\nThis would entail an analysis and comparison between the learners’ performance in each question and the learners’ performance ranking, and hence to identify any potential outliers that violate the alignment of question difficulty and learner’s level of knowledge\n\nTask 5: To offer valuable recommendations to optimize question bank content settings and enhance the quality of teaching and learning, based on the outcomes of the aforementioned analysis of Tasks 1 to 4 above.\n\nThis would entail deriving actionable recommendations based on the insights gleaned in the visual analytics of the datasets from tasks 1 to 4 above"
  },
  {
    "objectID": "Project Proposal/Group6_project_proposal.html#the-dataset",
    "href": "Project Proposal/Group6_project_proposal.html#the-dataset",
    "title": "Group 6 VA Project Proposal",
    "section": "The Dataset",
    "text": "The Dataset\nThe provided materials for the challenge include 3 datasets described below, as well as a separate document providing a more detailed description of the data and variables\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of multiple datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project"
  },
  {
    "objectID": "Project Proposal/Group6_project_proposal.html#approach",
    "href": "Project Proposal/Group6_project_proposal.html#approach",
    "title": "Group 6 VA Project Proposal",
    "section": "Approach",
    "text": "Approach\nThe broad approach for the project is shown in the figure below."
  },
  {
    "objectID": "Project Proposal/Group6_project_proposal.html#methodology",
    "href": "Project Proposal/Group6_project_proposal.html#methodology",
    "title": "Group 6 VA Project Proposal",
    "section": "Methodology",
    "text": "Methodology\nOur methodology systematically integrates data collection, data processing, analysis, pattern mining, modeling, and recommendations to create a comprehensive Visual Analytics solution for improving teaching strategies and course designs at NorthClass Institute, elaborated below.\n\nKey Data Preparation\nData preprocessing and cleaning is required at the initial phase to ensure usability and consistency of the data records for subsequent data analysis. Based on initial assessment of the datasets, the Key Data preparation steps required are as follows.\n\nRemove duplicates, handle missing values through imputation or deletion, and correct inconsistencies.\nCorrecting data type and addressing out-liers.\nCreate new relevant attributes if needed.\nMerging of Multiple datasets in Dataset 3 into a single continuous dataframe, and subsequently joining Datasets 1 and 2 with 3.\n\n\n\nKey DataViz\nTo address the aforesaid requirements, the following methodologies will be considered for the data visualisation (DataViz).\n\nFor task 1, the analysis of answering behaviors involves identifying key performance metrics such as average scores, answer correctness, and time correlations to evaluate learners’ understanding and application of knowledge. Descriptive statistics will be used to compute mean, median, and mode of scores for different topics and question types. Histograms, violin and box plots could be used to visualize score distributions. Performance metrics could be calculated such as accuracy rate, average attempts per question, and time efficiency. These statistical analysis and visualisations will be done in tandem with the classification of questions, to finally visualise and quantify mastery and weaknesses in the various knowledge areas.\nFor task 2, learning behavior patterns involve analyzing habitual actions. Time-series analysis will be used to identify peak answering times. And by studying the frequency and performance of different question types with bar charts, the preferred question type can also be identified. To profile and segment the learner groups, clustering analysis algorithms like K-means or hierarchical clustering can be used based on features engineered from the aforementioned behavior patterns and preferences.\nFor task 3, in modeling the relationship between knowledge acquisition and learning modes, feature selection may build upon analysis from task 1 for knowledge acquisition and task 2 for learning modes. Correlation and Association analysis can then be used to ascertain the statistical significance of the relationships if any.\nFor task 4, to identify question difficulty and learner knowledge levels, questions and learners can be ranked and categorized based on the historical performance data, and thresholds can be defined to sort and bin the data into levels. Cross-comparison can thus be done to filter and analyze data to detect inappropriate questions that are too difficult based on learner performance. An alternative group of inappropriate questions can also be suggested through visualising questions that ‘weak’ learners scored better and/or questions that ‘strong’ learners scored worse.\nLast but not least for task 5, based on the insights gleaned, actionable recommendations will be derived, (such as optimizing the question bank, customizing knowledge delivery based on learning patterns and preferences) aiming to enhance the overall quality of teaching and learning.\n\nAccordingly, a summary of the envisioned DataViz for this project will be as as follows.\n\nGeneral Analysis (for Task 1):\n\nBar plot for mean, median, and mode of scores for different topics and question types.\nBar plot for classifying answers into correct and incorrect categories.\nScatter plot for time vs. Correctness\nFacet plots to visualise multiple plots in a single space\n\nScore distribution and performance metrics (for Tasks 1 & 4): \n\nHistogram for score distribution, bar plot for accuracy rate\nFacet, Ridgeline or Raindrop plots for performance metrics and to compare distribution for various categories\n\nPeaking hours analysis (for Task 2):\n\nTime line chart: Displays the number of activities at different time points in a day to help identify peak hours.\nHeat map: Displays the distribution of daily or weekly activities to help discover high-frequency activities in a specific time period.\n\nClustering result display (for Task 2 & 3):\n\nRadar chart: Displays multiple dimensional features of each cluster.\nBox plot: Displays the distribution of each cluster to understand the differences of clusters.\n\nAssociation and Correlation Analysis (for Task 3 & 4):\n\nScatter plot with best-fit line/curve and shaded area for significance level using ggscaterstats for single numerical independent variable and dependent variable\nMultiple Violin plots using ggbetweenstats for multiple categorical independent variables and numerical independent variable"
  },
  {
    "objectID": "Project_Proposal/Group6_project_proposal.html",
    "href": "Project_Proposal/Group6_project_proposal.html",
    "title": "Group 6 VA Project Proposal",
    "section": "",
    "text": "With the given choices based on the VAST Challenge 2024 and CHINAVIS 2024 Data Visualisation Competition, the selected topic for this group project is Challenge I from the latter, titled「Data Analysis Inspires Wisdom」Time-Series Multivariate Education Data Visual Analytics Challenge. This topic was chosen due to the Project Group members’ contextual familiarity with the topic, as well as its alignment to the members’ learning interests in leveraging on key visual analytics tools, to explore and design data-driven solutions for seemingly complex problems.\nThe selected project is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. Fundamentally, the project’s raison d’etre is to provide actionable insights towards the institution’s endeavor to diagnose and analyze learners’ knowledge mastery levels, monitor the trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\nTo this end, data was collected from a select group of learners over a specified set of programming tasks over a particular learning period to facilitate the design and implementation of a Visual Analytics solution as a key outcome in this project.\n\n\n\nFrom the Challenge, the key problem statement was to perform a comprehensive analysis of multiple datasets that describe various aspects of the learner’s profile, learning patterns and status, to derive key insights to enhance teaching strategies and course design.\nConsequently the key requirements based on the 5 stipulated tasks in the challenge were as follows.\n\nTask 1: To provide a quantitative assessment of the learners’ knowledge mastery and identify weak links in their knowledge system, based on the multi-dimensional attributes such as answer scores and answer status in the learners’ log records of the learners’ question-answering behaviors.\n\nThis would entail an analysis of the learners’ aggregate performance in their programming tasks (a.k.a. questions in the dataset), including measures of central tendency, or any notable patterns that can glean insights towards knowledge mastery and weaknesses from the given datasets\n\nTask 2: To design and present learners’ profiles, based on learners’ personalized learning behavior patterns (including peak answering hours, preferred question types, correct answering rates, etc.), and various other attributes and characteristics\n\nThis would entail an analysis and profiling of of learners’ behaviours and attributes based on statistically significant patterns that are observed across the variables in the given datasets\n\nTask 3: To analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail an analysis to uncover the various learning modes and the patterns and/or correlations that it may have with the learners’ performance in the questions from the given datasets\n\nTask 4: To analyse the difficulty level of questions and learner’s level of knowledge (which should mostly align) and also to identify ‘inappropriate questions’ that may not align to this relationship.\n\nThis would entail an analysis and comparison between the learners’ performance in each question and the learners’ performance ranking, and hence to identify any potential outliers that violate the alignment of question difficulty and learner’s level of knowledge\n\nTask 5: To offer valuable recommendations to optimize question bank content settings and enhance the quality of teaching and learning, based on the outcomes of the aforementioned analysis of Tasks 1 to 4 above.\n\nThis would entail deriving actionable recommendations based on the insights gleaned in the visual analytics of the datasets from tasks 1 to 4 above"
  },
  {
    "objectID": "Project_Proposal/Group6_project_proposal.html#project-overview",
    "href": "Project_Proposal/Group6_project_proposal.html#project-overview",
    "title": "Group 6 VA Project Proposal",
    "section": "",
    "text": "With the given choices based on the VAST Challenge 2024 and CHINAVIS 2024 Data Visualisation Competition, the selected topic for this group project is Challenge I from the latter, titled「Data Analysis Inspires Wisdom」Time-Series Multivariate Education Data Visual Analytics Challenge. This topic was chosen due to the Project Group members’ contextual familiarity with the topic, as well as its alignment to the members’ learning interests in leveraging on key visual analytics tools, to explore and design data-driven solutions for seemingly complex problems.\nThe selected project is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. Fundamentally, the project’s raison d’etre is to provide actionable insights towards the institution’s endeavor to diagnose and analyze learners’ knowledge mastery levels, monitor the trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\nTo this end, data was collected from a select group of learners over a specified set of programming tasks over a particular learning period to facilitate the design and implementation of a Visual Analytics solution as a key outcome in this project.\n\n\n\nFrom the Challenge, the key problem statement was to perform a comprehensive analysis of multiple datasets that describe various aspects of the learner’s profile, learning patterns and status, to derive key insights to enhance teaching strategies and course design.\nConsequently the key requirements based on the 5 stipulated tasks in the challenge were as follows.\n\nTask 1: To provide a quantitative assessment of the learners’ knowledge mastery and identify weak links in their knowledge system, based on the multi-dimensional attributes such as answer scores and answer status in the learners’ log records of the learners’ question-answering behaviors.\n\nThis would entail an analysis of the learners’ aggregate performance in their programming tasks (a.k.a. questions in the dataset), including measures of central tendency, or any notable patterns that can glean insights towards knowledge mastery and weaknesses from the given datasets\n\nTask 2: To design and present learners’ profiles, based on learners’ personalized learning behavior patterns (including peak answering hours, preferred question types, correct answering rates, etc.), and various other attributes and characteristics\n\nThis would entail an analysis and profiling of of learners’ behaviours and attributes based on statistically significant patterns that are observed across the variables in the given datasets\n\nTask 3: To analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail an analysis to uncover the various learning modes and the patterns and/or correlations that it may have with the learners’ performance in the questions from the given datasets\n\nTask 4: To analyse the difficulty level of questions and learner’s level of knowledge (which should mostly align) and also to identify ‘inappropriate questions’ that may not align to this relationship.\n\nThis would entail an analysis and comparison between the learners’ performance in each question and the learners’ performance ranking, and hence to identify any potential outliers that violate the alignment of question difficulty and learner’s level of knowledge\n\nTask 5: To offer valuable recommendations to optimize question bank content settings and enhance the quality of teaching and learning, based on the outcomes of the aforementioned analysis of Tasks 1 to 4 above.\n\nThis would entail deriving actionable recommendations based on the insights gleaned in the visual analytics of the datasets from tasks 1 to 4 above"
  },
  {
    "objectID": "Project_Proposal/Group6_project_proposal.html#the-dataset",
    "href": "Project_Proposal/Group6_project_proposal.html#the-dataset",
    "title": "Group 6 VA Project Proposal",
    "section": "The Dataset",
    "text": "The Dataset\nThe provided materials for the challenge include 3 datasets described below, as well as a separate document providing a more detailed description of the data and variables\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of multiple datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project"
  },
  {
    "objectID": "Project_Proposal/Group6_project_proposal.html#approach",
    "href": "Project_Proposal/Group6_project_proposal.html#approach",
    "title": "Group 6 VA Project Proposal",
    "section": "Approach",
    "text": "Approach\nThe broad approach for the project is shown in the figure below."
  },
  {
    "objectID": "Project_Proposal/Group6_project_proposal.html#methodology",
    "href": "Project_Proposal/Group6_project_proposal.html#methodology",
    "title": "Group 6 VA Project Proposal",
    "section": "Methodology",
    "text": "Methodology\nOur methodology systematically integrates data collection, data processing, analysis, pattern mining, modeling, and recommendations to create a comprehensive Visual Analytics solution for improving teaching strategies and course designs at NorthClass Institute, elaborated below.\n\nKey Data Preparation\nData preprocessing and cleaning is required at the initial phase to ensure usability and consistency of the data records for subsequent data analysis. Based on initial assessment of the datasets, the Key Data preparation steps required are as follows.\n\nRemove duplicates, handle missing values through imputation or deletion, and correct inconsistencies.\nCorrecting data type and addressing out-liers.\nCreate new relevant attributes if needed.\nMerging of Multiple datasets in Dataset 3 into a single continuous dataframe, and subsequently joining Datasets 1 and 2 with 3.\n\n\n\nKey DataViz\nTo address the aforesaid requirements, the following methodologies will be considered for the data visualisation (DataViz).\n\nFor task 1, the analysis of answering behaviors involves identifying key performance metrics such as average scores, answer correctness, and time correlations to evaluate learners’ understanding and application of knowledge. Descriptive statistics will be used to compute mean, median, and mode of scores for different topics and question types. Histograms, violin and box plots could be used to visualize score distributions. Performance metrics could be calculated such as accuracy rate, average attempts per question, and time efficiency. These statistical analysis and visualisations will be done in tandem with the classification of questions, to finally visualise and quantify mastery and weaknesses in the various knowledge areas.\nFor task 2, learning behavior patterns involve analyzing habitual actions. Time-series analysis will be used to identify peak answering times. And by studying the frequency and performance of different question types with bar charts, the preferred question type can also be identified. To profile and segment the learner groups, clustering analysis algorithms like K-means or hierarchical clustering can be used based on features engineered from the aforementioned behavior patterns and preferences.\nFor task 3, in modeling the relationship between knowledge acquisition and learning modes, feature selection may build upon analysis from task 1 for knowledge acquisition and task 2 for learning modes. Correlation and Association analysis can then be used to ascertain the statistical significance of the relationships if any.\nFor task 4, to identify question difficulty and learner knowledge levels, questions and learners can be ranked and categorized based on the historical performance data, and thresholds can be defined to sort and bin the data into levels. Cross-comparison can thus be done to filter and analyze data to detect inappropriate questions that are too difficult based on learner performance. An alternative group of inappropriate questions can also be suggested through visualising questions that ‘weak’ learners scored better and/or questions that ‘strong’ learners scored worse.\nLast but not least for task 5, based on the insights gleaned, actionable recommendations will be derived, (such as optimizing the question bank, customizing knowledge delivery based on learning patterns and preferences) aiming to enhance the overall quality of teaching and learning.\n\nAccordingly, a summary of the envisioned DataViz for this project will be as as follows.\n\nGeneral Analysis (for Task 1):\n\nBar plot for mean, median, and mode of scores for different topics and question types.\nBar plot for classifying answers into correct and incorrect categories.\nScatter plot for time vs. Correctness\nFacet plots to visualise multiple plots in a single space\n\nScore distribution and performance metrics (for Tasks 1 & 4): \n\nHistogram for score distribution, bar plot for accuracy rate\nFacet, Ridgeline or Raindrop plots for performance metrics and to compare distribution for various categories\n\nPeaking hours analysis (for Task 2):\n\nTime line chart: Displays the number of activities at different time points in a day to help identify peak hours.\nHeat map: Displays the distribution of daily or weekly activities to help discover high-frequency activities in a specific time period.\n\nClustering result display (for Task 2 & 3):\n\nRadar chart: Displays multiple dimensional features of each cluster.\nBox plot: Displays the distribution of each cluster to understand the differences of clusters.\n\nAssociation and Correlation Analysis (for Task 3 & 4):\n\nScatter plot with best-fit line/curve and shaded area for significance level using ggscaterstats for single numerical independent variable and dependent variable\nMultiple Violin plots using ggbetweenstats for multiple categorical independent variables and numerical independent variable"
  },
  {
    "objectID": "team3/Take_home_Ex3/Take_home_Ex3.html#bivariate-and-multivariate-analysis-of-variables",
    "href": "team3/Take_home_Ex3/Take_home_Ex3.html#bivariate-and-multivariate-analysis-of-variables",
    "title": "Take-home Ex3",
    "section": "Bivariate and Multivariate analysis of variables",
    "text": "Bivariate and Multivariate analysis of variables\nstudent_ID Percent of submissions on weekdays Percent of submissions during working hrs Total no. of different qns_attempted Gini Index for qns in submission Mean selected question scores Mean submission memory size by qns Mean timeconsume by qns Total no. of submissions Mean no. of different answering methods per qns Gini index for answering methods used per qns Total memory size of submissions Total timeconsume of submissions Sum of overall highest submission scores Sum of overall highest submission scores for b3C9s knowledge Sum of overall highest submission scores for g7R2j knowledge Sum of overall highest submission scores for k4W1c knowledge Sum of overall highest submission scores for m3D1v knowledge Sum of overall highest submission scores for r8S3g knowledge Sum of overall highest submission scores for s8Y2f knowledge Sum of overall highest submission scores for t5V9e knowledge Sum of overall highest submission scores for y9W5d knowledge Percent of submissions absolutely correct No. of questions answered fully or partially correct Sum of points Overall Sum of points for b3C9s knowledge Sum of points for g7R2j knowledge Sum of points for k4W1c knowledge Sum of points for m3D1v knowledge Sum of points for r8S3g knowledge Sum of points for s8Y2f knowledge Sum of points for t5V9e knowledge Sum of points for y9W5d knowledge\n\n# Multi linear regression model for sum_highest_actual_score and sum_points overall\nmodel1 &lt;- lm(`Sum of overall highest submission scores` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel2 &lt;- lm(`Sum of points Overall` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\n\n\nggcoefstats(model1, \n            output = \"plot\") +\n  theme(\n    plot.title = element_text(size = 22),\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    legend.title = element_text(size = 22),\n    legend.text = element_text(size = 20)\n  )\n\n\n\nggcoefstats(model2, \n            output = \"plot\") +\n    theme(\n    plot.title = element_text(size = 22),\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    legend.title = element_text(size = 22),\n    legend.text = element_text(size = 20)\n  )\n\n\n\n\n\nmodel3 &lt;- lm(`Sum of points for b3C9s knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel4 &lt;- lm(`Sum of points for g7R2j knowledge` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel5 &lt;- lm(`Sum of points for k4W1c knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel6 &lt;- lm(`Sum of points for m3D1v knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel7 &lt;- lm(`Sum of points for r8S3g knowledge` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel8 &lt;- lm(`Sum of points for s8Y2f knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel9 &lt;- lm(`Sum of points for t5V9e knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel10 &lt;- lm(`Sum of points for y9W5d knowledge` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\n\n#a &lt;- \n  ggcoefstats(model3, \n            output = \"plot\")\n\n\n\n#b &lt;- \n  ggcoefstats(model4, \n            output = \"plot\")\n\n\n\n#c &lt;- \n  ggcoefstats(model5, \n            output = \"plot\")\n\n\n\n#d &lt;- \n  ggcoefstats(model6, \n            output = \"plot\")\n\n\n\n#e &lt;- \n  ggcoefstats(model7, \n            output = \"plot\")\n\n\n\n#f &lt;- \n  ggcoefstats(model8, \n            output = \"plot\")\n\n\n\n#g &lt;- \n  ggcoefstats(model9, \n            output = \"plot\")\n\n\n\n#h &lt;- \n  ggcoefstats(model10, \n            output = \"plot\")\n\n\n\n#(a + b) / (c + d) / (e + f) / (g + h)"
  },
  {
    "objectID": "Detailed_steps/DataPrep/DataPrep.html",
    "href": "Detailed_steps/DataPrep/DataPrep.html",
    "title": "Data Preparation",
    "section": "",
    "text": "These are the detailed steps taken for the Data Preparation and Cleaning, as a pre-cursor to the Feature engineering and Analysis to be done for the various tasks for the project."
  },
  {
    "objectID": "Detailed_steps/DataPrep/DataPrep.html#introduction",
    "href": "Detailed_steps/DataPrep/DataPrep.html#introduction",
    "title": "Data Preparation",
    "section": "",
    "text": "These are the detailed steps taken for the Data Preparation and Cleaning, as a pre-cursor to the Feature engineering and Analysis to be done for the various tasks for the project."
  },
  {
    "objectID": "Detailed_steps/DataPrep/DataPrep.html#getting-started",
    "href": "Detailed_steps/DataPrep/DataPrep.html#getting-started",
    "title": "Data Preparation",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Required R Package Libraries\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\nggstatplot: provides alternative statistical inference methods by default as an extension of the ggplot2 package\nplotly: R library for plotting interactive statistical graphs.\nrjson: Methods for Cluster analysis.\nvisNetwork: Extract and Visualize the Results of Multivariate Data Analyses.\nBiocManager: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\nigraph: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\ncluster\nfactoextra\nstats\nhms\ncaret\nggfortify\ngridExtra\nGGally\nparallelPlot\nseriation\ndendextend\nheatmaply\ncorrplot\nggalluvial\nentropy\nineq\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial, entropy, ineq) \n\n\n\nImporting the Data\nThe data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project\n\nThe code chunk below imports the dataset into R environment by using read_csv() function of readr, which is part of the tidyverse package.\n\ndf_StudentInfo &lt;- read_csv(\"data/Data_StudentInfo.csv\")\n\n\ndf_TitleInfo &lt;- read_csv(\"data/Data_TitleInfo.csv\")\n\n\ncsv_file_list &lt;- dir('data/Data_SubmitRecord')\ncsv_file_list &lt;- paste0(\"./data/Data_SubmitRecord/\",csv_file_list)\n\ndf_StudentRecord &lt;- NULL\nfor (file in csv_file_list) { # for every file...\n  file &lt;- read_csv(file)\n    df_StudentRecord &lt;- rbind(df_StudentRecord, file) # then stick together by rows\n}"
  },
  {
    "objectID": "Detailed_steps/DataPrep/DataPrep.html#data-preparation",
    "href": "Detailed_steps/DataPrep/DataPrep.html#data-preparation",
    "title": "Data Preparation",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nData Cleaning\nBefore data transformation, the cleanliness of the data set is first ascertained by checking for missing and duplicate data.\n\nMissing Data\ncolSums() and is.NA() functions are used to search for missing values as a whole for the 3 data sets in the code chunks as follows.\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentInfo))\n\n     index student_ID        sex        age      major \n         0          0          0          0          0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_TitleInfo))\n\n        index      title_ID         score     knowledge sub_knowledge \n            0             0             0             0             0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentRecord))\n\n      index       class        time       state       score    title_ID \n          0           0           0           0           0           0 \n     method      memory timeconsume  student_ID \n          0           0           0           0 \n\n\nFrom the outputs above, none of the variables contain missing values.\n\n\nCheck for duplicate rows\nUsing duplicated(), duplicate rows in each of the 3 data sets are identified and extracted in the following code chunks.\n\ndf_StudentInfo[duplicated(df_StudentInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\n\ndf_TitleInfo[duplicated(df_TitleInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, title_ID &lt;chr&gt;, score &lt;dbl&gt;, knowledge &lt;chr&gt;,\n#   sub_knowledge &lt;chr&gt;\n\n\n\ndf_StudentRecord[duplicated(df_StudentRecord), ]\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nFrom the outputs above, there were no duplicate rows found.\n\n\n\nData Wrangling for Inconsistencies\nTo get a better understanding of the variables in the original dataset, the glimpse() function is used in the following code chunks.\n\nglimpse(df_StudentInfo)\n\nRows: 1,364\nColumns: 5\n$ index      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ student_ID &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"63eef37311aaac915a45\", \"5d89810b20…\n$ sex        &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"male\", \"male\", \"ma…\n$ age        &lt;dbl&gt; 24, 21, 23, 21, 22, 19, 21, 18, 21, 24, 23, 20, 18, 18, 23,…\n$ major      &lt;chr&gt; \"J23517\", \"J87654\", \"J87654\", \"J78901\", \"J40192\", \"J57489\",…\n\n\n\nglimpse(df_TitleInfo)\n\nRows: 44\nColumns: 5\n$ index         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title_ID      &lt;chr&gt; \"Question_VgKw8PjY1FR6cm2QI9XW\", \"Question_q7OpB2zCMmW9w…\n$ score         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ knowledge     &lt;chr&gt; \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"t…\n$ sub_knowledge &lt;chr&gt; \"r8S3g_l0p5viby\", \"r8S3g_n0m9rsw4\", \"r8S3g_l0p5viby\", \"r…\n\n\n\nglimpse(df_StudentRecord)\n\nRows: 232,818\nColumns: 10\n$ index       &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n\n\n\nIdentifying Other Unexpected Duplicate Values\nConsidering intuitively unique values for certain variables or dependent variables, other forms of duplicates are also identified and cleaned where relevant.\n\nDuplicate student_ID in StudentInfo\n\n\n# Find the duplicated student_IDs\nduplicates &lt;- df_StudentInfo[duplicated(df_StudentInfo$student_ID) | duplicated(df_StudentInfo$student_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate student_IDs\nduplicates\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\nFrom the output above, no duplicates found.\n\nDuplicate title_ID (aka questions) in TitleInfo\n\n\n# Find the duplicated title_IDs\nduplicates &lt;- df_TitleInfo[duplicated(df_TitleInfo$title_ID) | duplicated(df_TitleInfo$title_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate title_IDs\nduplicates\n\n# A tibble: 12 × 5\n   index title_ID                      score knowledge sub_knowledge \n   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n 1     2 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_n0m9rsw4\n 2     3 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_l0p5viby\n 3    21 Question_QRm48lXxzdP7Tn1WgNOf     3 y9W5d     y9W5d_c0w4mj5h\n 4    22 Question_QRm48lXxzdP7Tn1WgNOf     3 m3D1v     m3D1v_r1d7fr3l\n 5    23 Question_pVKXjZn0BkSwYcsa7C31     3 y9W5d     y9W5d_c0w4mj5h\n 6    24 Question_pVKXjZn0BkSwYcsa7C31     3 m3D1v     m3D1v_r1d7fr3l\n 7    26 Question_lU2wvHSZq7m43xiVroBc     3 y9W5d     y9W5d_c0w4mj5h\n 8    27 Question_lU2wvHSZq7m43xiVroBc     3 k4W1c     k4W1c_h5r6nux7\n 9    30 Question_x2Fy7rZ3SwYl9jMQkpOD     3 y9W5d     y9W5d_c0w4mj5h\n10    31 Question_x2Fy7rZ3SwYl9jMQkpOD     3 s8Y2f     s8Y2f_v4x8by9j\n11    36 Question_oCjnFLbIs4Uxwek9rBpu     3 g7R2j     g7R2j_e0v1yls8\n12    37 Question_oCjnFLbIs4Uxwek9rBpu     3 m3D1v     m3D1v_r1d7fr3l\n\n\n\nunique(duplicates$knowledge)\n\n[1] \"r8S3g\" \"y9W5d\" \"m3D1v\" \"k4W1c\" \"s8Y2f\" \"g7R2j\"\n\nunique(duplicates$sub_knowledge)\n\n[1] \"r8S3g_n0m9rsw4\" \"r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h\" \"m3D1v_r1d7fr3l\"\n[5] \"k4W1c_h5r6nux7\" \"s8Y2f_v4x8by9j\" \"g7R2j_e0v1yls8\"\n\n\nFrom the outputs above, some questions (title_ID) belong to up to 2 knowledge areas or 2 sub-knowledge areas, where the scores for the former are consistently 3, and for the latter, 1. This overlap in title_ID affects 6 title_IDs, spreads across 6 knowledge areas and 7 sub-knowledge areas.\nThe unique values for knowledge and sub-knowledge areas are obtained in the following code chunk to better understand the complexity of these 2 variables.\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nBased on the output above, there is a total of 8 knowledge areas and 15 sub-knowledge areas. This suggests that majority of the knowledge areas and approximately half of sub-knowledge areas have overlapping title_ID. From the nomenclature, each sub-knowledge area is tagged to only 1 knowledge area.\nTo meaningfully analyse the relationship between knowledge areas & sub knowledge areas and other variables, additional columns are introduced where the values in these 2 columns are transposed as column labels with binary values to indicate the tagging of each question to that value. This is done in the following code chunk.\n\n# Transpose the knowledge column to create new columns for each unique value\ndf_TitleInfo1 &lt;- df_TitleInfo %&gt;%\n  mutate(knowledge_presence = 1) %&gt;%\n  spread(key = knowledge, value = knowledge_presence, fill = 0)\n\n# Transpose the sub_knowledge column to create new columns for each unique value\ndf_TitleInfo2 &lt;- df_TitleInfo %&gt;%\n  mutate(sub_knowledge_presence = 1) %&gt;%\n  spread(key = sub_knowledge, value = sub_knowledge_presence, fill = 0)\n\n# Combine the new columns with the original dataframe\ndf_TitleInfo3 &lt;- df_TitleInfo2 %&gt;%\n  distinct(index, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo1, by = \"index\") %&gt;%\n  distinct(index, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo, by = \"index\")\n  \n\n# Reassign values to the knowledge & sub_knowledge columns for repeated title_ID rows\ndf_TitleInfo_gp &lt;- df_TitleInfo3 %&gt;%\n  group_by(title_ID) %&gt;%\n  summarise(across(where(is.numeric), max, na.rm = TRUE),\n            knowledge = paste(unique(knowledge.x), collapse = \"_\"),\n            sub_knowledge = paste(unique(sub_knowledge.x), collapse = \"_\")) %&gt;%\n  select(-score.y,\n         -score.x,\n         -index)\n\nglimpse(df_TitleInfo_gp)\n\nRows: 38\nColumns: 27\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3oPyUzDmQtcM…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ score          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"m3D1v\", \"g7R2j\", \"y9W5d\", \"m3D1v\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"m3D1v_r1d7fr3l\", \"…\n\nunique(df_TitleInfo_gp$knowledge)\n\n [1] \"t5V9e\"       \"m3D1v\"       \"g7R2j\"       \"y9W5d\"       \"r8S3g\"      \n [6] \"b3C9s\"       \"y9W5d_m3D1v\" \"y9W5d_k4W1c\" \"g7R2j_m3D1v\" \"y9W5d_s8Y2f\"\n\nunique(df_TitleInfo_gp$sub_knowledge)\n\n [1] \"t5V9e_e1k6cixp\"                \"m3D1v_r1d7fr3l\"               \n [3] \"g7R2j_e0v1yls8\"                \"y9W5d_c0w4mj5h\"               \n [5] \"m3D1v_v3d9is1x\"                \"y9W5d_p8g6dgtv\"               \n [7] \"r8S3g_n0m9rsw4\"                \"y9W5d_e2j7p95s\"               \n [9] \"b3C9s_j0v1yls8\"                \"m3D1v_t0v5ts9h\"               \n[11] \"y9W5d_c0w4mj5h_m3D1v_r1d7fr3l\" \"r8S3g_l0p5viby\"               \n[13] \"g7R2j_j1g8gd3v\"                \"b3C9s_l4z6od7y\"               \n[15] \"y9W5d_c0w4mj5h_k4W1c_h5r6nux7\" \"g7R2j_e0v1yls8_m3D1v_r1d7fr3l\"\n[17] \"r8S3g_n0m9rsw4_r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h_s8Y2f_v4x8by9j\"\n\n\n\nDuplicate class for each Individual Students in StudentRecord\n\n\n# Identify students with multiple classes\nstudents_multiple_classes &lt;- df_StudentRecord %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(unique_classes = n_distinct(class)) %&gt;%\n  filter(unique_classes &gt; 1)\n\nstudents_multiple_classes_entries &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  arrange(desc(student_ID))\n\n# Display the results\nprint(students_multiple_classes_entries)\n\n# A tibble: 12 × 3\n# Groups:   student_ID [6]\n   student_ID           class   count\n   &lt;chr&gt;                &lt;chr&gt;   &lt;int&gt;\n 1 r9m46ndmmmzeeehft96z Class15   140\n 2 r9m46ndmmmzeeehft96z class       1\n 3 qz6jjynwbd3szlp0rj04 Class1    136\n 4 qz6jjynwbd3szlp0rj04 class       1\n 5 nd9xpohv0s4ttw0o7fts Class8    143\n 6 nd9xpohv0s4ttw0o7fts class       1\n 7 lqm8jh0uggps7yd0lx2x Class8    132\n 8 lqm8jh0uggps7yd0lx2x class       1\n 9 isa355t9q5rut5fm8aml Class1    142\n10 isa355t9q5rut5fm8aml class       1\n11 ezdogkk0jqt4nvvvbnxp Class7    125\n12 ezdogkk0jqt4nvvvbnxp class       1\n\n\nBased on the output above, it is apparent that the 2nd class for each of the student above is an erroneous value. Hence this inconsistency will be cleaned in the following code chunk\n\n# Step 1: Identify the correct class for each student (the class with the highest frequency)\ncorrect_classes &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice(1) %&gt;%\n  select(student_ID, correct_class = class)\n\n# Step 2: Replace wrong class values\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  left_join(correct_classes, by = \"student_ID\") %&gt;%\n  mutate(class = ifelse(!is.na(correct_class), correct_class, class)) %&gt;%\n  select(-correct_class)\n\nFor completeness, a check is done for existence of other students with class that has no class number in the following code chunk.\n\nMissingClassNo &lt;- df_StudentRecord %&gt;%\n  filter(class == \"class\")\nMissingClassNo\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nBased on the output above, there are no further students with class without number.\n\n\nIdentifying Other Unexpected and/or Missing Values\n\nMissing Student_ID and title_ID in StudentRecord are also identified.\n\n\nmissing_students &lt;- anti_join(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Display the missing student IDs\nmissing_student_ids &lt;- missing_students %&gt;% select(student_ID) %&gt;% distinct()\nprint(missing_student_ids)\n\n# A tibble: 1 × 1\n  student_ID           \n  &lt;chr&gt;                \n1 44c7cf3881ae07f7fb3eD\n\n\n\nmissing_questions &lt;- anti_join(df_StudentRecord, df_TitleInfo, by = \"title_ID\")\n\n# Display the missing title IDs\nmissing_questions &lt;- missing_questions %&gt;% select(title_ID) %&gt;% distinct()\nprint(missing_questions)\n\n# A tibble: 0 × 1\n# ℹ 1 variable: title_ID &lt;chr&gt;\n\n\nThere is 1 missing student between either StudentRecord or StudentInfo, but no missing questions. Since there is partial missing info on this student, it isn’t meaningful to include in this analysis, hence the student_ID will be excluded in the following code chunk.\n\ndf_StudentInfo &lt;- df_StudentInfo %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\n\n\nOther unexpected values\n\nThe unique values for each column is queried to check for unexpected values in the following code chunk, wherein Index, time, class, title_ID and student_ID are excluded since they will be dealt with separately\n\nunique(df_StudentRecord$state)\n\n [1] \"Absolutely_Correct\" \"Error1\"             \"Absolutely_Error\"  \n [4] \"Error6\"             \"Error4\"             \"Partially_Correct\" \n [7] \"Error2\"             \"Error3\"             \"Error5\"            \n[10] \"Error7\"             \"Error8\"             \"Error9\"            \n[13] \"�������\"           \n\nunique(df_StudentRecord$score)\n\n[1] 3 4 0 1 2\n\nunique(df_StudentRecord$method)\n\n[1] \"Method_Cj9Ya2R7fZd6xs1q5mNQ\" \"Method_gj1NLb4Jn7URf9K2kQPd\"\n[3] \"Method_5Q4KoXthUuYz3bvrTDFm\" \"Method_m8vwGkEZc3TSW2xqYUoR\"\n[5] \"Method_BXr9AIsPQhwNvyGdZL57\"\n\nunique(df_StudentRecord$memory)\n\n  [1]   320   356   196   308     0   312   328   512   324   188   316   344\n [13]   444   192   332   484   360   200   340   184   476   492   180   448\n [25]   464  8544   204   496   364   460   508   456   352   480   348   488\n [37]   468   400   616   472   384   376   452   336   588   604   440   600\n [49]   580   500   640   520   436   368   612   504   736   632  8448   220\n [61]   372   208   828   256   568   576   628   756   620   700   212   592\n [73]   380   396   432   404   644   564   748   216   264   708   768   304\n [85]   420   624  8516  8644   288  8632  8640  8512   408   260   292   608\n [97]  8580   636   536   424   596   272   388   300   280   268   176   160\n[109]   296   416   240   284   248   172  8388   832  4164  4284   428   168\n[121]   572   164   276   528   392   412  8668  8500  8540  8664  8536  8576\n[133]  8628  8504  8800  8524  8392  8548   692   952  8508  8648  9664  9536\n[145]  9564 49852 59616  1332   948   824   724  2876  3024 24668 25208 26712\n[157] 23968   732 25248 22740   712  8520   720 18264   224  4984  8696 20272\n[169] 19576   516  8976  9028  9532   544   584   552   524  5624 29688   688\n[181] 30940 44020   740   556 51376 14656 65536   680 30440 30284 23128 28112\n[193]   760 15060 25660 23356 31796   804 24768 24232 12792 14720 26172 29020\n[205] 32992 28492 10568  8460  8404   908   652   540  8620 34268 11348 11640\n[217] 13124   532 12608 15028  1400 32544 39612 27272 28852 29248  8452  8616\n[229]  8480  8528   560 13576  8436   548  2012 24896   232 21728 21148  4424\n[241]  7640 43512 39912 19936 12580  2412  2436 24224  4296  4332  6392 25912\n[253] 21332 20128   668 35948  2360  8612  8384  5560 26548 25532 13112 15288\n[265] 13992 49336 53216 15040 13780  8496  8424 37184  8476  8400  8408 30656\n[277]  8156  8140  8064  8136 11236  5616  8160  4192 23116 19784 22908 21176\n[289] 18276 20708 19868 16348 18716 17208 19588 14824 20780 20204 24932 21084\n[301] 24992 21884 18764 26624 24368 13240 22988  3740 43532 26084 26320 13340\n[313] 11372 46460 49464 13356  8144  8564  1720 13892 14488 10580 23576  8396\n[325] 15212 15340   872 25648 25920 27028 24356 23544  7416  6560  4852  8556\n[337] 32088 32716 44216  4292   228 33212 33736 27228 27288 11764 10540 11560\n[349] 10456 11384 10708 32932 25940 17800 16764 46908 30512  9368  9472 19156\n[361]  2348 36136  8132  4708 39048 21152 30632 27200   656   252 47096  8552\n[373]  8464 14040 36984  2384  1792  6084  5844  2456  2440 26452 27364   648\n[385]   244 23168 24324  8420 41460 40568 34316   896  1472  7156 23740  6444\n[397]  6972  6200  6060  7488  6700  6580  5184  4948  5052  5820  6120  5404\n[409]  5028  5180  5100  5068  5020  5204  5976  5176  5048  5884  5824  5828\n[421]  5060  5072  5056  6076  6328  5076  8492  8428   236  7340  6668  7492\n[433]  8412  8652 17176  6852  6616  6032 45288 50140 40348 16848 21820 20856\n[445] 26296 28128 31560 17272 17656 37548 34476 38428 30456 41624 34224 18148\n[457] 20816   128   808   156   844   728   716   696   836   676  4324   860\n[469]  1980  8812   660  8636   684  8756   704  8532  8572  1920  1972  2332\n[481]  2172  2296  2280 13908 63088 15432 15680 15624 15824 15956 15724 15292\n[493]  8796  1880  1996  1992 11256 11268 11264 29240 29144 28752 27988  6068\n[505]  1180 28536 11032 39216 35632 28600  2104  8656 36028 38432 12456 30164\n[517]  1268  1328  1316  1240 50220  4540 35888  1976  4440 14336 14384 45680\n[529] 39080 28484 39104 53732  8680  8692  8660 14136  4564  4480 28848 29112\n[541] 18856  8792  8600  8592 41404 37052 36532 37804 33084 37368 30820 50620\n[553] 26248 22264 26616 25900   752 47040 14644 40636 43128 33568 36248 33088\n[565] 28140 28084 30532 30572 48376 47640 17400 20288 28724 20216 12664 12204\n[577] 11960 27188 15700 15664  4580  4584 28036 28732 34004 33508 31808  1528\n[589]  1716 13752  9592  9520  9784  9208  8828 28716 27536 28584  1704  1620\n[601] 13096 14132 14584 57528 45500  7096  2168  2236 12984 20412 31172 29296\n[613] 54356 54336 47548 41664 41812 13624  1336  1348 13496 55524  1352  1356\n[625] 42052   744   996   984   940  1016 29012 28080 26036  7344  7232  7476\n[637]  7828 13956 43452  1456  1324  1364 43196 27964 10812   972  1340  4692\n[649] 27248 44592 44860 46576 20464 52656 52996 48964 49516  6904  6592  6584\n[661]  8672 46852 40364 14500 14712 17740 17620 52584  8488 36488 44204 44500\n[673] 42300 45228 17980 37460 28240 28988 53288 58424  9540  9524  6936  6204\n[685] 54596 28604 29528 42804 12856 13776 15720  4156 12472  8704  8688 29300\n[697] 18612 12976 32376  8776 13548 26456  1884  1752   764  4172 53316 52160\n[709] 47036 45632 53396 51320 12468 11496 53604\n\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"--\"  \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"309\" \"331\"\n [61] \"36\"  \"65\"  \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\" \n [73] \"190\" \"163\" \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\" \n [85] \"66\"  \"326\" \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\" \n [97] \"394\" \"385\" \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\"\n[109] \"55\"  \"84\"  \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"-\"   \"135\" \"103\" \"114\"\n[121] \"258\" \"254\" \"85\"  \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\"\n[133] \"246\" \"146\" \"89\"  \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\"\n[145] \"160\" \"159\" \"182\" \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"280\" \"274\" \"269\"\n[157] \"288\" \"271\" \"136\" \"276\" \"277\" \"356\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\nunique(df_StudentInfo$sex)\n\n[1] \"female\" \"male\"  \n\nunique(df_StudentInfo$age)\n\n[1] 24 21 23 22 19 18 20\n\nunique(df_StudentInfo$major)\n\n[1] \"J23517\" \"J87654\" \"J78901\" \"J40192\" \"J57489\"\n\n\n\nunique(df_TitleInfo$score)\n\n[1] 1 2 3 4\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nFrom the outputs above, there is an unexpected value for state and timeconsume in StudentRecord.\nStarting with state, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_state &lt;- df_StudentRecord %&gt;%\n  filter (state == '�������')\nOutlier_state\n\n# A tibble: 6 × 10\n  index class     time state score title_ID method memory timeconsume student_ID\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n1  6344 Class10 1.70e9 ����…     0 Questio… Metho…  65536 309         c681117f7…\n2  6346 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n3  6347 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n4 10138 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         1883af270…\n5 16420 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         hpb03ydul…\n6 16458 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         ljylby8in…\n\n\nFrom the output above, there are only 6 rows that are affected. Further cross-validation with the data description document found that there should only be 12 unique values for this variable, and including this outlier state value will give 13. Hence this is likely a wrong entry, and so it will be excluded from the analysis in the following code chunk.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (state != '�������')\n\nFor timeconsume, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_timeconsume &lt;- df_StudentRecord %&gt;%\n  filter (timeconsume %in% c('-', '--'))\nOutlier_timeconsume\n\n# A tibble: 2,612 × 10\n   index class    time state score title_ID method memory timeconsume student_ID\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1   191 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9417c1b4c…\n 2   321 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 3   322 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 4   366 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 5   396 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 6   397 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 7   422 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 8   423 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 9   424 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n10   425 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n# ℹ 2,602 more rows\n\n\nBased on the output, there is a sizable number of 2,612 rows with the unexpected value. Hence these rows will be kept in the analysis and replaced with 0 (since there is no existing values of 0 too), however subsequent analysis in this exercise involving the timeconsume variable will note these values as missing values. This is done in the following code chunk\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = ifelse(timeconsume %in% c(\"-\", \"--\"), 0, timeconsume))\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"0\"   \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"36\"  \"65\" \n [61] \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\"  \"190\" \"163\"\n [73] \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\"  \"66\"  \"326\"\n [85] \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\"  \"394\" \"385\"\n [97] \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\" \"55\"  \"84\" \n[109] \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"135\" \"103\" \"114\" \"258\" \"254\" \"85\" \n[121] \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\" \"246\" \"146\" \"89\" \n[133] \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\" \"160\" \"159\" \"182\"\n[145] \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"331\" \"280\" \"274\" \"269\" \"288\" \"271\"\n[157] \"136\" \"276\" \"277\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\n\nRemoving Index Col\nEach data set contains an index column, which is possibly to keep track of the original order and the total number of rows. This is no longer required and relevant in the analysis, hence it will be excluded.\n\n#remove index column\ndf_StudentRecord &lt;- df_StudentRecord %&gt;% select(-1)\ndf_TitleInfo &lt;- df_TitleInfo %&gt;% select(-1)\ndf_StudentInfo &lt;- df_StudentInfo %&gt;% select(-1)\n\n\n\nCorrecting Data Types\nBased on the glimpse() function, the time variable of the StudentRecord is currently in numerical format. This will be corrected to date time format with the following steps.\nStep 1: From the data description document, the data collection period spans 148 days from 31/8/2023 to 25/1/2024, and the time variable of the StudentRecord in this data set is in seconds. This is compared against the min and max values of the time variable converted to days and deducted from the given start and end date of the collection period given, in the following code chunk.\n\n# Get the min and max values of the time column\nmin_time &lt;- min(df_StudentRecord$time, na.rm = TRUE)\nmax_time &lt;- max(df_StudentRecord$time, na.rm = TRUE)\n\n# Display the min & max values\ndate_adjustment1 &lt;- as.numeric(as.Date(\"2023-08-31\")) - (min_time / 24 / 60 / 60)\ndate_adjustment2 &lt;- as.numeric(as.Date(\"2024-01-25\")) - (max_time / 24 / 60 / 60)\ndate_adjustmentavg &lt;- as.Date((date_adjustment1 + date_adjustment2)/2, origin = \"1970-01-01\")\ndate_adjustmentavg\n\n[1] \"1969-12-31\"\n\n\nStep 2: Apply date_adjustmentavg to the time variable to amend the data type to date time format in the folloiwing code chunk\n\n# Convert time from timestamp to POSIXct\ndf_StudentRecord$time_change &lt;- as.POSIXct(df_StudentRecord$time, origin=date_adjustmentavg, tz=\"UTC\")\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\nFurther, the timeconsume variable will be converted to numeric, wherein since the ‘-’ and ‘–’ values found earlier had taken the value of 0, there will not be an issue of NA values affecting subsequent analysis.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = as.numeric(timeconsume))\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;dbl&gt; 3, 3, 2, 2, 3, 5, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 5…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\n\n\n\nCreate Merged Dataset\nTo prepare for cross-dataset visualisation and analysis of variables, the 3 data sets are joined on title_id and student_id variables in the following code chunks.\n\n# Merge StudentInfo with SubmitRecord based on student_ID\nmerged_data &lt;- merge(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Merge TitleInfo with the already merged data based on title_ID\nmerged_data &lt;- merge(merged_data, df_TitleInfo_gp, by = \"title_ID\")\n\nmerged_data &lt;- merged_data %&gt;%\n  rename(\n    actual_score = score.x,\n    question_score = score.y\n  )\n\n\nsaveRDS(merged_data, \"merged_data_df.rds\")\n\n\nglimpse (merged_data)\n\nRows: 232,811\nColumns: 39\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8EK…\n$ student_ID     &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b2292…\n$ class          &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"Cla…\n$ time           &lt;dbl&gt; 1696330917, 1699625054, 1697444103, 1695964704, 1697727…\n$ state          &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"Pa…\n$ actual_score   &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1…\n$ method         &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvyGd…\n$ memory         &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320, 3…\n$ timeconsume    &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3, 3…\n$ time_change    &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16 0…\n$ sex            &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"ma…\n$ age            &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 21,…\n$ major          &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J401…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ question_score &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"…"
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html",
    "href": "Detailed_steps/Task3/Task3.html",
    "title": "Task3",
    "section": "",
    "text": "These are the detailed steps taken for Task 3 of the project.\n\n\nThe key objective of Task 3 is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html#introduction",
    "href": "Detailed_steps/Task3/Task3.html#introduction",
    "title": "Task3",
    "section": "",
    "text": "These are the detailed steps taken for Task 3 of the project.\n\n\nThe key objective of Task 3 is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html#getting-started",
    "href": "Detailed_steps/Task3/Task3.html#getting-started",
    "title": "Task3",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Required R Package Libraries\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\nggstatplot: provides alternative statistical inference methods by default as an extension of the ggplot2 package\nplotly: R library for plotting interactive statistical graphs.\nrjson: Methods for Cluster analysis.\nvisNetwork: Extract and Visualize the Results of Multivariate Data Analyses.\nBiocManager: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\nigraph: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\ncluster\nfactoextra\nstats\nhms\ncaret\nggfortify\ngridExtra\nGGally\nparallelPlot\nseriation\ndendextend\nheatmaply\ncorrplot\nggalluvial\nentropy\nineq\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial, entropy, ineq) \n\n\n\nImporting the Data\nThe data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project\n\nFrom the raw data, the file was cleaned, prepared and merged in the Data Preparation Steps.\nThe code chunk below imports the prepared dataset into R environment by using read_csv() function of readr, which is part of the tidyverse package.\n\nmerged_data &lt;- readRDS(\"merged_data_df.rds\")\n\nglimpse (merged_data)\n\nRows: 232,811\nColumns: 39\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8EK…\n$ student_ID     &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b2292…\n$ class          &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"Cla…\n$ time           &lt;dbl&gt; 1696330917, 1699625054, 1697444103, 1695964704, 1697727…\n$ state          &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"Pa…\n$ actual_score   &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1…\n$ method         &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvyGd…\n$ memory         &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320, 3…\n$ timeconsume    &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3, 3…\n$ time_change    &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16 0…\n$ sex            &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"ma…\n$ age            &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 21,…\n$ major          &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J401…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ question_score &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"…"
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html#learning-modes",
    "href": "Detailed_steps/Task3/Task3.html#learning-modes",
    "title": "Task3",
    "section": "Learning modes",
    "text": "Learning modes\nBased on the given data, the relevant features that best defines a learner’s learning mode is assessed to be as follows:\n\nPeak answering hours determined by (a) day of the week and (b) time of the day\nVariety of question types attempted determined by (a) total number of different questions attempted, (b) total number of different knowledge and sub knowledge areas covered,\nDepth of question types and answers determined by (a) mean question scores, (b) mean memory size of file submissions across questions\nLevel of learning effort determined by (a) total number of answering attempts, (b) average number of different answering methods used across questions, (C) total memory size of file submission\nCategorical preferences\n\n\nFeature engineering\n\nPeak answering hours Boolean Integer Variables\nSplitting Date and time up from the earlier created time_change date-time variable, and adding 2 derived variables for boolean integer values for weekday (Mon to Fri) and working hours (8am to 8pm) with the following code chunk.\n\nmerged_data_lm &lt;- merged_data %&gt;%\n  mutate(\n    date = as.Date(time_change),\n    time = as_hms(format(time_change, \"%H:%M:%S\")),\n    is_weekday = as.numeric(wday(date) %in% 2:6),  # Monday to Friday 1, else 0\n    is_working_hours = as.numeric(hour(time) &gt;= 8 & hour(time) &lt; 20)  # 8am to 8pm 1, else 0\n  )\n\nglimpse(merged_data_lm)\n\nRows: 232,811\nColumns: 42\n$ title_ID         &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8…\n$ student_ID       &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b22…\n$ class            &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"C…\n$ time             &lt;time&gt; 04:09:22, 07:11:39, 01:22:28, 22:25:49, 08:11:04, 02…\n$ state            &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"…\n$ actual_score     &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1,…\n$ method           &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvy…\n$ memory           &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320,…\n$ timeconsume      &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3,…\n$ time_change      &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16…\n$ sex              &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"…\n$ age              &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 2…\n$ major            &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J4…\n$ b3C9s_j0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ b3C9s_l4z6od7y   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_e0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_j1g8gd3v   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c_h5r6nux7   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_r1d7fr3l   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_t0v5ts9h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_v3d9is1x   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_l0p5viby   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_n0m9rsw4   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f_v4x8by9j   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e_e1k6cixp   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d_c0w4mj5h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_e2j7p95s   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_p8g6dgtv   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ b3C9s            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ question_score   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ knowledge        &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\",…\n$ sub_knowledge    &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\",…\n$ date             &lt;date&gt; 2023-10-03, 2023-11-10, 2023-10-16, 2023-09-28, 2023…\n$ is_weekday       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ is_working_hours &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nGroup By Student ID\nThe following variables will be obtained with the code chunk below in preparation for clustering analysis\n\nPeak answering hours\n\n\npercentage of answers on weekdays,\npercentage of answers during working hours\n\n\nVariety of question types attempted\n\n\ntotal number of different questions attempted,\ntotal number of different knowledge and sub knowledge areas covered,\n\n\nDepth of question types\n\n\nmean question scores,\nmean memory size of file submissions across questions\nmean time consume across questions\n\n\nLevel of learning effort in answers submitted\n\n\ntotal number of answering attempts,\naverage number of different answering methods used across questions,\ntotal memory size of file submission\ntotal time consume for answers submitted\n\n\nStudentLM_data &lt;- merged_data_lm %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    `Percent of submissions on weekdays` = sum(is_weekday, na.rm = TRUE) / n() * 100,\n    `Percent of submissions during working hrs` = sum(is_working_hours, na.rm = TRUE) / n() * 100,\n    `Total no. of different qns_attempted` = n_distinct(title_ID, na.rm = TRUE),\n    `Gini Index for qns in submission` = Gini(table(title_ID)),\n    `Span of different knowledge in qns` = sum(colSums(across(29:36, as.numeric)) &gt; 0),\n    `Span of different sub knowledge in qns` = sum(colSums(across(14:28, as.numeric)) &gt; 0),\n    `Mean selected question scores` = mean(question_score, na.rm = TRUE),\n    `Mean submission memory size by qns` = mean(sapply(split(memory, title_ID), mean, na.rm = TRUE), na.rm = TRUE),\n    `Mean timeconsume by qns` = mean(sapply(split(timeconsume, title_ID), mean, na.rm = TRUE), na.rm = TRUE),\n    `Total no. of submissions` = n(),\n    `Mean no. of different answering methods per qns` = mean(sapply(split(method, title_ID), function(x) n_distinct(x, na.rm = TRUE)), na.rm = TRUE),\n    `Gini index for answering methods used per qns` = Gini(table(method)),\n    `Total memory size of submissions` = sum(memory, na.rm = TRUE),\n    `Total timeconsume of submissions` = sum(timeconsume, na.rm = TRUE)\n  )\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 15\n$ student_ID                                        &lt;chr&gt; \"0088dc183f73c83f763…\n$ `Percent of submissions on weekdays`              &lt;dbl&gt; 94.88372, 88.75000, …\n$ `Percent of submissions during working hrs`       &lt;dbl&gt; 14.883721, 10.416667…\n$ `Total no. of different qns_attempted`            &lt;int&gt; 38, 38, 38, 38, 38, …\n$ `Gini Index for qns in submission`                &lt;dbl&gt; 0.47380661, 0.395175…\n$ `Span of different knowledge in qns`              &lt;int&gt; 8, 8, 8, 8, 8, 8, 8,…\n$ `Span of different sub knowledge in qns`          &lt;int&gt; 15, 15, 15, 15, 15, …\n$ `Mean selected question scores`                   &lt;dbl&gt; 2.339535, 2.266667, …\n$ `Mean submission memory size by qns`              &lt;dbl&gt; 249.1707, 419.3248, …\n$ `Mean timeconsume by qns`                         &lt;dbl&gt; 3.543709, 12.390546,…\n$ `Total no. of submissions`                        &lt;int&gt; 215, 240, 478, 119, …\n$ `Mean no. of different answering methods per qns` &lt;dbl&gt; 2.815789, 3.289474, …\n$ `Gini index for answering methods used per qns`   &lt;dbl&gt; 0.04279070, 0.070000…\n$ `Total memory size of submissions`                &lt;dbl&gt; 48164, 97820, 136496…\n$ `Total timeconsume of submissions`                &lt;dbl&gt; 802, 2583, 9814, 415…\n\n\n\n\nUnivariate Analysis of features\n\n# Define the function to create combined box plot and histogram\ncreate_combined_plot &lt;- function(data, variable) {\n  ggplot(data, aes_string(x = paste0(\"`\", variable, \"`\"))) +\n    # Histogram\n    geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    geom_density(alpha = 0.3, fill = \"orange\") +\n    # Box plot\n    geom_boxplot(aes(y = 0), width = 0.1, color = \"red\", position = position_nudge(y = -0.1)) +\n    theme_minimal() +\n    labs(x = variable, y = \"Density\") +\n    ggtitle(paste(\"Combined Histogram and Box Plot for\", variable))\n}\n\n\n# Variables to plot\nvariables &lt;- names(StudentLM_data)[2:15]\n\n# Create combined plots for each variable\nplots &lt;- lapply(variables, function(var) create_combined_plot(StudentLM_data, var))\n\n# Display the plots\nfor (p in plots) {\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck for high colinearity\n\nSLM.cor &lt;- cor(StudentLM_data[, 2:15])\n\ncorrplot(SLM.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = StudentLM_data, \n  cor.vars = 2:15)\n\n\n\n\n\n\n\n\n\n\nRemoving highly skewed and correlated columns\nBased on the output from the univariate and correlation analysis, 2 variables were found to be highly skewed and concentrated within 1 or 2 values, hence they are removed for more meaningful analysis, with the following code chunk. For high correlation with a threshold of &gt;0.8, 3 variables were found to be highly correlated, of which, 2 have been removed as highly skewed, leaving total_different_questions_attempted in the data frame.\n\nStudentLM_data &lt;- StudentLM_data %&gt;%\n  select(-`Span of different knowledge in qns`, \n#         -`Total timeconsume of submissions`, \n#         -`Total memory size of submissions`, \n#         -`Total no. of submissions`, \n#         -`Mean no. of different answering methods per qns`, \n#         -`Gini index for answering methods used per qns`, \n#         -`Total no. of different qns_attempted`, \n         -`Span of different sub knowledge in qns`)\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 13\n$ student_ID                                        &lt;chr&gt; \"0088dc183f73c83f763…\n$ `Percent of submissions on weekdays`              &lt;dbl&gt; 94.88372, 88.75000, …\n$ `Percent of submissions during working hrs`       &lt;dbl&gt; 14.883721, 10.416667…\n$ `Total no. of different qns_attempted`            &lt;int&gt; 38, 38, 38, 38, 38, …\n$ `Gini Index for qns in submission`                &lt;dbl&gt; 0.47380661, 0.395175…\n$ `Mean selected question scores`                   &lt;dbl&gt; 2.339535, 2.266667, …\n$ `Mean submission memory size by qns`              &lt;dbl&gt; 249.1707, 419.3248, …\n$ `Mean timeconsume by qns`                         &lt;dbl&gt; 3.543709, 12.390546,…\n$ `Total no. of submissions`                        &lt;int&gt; 215, 240, 478, 119, …\n$ `Mean no. of different answering methods per qns` &lt;dbl&gt; 2.815789, 3.289474, …\n$ `Gini index for answering methods used per qns`   &lt;dbl&gt; 0.04279070, 0.070000…\n$ `Total memory size of submissions`                &lt;dbl&gt; 48164, 97820, 136496…\n$ `Total timeconsume of submissions`                &lt;dbl&gt; 802, 2583, 9814, 415…\n\n\nNoting that there would be complex interaction of these features that could eventually shape a learners’ knowledge acquisition, coupled with the fact that there would be varying levels of each feature present in each learner, it was concluded that cluster analysis of the aforementioned features across the students would provide the most meaningful relationship between learning mode patterns and knowledge acquisition of learners.\n\n\n\nNumber of K-Means clusters\nTo determine the ideal number of clusters for K-means clustering on the recompiled learners’ learning mode features, a silhouette analysis and SSE elbow method are performed in the following code chunks.\n\nSilhouette analysis\n\n# Exclude non-numeric columns\nStudentLM_data_numeric &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Function to compute silhouette widths\nsilhouette_analysis &lt;- function(data, max_clusters) {\n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(kmeans_result$cluster, dist(data))\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 12\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(StudentLM_data_numeric, max_clusters)\n\n# Plot the average silhouette widths\nplot(1:max_clusters, avg_sil_widths, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\n\n\n\n\nSSE-Elbow method\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 18\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(StudentLM_data_numeric, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nK Means clustering and Visualisation\nK Means clustering is then performed on the recompiled learners’ learning mode features with the number of clusters set as 2 based on the above results, in the following code chunk\n\n# Drop the student_ID column\nclustering_data &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Standardize the data\nclustering_data_scaled &lt;- scale(clustering_data)\n\n# Perform k-means clustering\nset.seed(123)  # For reproducibility\nkmeans_result &lt;- kmeans(clustering_data_scaled, centers = 2, nstart = 25)\n\n# Add the cluster assignments to the original data\nStudentLM_data$cluster &lt;- kmeans_result$cluster\n\nThe first plot for visualisation of the K means cluster is the Principal Component Analysis (PCA) Plot, which gives an initial sensing of the separation of the clusters based on first 2 PCA components that rank the highest in distinctness amongst the features used. This is plotted with the following code chunk.\n\n# Perform PCA\npca_result &lt;- prcomp(StudentLM_data[-1], scale. = TRUE)\n\n# Get PCA scores\npca_scores &lt;- as.data.frame(predict(pca_result))\n\n# Add cluster information to PCA scores\npca_scores$cluster &lt;- factor(StudentLM_data$cluster)\n\n# Plot PCA results with cluster color coding\npca_plot &lt;- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_discrete(name = \"Cluster\") +\n  labs(x = \"Principal Component 1\", y = \"Principal Component 2\",\n       title = \"PCA Plot of Clusters\") +\n  theme_minimal()\n\n# Display the plot\npca_plot\n\n\n\n\n\n\n\n\nBased on the PCA plot, the clusters are visually clearly separated, suggesting that the clusters are distinct, especially in relation to the top 2 PCA components in the x and y-axis.\nNext to visualise the distribution of the 2 clusters across all the features used for the K Means clustering, a parallel coordinate plot is used, with the following code chunk.\n\nStudentLM_data_factor &lt;- StudentLM_data\nStudentLM_data_factor$cluster &lt;- as.character(StudentLM_data_factor$cluster)\n\nggparcoord(data = StudentLM_data_factor,\n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n  theme(\n    plot.title = element_text(size = 20),\n    axis.text.x = element_text(angle = 30, hjust = 0.8, size = 18),\n    axis.text.y = element_text(size = 18),\n    axis.title.x = element_text(size = 18),\n    axis.title.y = element_text(size = 18),\n    legend.title = element_text(size = 18),\n    legend.text = element_text(size = 18)\n    )\n\n\n\n\n\n\n\nggparcoord(data = StudentLM_data_factor,\n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Split Plot of Students' learning modes\")+\n  facet_wrap(~ cluster)+\n  theme(\n    plot.title = element_text(size = 20),\n    axis.text.x = element_text(angle = 30, hjust = 0.8, size = 18),\n    axis.text.y = element_text(size = 18),\n    axis.title.x = element_text(size = 18),\n    axis.title.y = element_text(size = 18),\n    legend.title = element_text(size = 18),\n    legend.text = element_text(size = 18)\n    )\n\n\n\n\n\n\n\n\nBased on the plot, there is varying degree of distinction in separation between the 2 clusters across different variables. The more distinct separation are in variables such as total timeconsume of answers, total memory size of answers, mean different answering methods per question, total answering attempts and question selection gini index, where cluster 2 tends to fare better in these metrics suggesting that perhaps cluster 2 may be the more hardworking learning mode among the 2.\nAn Alluvial plot is also used in the following code chunk for an alternative visualisation of the clustering, where variables are binned into 5 equally sized bins based on distribution of student_ID.\n\n# Define a function to bin numerical variables based on the distribution of student_IDs\nbin_variable_equal_ids &lt;- function(x, bins = 5) {\n  n &lt;- length(x)\n  quantile_ranks &lt;- ceiling(rank(x, ties.method = \"first\") / (n / bins))\n  as.factor(quantile_ranks)\n}\n\n# Apply the binning function to numerical columns, excluding student_ID and cluster\nStudentLM_data_binned &lt;- StudentLM_data %&gt;%\n  mutate(across(-c(student_ID, cluster), ~ bin_variable_equal_ids(., bins = 5)))\n\n# Convert data to long format\nStudentLM_data_long &lt;- StudentLM_data_binned %&gt;%\n  pivot_longer(cols = -c(student_ID, cluster), names_to = \"variable\", values_to = \"value\")\n\n# Check rows with NA values\nStudentLM_data_checkNA &lt;- StudentLM_data_long %&gt;%\n  filter(if_any(everything(), ~ is.na(.)))\n\nglimpse(StudentLM_data_checkNA)\n\nRows: 0\nColumns: 4\n$ student_ID &lt;chr&gt; \n$ cluster    &lt;int&gt; \n$ variable   &lt;chr&gt; \n$ value      &lt;fct&gt; \n\n\n\n# Ensure the 'cluster' variable is in discrete values (1 and 2)\nStudentLM_data_long &lt;- StudentLM_data_long %&gt;%\n  mutate(cluster = as.factor(cluster))\n\n# Ensure there are no NA values in the cluster column\nStudentLM_data_long &lt;- StudentLM_data_long %&gt;%\n  filter(!is.na(cluster))\n\n# Create the alluvial plot\nggplot(StudentLM_data_long,\n       aes(x = variable, stratum = value, alluvium = student_ID)) +\n#  geom_flow(stat = \"alluvium\", lode.guidance = \"forward\", color = \"white\") +\n  geom_alluvium(aes(fill = cluster)) +\n  geom_stratum() +\n  scale_x_discrete(limits = unique(StudentLM_data_long$variable), expand = c(0.5, 0.1)) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Learning Mode Clusters\",\n       x = \"Variables\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))        \n\n\n\n\n\n\n\n#        axis.text.y = element_text(size = 8),        plot.title = element_text(size = 12),        legend.title = element_text(size = 10),        legend.text = element_text(size = 8),        axis.title.x = element_text(size = 10),        axis.title.y = element_text(size = 10),        plot.margin = unit(c(1, 1, 1, 1), \"cm\"))\n\n\n\nNumber of clusters and method for Hierarchical Clustering\nAs an alternative to K means, hierarchical clustering is also considered, and initiates with mapping the data frame into a data matrix, and thereby using the dend_expend function to determine the best clustering method.\n\nStudentLM_data1 &lt;- StudentLM_data  %&gt;%\n  select(-cluster) \n#  mutate(across(everything(), scale))\n\nrow.names(StudentLM_data1) &lt;- StudentLM_data$student_ID\n#StudentLM_data1 &lt;- select(StudentLM_data1, c(1, 2:13))\nStudentLM_data_matrix1 &lt;- data.matrix(StudentLM_data1)\n\nStudentLM_data_d1 &lt;- dist(\n  normalize(StudentLM_data_matrix1[, -c(1)]), \n  method = \"euclidean\")\ndend_expend(StudentLM_data_d1)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.2669653\n2      unknown        ward.D2 0.3637016\n3      unknown         single 0.7571795\n4      unknown       complete 0.5926958\n5      unknown        average 0.8032403\n6      unknown       mcquitty 0.6096951\n7      unknown         median 0.6870881\n8      unknown       centroid 0.7422864\n\n\nBased on the output above, the average method will be the most optimal.\nA silhoutte plot in the same approach as before is also done with the following code chunk to determine the optimal number of clusters to achieve higher distinction in cluster separation for hierarchical clustering.\n\nStudentLM_data_clust &lt;- hclust(StudentLM_data_d1, method = \"average\")\nnum_k &lt;- find_k(StudentLM_data_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nBased on the output above, 9 clusters were identified to be optimal in terms of distinction in separation.\n\n\nHierarchical Clustering and Visualisation\nNow using the 2 parameters, the hierarchical clustering using the same visualisations of PCA, Parallel Coordinate Plots and Alluvial Plots are created with the following code chunks.\n\n# Cut the tree into a 9 clusters\ncluster_cut &lt;- cutree(StudentLM_data_clust, k = 9)\n\n# Add the cluster assignment to the data\nStudentLM_data1_n &lt;- normalize(StudentLM_data1)\n\nStudentLM_data1_n$cluster_hc &lt;- as.factor(cluster_cut)\n\nglimpse(StudentLM_data1_n)\n\nRows: 1,364\nColumns: 14\n$ student_ID                                        &lt;chr&gt; \"0088dc183f73c83f763…\n$ `Percent of submissions on weekdays`              &lt;dbl&gt; 0.9488372, 0.8875000…\n$ `Percent of submissions during working hrs`       &lt;dbl&gt; 0.14883721, 0.104166…\n$ `Total no. of different qns_attempted`            &lt;dbl&gt; 1.0000000, 1.0000000…\n$ `Gini Index for qns in submission`                &lt;dbl&gt; 0.66600804, 0.555479…\n$ `Mean selected question scores`                   &lt;dbl&gt; 0.6515011, 0.6160606…\n$ `Mean submission memory size by qns`              &lt;dbl&gt; 0.15476836, 0.288330…\n$ `Mean timeconsume by qns`                         &lt;dbl&gt; 0.05099268, 0.406939…\n$ `Total no. of submissions`                        &lt;dbl&gt; 0.25330132, 0.283313…\n$ `Mean no. of different answering methods per qns` &lt;dbl&gt; 0.51492537, 0.649253…\n$ `Gini index for answering methods used per qns`   &lt;dbl&gt; 0.16432955, 0.294312…\n$ `Total memory size of submissions`                &lt;dbl&gt; 0.06864675, 0.139881…\n$ `Total timeconsume of submissions`                &lt;dbl&gt; 0.043706487, 0.14211…\n$ cluster_hc                                        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n# Perform PCA\npca_result &lt;- prcomp(StudentLM_data1_n[, -c(1, 14)], scale = FALSE)\npca_df &lt;- as.data.frame(pca_result$x[, 1:2])  # Example: Extracting the first two principal components\n\n# Add cluster information to PCA scores\npca_scores$cluster_hc &lt;- factor(StudentLM_data1_n$cluster_hc)\n\n# Plot PCA with cluster_hc\nggplot(pca_df, aes(x = PC1, y = PC2, color = factor(StudentLM_data1_n$cluster_hc))) +\n  geom_point() +\n  labs(title = \"PCA Plot with Clustering\", x = \"Principal Component 1\", y = \"Principal Component 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggparcoord(data = StudentLM_data1_n, \n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n# Define a function to bin numerical variables based on the distribution of student_IDs\nbin_variable_equal_ids &lt;- function(x, bins = 5) {\n  n &lt;- length(x)\n  quantile_ranks &lt;- ceiling(rank(x, ties.method = \"first\") / (n / bins))\n  as.factor(quantile_ranks)\n}\n\n# Apply the binning function to numerical columns, excluding student_ID and cluster\nStudentLM_data_binned1 &lt;- StudentLM_data1_n %&gt;%\n  mutate(across(-c(student_ID, cluster_hc), ~ bin_variable_equal_ids(., bins = 5)))\n\n# Convert data to long format\nStudentLM_data_long1 &lt;- StudentLM_data_binned1 %&gt;%\n  pivot_longer(cols = -c(student_ID, cluster_hc), names_to = \"variable\", values_to = \"value\")\n\n# Check rows with NA values\nStudentLM_data_checkNA &lt;- StudentLM_data_long1 %&gt;%\n  filter(if_any(everything(), ~ is.na(.)))\n\nglimpse(StudentLM_data_checkNA)\n\nRows: 0\nColumns: 4\n$ student_ID &lt;chr&gt; \n$ cluster_hc &lt;fct&gt; \n$ variable   &lt;chr&gt; \n$ value      &lt;fct&gt; \n\n\n\n# Ensure the 'cluster' variable is in discrete values (1 and 2)\nStudentLM_data_long1 &lt;- StudentLM_data_long1 %&gt;%\n  mutate(cluster_hc = as.factor(cluster_hc))\n\n# Ensure there are no NA values in the cluster column\nStudentLM_data_long1 &lt;- StudentLM_data_long1 %&gt;%\n  filter(!is.na(cluster_hc))\n\n# Create the alluvial plot\nggplot(StudentLM_data_long1,\n       aes(x = variable, stratum = value, alluvium = student_ID)) +\n#  geom_flow(stat = \"alluvium\", lode.guidance = \"forward\", color = \"white\") +\n  geom_alluvium(aes(fill = cluster_hc)) +\n  geom_stratum() +\n  scale_x_discrete(limits = unique(StudentLM_data_long$variable), expand = c(0.5, 0.1)) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Learning Mode Clusters\",\n       x = \"Variables\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n        axis.text.y = element_text(size = 8),\n        plot.title = element_text(size = 12),\n        legend.title = element_text(size = 10),\n        legend.text = element_text(size = 8),\n        axis.title.x = element_text(size = 10),\n        axis.title.y = element_text(size = 10),\n        plot.margin = unit(c(1, 1, 1, 1), \"cm\"))\n\n\n\n\n\n\n\n\nFrom the plots above, it can be observed that although hierarchical clustering could give a higher number of optimum clusters, the separation and distinction, is much worse in comparison with the optimum of 2 clusters using K-means. Hence for subsequent analysis, the K-means clustering results will be used instead."
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html#knowledge-acquisition",
    "href": "Detailed_steps/Task3/Task3.html#knowledge-acquisition",
    "title": "Task3",
    "section": "Knowledge Acquisition",
    "text": "Knowledge Acquisition\nBased on the given data, the relevant features that best defines a learner’s knowledge acquisition is assessed to be as follows:\n\nKnowledge mastery determined by (a) overall sum of highest actual score for each question attempted, (b) sum of highest actual score of each question by knowledge area\nKnowledge mastery determined by combined metric defined in Task 1, (a) overall total mastery points, (d) sum of mastery points by knowledge area\ncorrect answering rate determined by (a) percentage of answers absolutely correct, (b) total number of questions with answers absolutely correct and partially correct\n\n\nFeature engineering\n\nGroup By Student ID\nThe following variables will be obtained with the code chunks below in preparation for visualisation and analysis of Knowledge Acquisition with respect to the various learning modes.\n\nKnowledge mastery by actual score\n\n\noverall sum of highest actual score for each question attempted and\nsum of highest actual score of each question by knowledge area\n\n\nKnowledge mastery by mastery points\n\nRecap on Mastery Point metric from task 1:\n\nProportion of absolutely and partially correct attempts: - absolutely correct attempts - award 1 pt - partially correct attempts - award (actual_score / question_score) - normalise attempts across questions - uses (total point / total attempts)\n\nUse of more than 1 method per question - multiply by no. of methods if absolutely correct attempt submitted for that question\n\n\noverall total mastery points\nsum of mastery points by knowledge group\n\n\nCorrect answering rate\n\n\npercentage of answers absolutely correct,\ntotal number of questions with answers absolutely correct and partially correct\n\n\nStudentKA_data &lt;- merged_data %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    # Part (a): Sum of highest actual score for each question attempted\n    `Sum of overall highest submission scores` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"])\n    })),\n    \n    # Part (b): Sum of highest actual score for each knowledge area\n    `Sum of overall highest submission scores for b3C9s knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 29])\n    })),\n    `Sum of overall highest submission scores for g7R2j knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 30])\n    })),\n    `Sum of overall highest submission scores for k4W1c knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 31])\n    })),\n    `Sum of overall highest submission scores for m3D1v knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 32])\n    })),\n    `Sum of overall highest submission scores for r8S3g knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 33])\n    })),\n    `Sum of overall highest submission scores for s8Y2f knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 34])\n    })),\n    `Sum of overall highest submission scores for t5V9e knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 35])\n    })),\n    `Sum of overall highest submission scores for y9W5d knowledge` = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"] * merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 36])\n    })),\n    \n    # Part (c): Percentage of answers absolutely correct\n    `Percent of submissions absolutely correct` = (sum(state == \"Absolutely_Correct\") / n()) * 100,\n    \n    # Part (d): Total number of questions with answers absolutely correct and partially correct\n    `No. of questions answered fully or partially correct` = length(unique(title_ID[state %in% c(\"Partially_Correct\", \"Absolutely_Correct\")]))\n    \n  )\n\n\nglimpse(StudentKA_data)\n\n\n# Assign points to attempts based on state\nadjusted_scores &lt;- merged_data %&gt;%\n  mutate(points = case_when(\n    state == \"Absolutely_Correct\" ~ 1,\n    state == \"Partially_Correct\" ~ actual_score / question_score,\n    TRUE ~ 0 # default case for any unexpected states\n  ))\n\n# Assign points to title_IDs per student factoring in normalisation and multiple methods used\nmastery_scores1 &lt;- adjusted_scores %&gt;%\n  group_by(student_ID, title_ID, knowledge, class) %&gt;%\n  summarise(\n    total_points = sum(points),\n    total_attempts = n(),\n    unique_methods = n_distinct(method),\n    absolutely_correct_methods = sum(points == 1)\n  ) %&gt;%\n  mutate(\n    adjusted_points = total_points / total_attempts,\n    adjusted_points = adjusted_points * ifelse(absolutely_correct_methods &gt; 0, unique_methods, 1)\n  )\n\nglimpse(mastery_scores1)\n\n\nunique(mastery_scores1$knowledge)\n\n\n# Combine the adjusted score with knowledge-transposed titleInfo dataframe\nmastery_scores2 &lt;- df_TitleInfo_gp %&gt;%\n  distinct(title_ID, .keep_all = TRUE) %&gt;%\n  left_join(mastery_scores1, by = \"title_ID\") %&gt;%\n  rename(knowledge = knowledge.x) %&gt;%\n  select(-score,\n         -knowledge.y)\n  \nglimpse(mastery_scores2)\n\n\n# Summing up points for Overall and Specific Knowledge Mastery for each Student\nmastery_scores &lt;- mastery_scores2 %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    # Part (a): Sum of total points across all questions\n    `Sum of points Overall` = sum(adjusted_points),\n    \n    # Part (b): Sum of highest actual score for each knowledge area\n    `Sum of points for b3C9s knowledge` = sum(case_when(\n      b3C9s == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for g7R2j knowledge` = sum(case_when(\n      g7R2j == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for k4W1c knowledge` = sum(case_when(\n      k4W1c == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for m3D1v knowledge` = sum(case_when(\n      m3D1v == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for r8S3g knowledge` = sum(case_when(\n      r8S3g == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for s8Y2f knowledge` = sum(case_when(\n      s8Y2f == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for t5V9e knowledge` = sum(case_when(\n      t5V9e == 1 ~ adjusted_points,\n      TRUE ~ 0\n    )),\n    \n    `Sum of points for y9W5d knowledge` = sum(case_when(\n      y9W5d == 1 ~ adjusted_points,\n      TRUE ~ 0\n    ))\n    \n  )\n\nglimpse(mastery_scores)\n\n\n# Compiling the knowledge acquisition metrics\nStudentKA_data_merged &lt;- left_join(StudentKA_data, mastery_scores, by = \"student_ID\")\n\nsaveRDS(StudentKA_data_merged, \"StudentKA_data_merged.rds\")\n\n\nStudentKA_data_merged &lt;- read_rds(\"StudentKA_data_merged.rds\")\n\nglimpse(StudentKA_data_merged)\n\nRows: 1,364\nColumns: 21\n$ student_ID                                                     &lt;chr&gt; \"0088dc…\n$ `Sum of overall highest submission scores`                     &lt;dbl&gt; 100, 10…\n$ `Sum of overall highest submission scores for b3C9s knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for g7R2j knowledge` &lt;dbl&gt; 15, 15,…\n$ `Sum of overall highest submission scores for k4W1c knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for m3D1v knowledge` &lt;dbl&gt; 36, 36,…\n$ `Sum of overall highest submission scores for r8S3g knowledge` &lt;dbl&gt; 5, 5, 5…\n$ `Sum of overall highest submission scores for s8Y2f knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for t5V9e knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for y9W5d knowledge` &lt;dbl&gt; 33, 33,…\n$ `Percent of submissions absolutely correct`                    &lt;dbl&gt; 20.9302…\n$ `No. of questions answered fully or partially correct`         &lt;int&gt; 38, 38,…\n$ `Sum of points Overall`                                        &lt;dbl&gt; 38.7568…\n$ `Sum of points for b3C9s knowledge`                            &lt;dbl&gt; 3.00000…\n$ `Sum of points for g7R2j knowledge`                            &lt;dbl&gt; 6.74242…\n$ `Sum of points for k4W1c knowledge`                            &lt;dbl&gt; 1.16666…\n$ `Sum of points for m3D1v knowledge`                            &lt;dbl&gt; 14.0000…\n$ `Sum of points for r8S3g knowledge`                            &lt;dbl&gt; 4.04700…\n$ `Sum of points for s8Y2f knowledge`                            &lt;dbl&gt; 0.95238…\n$ `Sum of points for t5V9e knowledge`                            &lt;dbl&gt; 3.96739…\n$ `Sum of points for y9W5d knowledge`                            &lt;dbl&gt; 11.6666…\n\n\n\n# Define the function to create combined box plot and histogram\ncreate_combined_plot &lt;- function(data, variable) {\n  ggplot(data, aes_string(x = paste0(\"`\", variable, \"`\"))) +\n    # Histogram\n    geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    geom_density(alpha = 0.3, fill = \"orange\") +\n    # Box plot\n    geom_boxplot(aes(y = 0), width = 0.1, color = \"red\", position = position_nudge(y = -0.1)) +\n    theme_minimal() +\n    labs(x = variable, y = \"Density\") +\n    ggtitle(paste(\"Combined Histogram and Box Plot for\", variable))\n}\n\n\n# Variables to plot\nvariables &lt;- names(StudentKA_data_merged)[2:21]\n\n# Create combined plots for each variable\nplots &lt;- lapply(variables, function(var) create_combined_plot(StudentKA_data_merged, var))\n\n# Display the plots\nfor (p in plots) {\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMerging Students’ Learning Modes with Knowledge Acqusition features\nWith the both data frames prepared, they will now be merged for the next sub-task which involves comparison of learners’ knowledge acquisition with respect to learning mode, and subsequently to identify patterns and relationships.\n\n# Join the two dataframes on the column student_ID\nStudentLMKA_data &lt;- left_join(StudentLM_data, StudentKA_data_merged, by = \"student_ID\")\n\nglimpse(StudentLMKA_data)\n\nRows: 1,364\nColumns: 34\n$ student_ID                                                     &lt;chr&gt; \"0088dc…\n$ `Percent of submissions on weekdays`                           &lt;dbl&gt; 94.8837…\n$ `Percent of submissions during working hrs`                    &lt;dbl&gt; 14.8837…\n$ `Total no. of different qns_attempted`                         &lt;int&gt; 38, 38,…\n$ `Gini Index for qns in submission`                             &lt;dbl&gt; 0.47380…\n$ `Mean selected question scores`                                &lt;dbl&gt; 2.33953…\n$ `Mean submission memory size by qns`                           &lt;dbl&gt; 249.170…\n$ `Mean timeconsume by qns`                                      &lt;dbl&gt; 3.54370…\n$ `Total no. of submissions`                                     &lt;int&gt; 215, 24…\n$ `Mean no. of different answering methods per qns`              &lt;dbl&gt; 2.81578…\n$ `Gini index for answering methods used per qns`                &lt;dbl&gt; 0.04279…\n$ `Total memory size of submissions`                             &lt;dbl&gt; 48164, …\n$ `Total timeconsume of submissions`                             &lt;dbl&gt; 802, 25…\n$ cluster                                                        &lt;int&gt; 1, 2, 2…\n$ `Sum of overall highest submission scores`                     &lt;dbl&gt; 100, 10…\n$ `Sum of overall highest submission scores for b3C9s knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for g7R2j knowledge` &lt;dbl&gt; 15, 15,…\n$ `Sum of overall highest submission scores for k4W1c knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for m3D1v knowledge` &lt;dbl&gt; 36, 36,…\n$ `Sum of overall highest submission scores for r8S3g knowledge` &lt;dbl&gt; 5, 5, 5…\n$ `Sum of overall highest submission scores for s8Y2f knowledge` &lt;dbl&gt; 3, 3, 3…\n$ `Sum of overall highest submission scores for t5V9e knowledge` &lt;dbl&gt; 10, 10,…\n$ `Sum of overall highest submission scores for y9W5d knowledge` &lt;dbl&gt; 33, 33,…\n$ `Percent of submissions absolutely correct`                    &lt;dbl&gt; 20.9302…\n$ `No. of questions answered fully or partially correct`         &lt;int&gt; 38, 38,…\n$ `Sum of points Overall`                                        &lt;dbl&gt; 38.7568…\n$ `Sum of points for b3C9s knowledge`                            &lt;dbl&gt; 3.00000…\n$ `Sum of points for g7R2j knowledge`                            &lt;dbl&gt; 6.74242…\n$ `Sum of points for k4W1c knowledge`                            &lt;dbl&gt; 1.16666…\n$ `Sum of points for m3D1v knowledge`                            &lt;dbl&gt; 14.0000…\n$ `Sum of points for r8S3g knowledge`                            &lt;dbl&gt; 4.04700…\n$ `Sum of points for s8Y2f knowledge`                            &lt;dbl&gt; 0.95238…\n$ `Sum of points for t5V9e knowledge`                            &lt;dbl&gt; 3.96739…\n$ `Sum of points for y9W5d knowledge`                            &lt;dbl&gt; 11.6666…\n\n\n\n\nVisualisation of Knowledge Aquisition by learning mode clusters\nTo visualise differences in the performance in total number of questions that had correct or partially correct answers, a ridgeline plot is utilised to compare the shape of distribution of students in both clusters on the same axes, using the following code chunk.\n\nStudentLMKA_data$cluster &lt;- as.factor(StudentLMKA_data$cluster)\n\n# Plot\nggplot(StudentLMKA_data, \n       aes(x = `No. of questions answered fully or partially correct`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\nggplot(StudentLMKA_data, \n       aes(x = `Sum of points Overall`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nCluster 2 has a sharper peak and more packed to the right which suggests that students in this cluster generally performed better, while for cluster 1 there is a 2nd smaller group of that performs even worse.\nA multi faceted plot to compare the distribution of answering performance based on mastery points in respect to the 2 clusters across 6 knowledge areas is plotted with the following code chunk.\n\n#| fig-width: 12\n#| fig-height: 7\n\n#a &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for b3C9s knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#b &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for g7R2j knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#c &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for m3D1v knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#d &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for r8S3g knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#e &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for t5V9e knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#f &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for y9W5d knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#g &lt;- \nggplot(StudentLMKA_data, \n       aes(x = `Sum of points for k4W1c knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#h &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of points for s8Y2f knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#(a + b) / (c + d) / (e + f) / (g + h)\n\nA multi faceted plot to compare the distribution of answering performance based on highest actual score in respect to the 2 clusters across 6 knowledge areas is also plotted with the following code chunk.\n\n#a &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for b3C9s knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#b &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for g7R2j knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#c &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for m3D1v knowledge`,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#d &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for r8S3g knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#e &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for t5V9e knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#f &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for y9W5d knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#g &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for k4W1c knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#h &lt;- \n  ggplot(StudentLMKA_data, \n       aes(x = `Sum of overall highest submission scores for s8Y2f knowledge`, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n#(a + b) / (c + d) / (e + f) / (g + h)\n\nThe findings are highly congruent with the earlier ridge plost, which found that cluster 2 had performed better with a sharper peak and more concentration of learners to the right, where as cluster 1 had small pockets of learners to the left instead.\nA statistical violin plot to perform both a mathematical 2 sample mean test in tandem with a visual analysis of the difference in the distribution of the students’ total actual score in the answering records in respect of the 2 clusters is plot with the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = `Sum of overall highest submission scores`,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein cluster 2 fared better than cluster 1, it also shows that cluster 2 is much smaller than cluster 1.\nLastly a similar statistical violin plot to analyse the differences in percentage of answers that were absolutely correct in respect of the 2 clusters is plot in the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = `Percent of submissions absolutely correct`,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein surprisingly, cluster 1 had fared better than cluster 2, cluster 2 had a smaller spread and more concentrated compared to cluster 1.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = `Sum of points Overall`,\n  type = \"np\",\n  messages = FALSE\n)"
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html#bivariate-and-multivariate-analysis-of-variables",
    "href": "Detailed_steps/Task3/Task3.html#bivariate-and-multivariate-analysis-of-variables",
    "title": "Task3",
    "section": "Bivariate and Multivariate analysis of variables",
    "text": "Bivariate and Multivariate analysis of variables\nAs an alternative perspective, the learning mode features are now plot on a multi linear regression model against the knowledge acquistion features of highest actual score and mastery points in the following code chunks.\n\n# Multi linear regression model for sum_highest_actual_score and sum_points overall\nmodel1 &lt;- lm(`Sum of overall highest submission scores` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel2 &lt;- lm(`Sum of points Overall` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\n\n\nggcoefstats(model1, \n            output = \"plot\") +\n  theme(\n    plot.title = element_text(size = 22),\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    legend.title = element_text(size = 22),\n    legend.text = element_text(size = 20)\n  )\n\n\n\n\n\n\n\nggcoefstats(model2, \n            output = \"plot\") +\n    theme(\n    plot.title = element_text(size = 22),\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    legend.title = element_text(size = 22),\n    legend.text = element_text(size = 20)\n  )\n\n\n\n\n\n\n\n\n\nmodel3 &lt;- lm(`Sum of points for b3C9s knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel4 &lt;- lm(`Sum of points for g7R2j knowledge` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel5 &lt;- lm(`Sum of points for k4W1c knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel6 &lt;- lm(`Sum of points for m3D1v knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel7 &lt;- lm(`Sum of points for r8S3g knowledge` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel8 &lt;- lm(`Sum of points for s8Y2f knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel9 &lt;- lm(`Sum of points for t5V9e knowledge` ~  \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\nmodel10 &lt;- lm(`Sum of points for y9W5d knowledge` ~ \n               `Percent of submissions on weekdays`+\n               `Percent of submissions during working hrs`+\n               `Total no. of different qns_attempted`+\n               `Gini Index for qns in submission`+\n               `Mean selected question scores`+\n               `Mean submission memory size by qns`+\n               `Mean timeconsume by qns`+\n               `Total no. of submissions`+\n               `Mean no. of different answering methods per qns`+\n               `Gini index for answering methods used per qns`+\n               `Total memory size of submissions`+\n               `Total timeconsume of submissions`, data = StudentLMKA_data)\n\n#a &lt;- \n  ggcoefstats(model3, \n            output = \"plot\")\n\n\n\n\n\n\n\n#b &lt;- \n  ggcoefstats(model4, \n            output = \"plot\")\n\n\n\n\n\n\n\n#c &lt;- \n  ggcoefstats(model5, \n            output = \"plot\")\n\n\n\n\n\n\n\n#d &lt;- \n  ggcoefstats(model6, \n            output = \"plot\")\n\n\n\n\n\n\n\n#e &lt;- \n  ggcoefstats(model7, \n            output = \"plot\")\n\n\n\n\n\n\n\n#f &lt;- \n  ggcoefstats(model8, \n            output = \"plot\")\n\n\n\n\n\n\n\n#g &lt;- \n  ggcoefstats(model9, \n            output = \"plot\")\n\n\n\n\n\n\n\n#h &lt;- \n  ggcoefstats(model10, \n            output = \"plot\")\n\n\n\n\n\n\n\n#(a + b) / (c + d) / (e + f) / (g + h)"
  },
  {
    "objectID": "Detailed_steps/Task3/Task3.html#conclusion",
    "href": "Detailed_steps/Task3/Task3.html#conclusion",
    "title": "Task3",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the visual analysis of learning modes clustering found that 2 substantially distinct clusters can be formed using the selected students’ learning mode features, whereby cluster 2 tends to be the more earnest learning mode cluster.\nUsing these clusters to draw a relationship with indicators of students’ knowledge acquistion found that\n\nattempting questions over a wider span of questions and different knowledge,\nattempting more demanding questions that required more effort,\napplying more effort in answer attempts were congruent with knowledge acquisition in terms of higher scores, higher proportion of correct submissions and the use of more answering methods, especially for knowledge in g7R2j, m3D1v, t5V9e and y9W5. Furthermore, we also discovered that,\napplying a more evenly spread effort across questions and\nattempting higher scoring questions had a statistically more significant effect on overall points and scores."
  },
  {
    "objectID": "team2/final.html",
    "href": "team2/final.html",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "",
    "text": "show the code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(fmsb)\nlibrary(reshape2)\nlibrary(networkD3)\nlibrary(ggalluvial)\nlibrary(fastDummies)\nlibrary(parallelPlot)\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial)"
  },
  {
    "objectID": "team2/final.html#data-observation",
    "href": "team2/final.html#data-observation",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "2.1 Data Observation",
    "text": "2.1 Data Observation\nfirst, we merge all the submission record.\n\n\nshow the code\nfile1 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class1.csv\"\nfile2 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class2.csv\"\nfile3 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class3.csv\"\nfile4 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class4.csv\"\nfile5 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class5.csv\"\nfile6 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class6.csv\"\nfile7 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class7.csv\"\nfile8 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class8.csv\"\nfile9 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class9.csv\"\nfile10 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class10.csv\"\nfile11 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class11.csv\"\nfile12 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class12.csv\"\nfile13 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class13.csv\"\nfile14 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class14.csv\"\nfile15 &lt;- \"data/Data_SubmitRecord/SubmitRecord-Class15.csv\"\n\n# 读取 CSV 文件\ndata1 &lt;- read.csv(file1)\ndata2 &lt;- read.csv(file2)\ndata3 &lt;- read.csv(file3)\ndata4 &lt;- read.csv(file4)\ndata5 &lt;- read.csv(file5)\ndata6 &lt;- read.csv(file6)\ndata7 &lt;- read.csv(file7)\ndata8 &lt;- read.csv(file8)\ndata9 &lt;- read.csv(file9)\ndata10 &lt;- read.csv(file10)\ndata11 &lt;- read.csv(file11)\ndata12 &lt;- read.csv(file12)\ndata13 &lt;- read.csv(file13)\ndata14 &lt;- read.csv(file14)\ndata15 &lt;- read.csv(file15)\n\nsubmit_data &lt;- bind_rows(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11, data12, data13, data14, data15,)\n\nhead(submit_data)\n\n\n  index  class       time              state score\n1     0 Class1 1704209872 Absolutely_Correct     3\n2     1 Class1 1704209852 Absolutely_Correct     3\n3     2 Class1 1704209838 Absolutely_Correct     3\n4     3 Class1 1704208923 Absolutely_Correct     3\n5     4 Class1 1704208359 Absolutely_Correct     4\n6     5 Class1 1704208330             Error1     0\n                       title_ID                      method memory timeconsume\n1 Question_bumGRTJ0c8p4v5D6eHZa Method_Cj9Ya2R7fZd6xs1q5mNQ    320           3\n2 Question_62XbhBvJ8NUSnApgDL94 Method_gj1NLb4Jn7URf9K2kQPd    356           3\n3 Question_ZTbD7mxr2OUp8Fz6iNjy Method_5Q4KoXthUuYz3bvrTDFm    196           2\n4 Question_xqlJkmRaP0otZcX4fK3W Method_m8vwGkEZc3TSW2xqYUoR    308           2\n5 Question_FNg8X9v5zcbB1tQrxHR3 Method_Cj9Ya2R7fZd6xs1q5mNQ    320           3\n6 Question_FNg8X9v5zcbB1tQrxHR3 Method_gj1NLb4Jn7URf9K2kQPd      0           5\n            student_ID\n1 8b6d1125760bd3939b6e\n2 8b6d1125760bd3939b6e\n3 8b6d1125760bd3939b6e\n4 63eef37311aaac915a45\n5 5d89810b20079366fcc2\n6 5d89810b20079366fcc2\n\n\nshow the code\nwrite.csv(submit_data, \"data/submit_data.csv\", row.names = FALSE)\n\n\nNow we have 3 data sets in total, which are:\n\nStudent information data\nQuestion title information\nSubmission record information\n\n\n\nShow the code\nstu_info &lt;- read.csv('data/Data_Studentinfo.csv')\ntit_info &lt;- read.csv('data/Data_Titleinfo.csv')\nsub_info &lt;- read.csv('data/submit_data.csv')\nsummary(stu_info)\n\n\n     index         student_ID            sex                 age       \n Min.   :   1.0   Length:1364        Length:1364        Min.   :18.00  \n 1st Qu.: 341.8   Class :character   Class :character   1st Qu.:19.00  \n Median : 682.5   Mode  :character   Mode  :character   Median :21.00  \n Mean   : 682.5                                         Mean   :21.05  \n 3rd Qu.:1023.2                                         3rd Qu.:23.00  \n Max.   :1364.0                                         Max.   :24.00  \n    major          \n Length:1364       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nShow the code\nsummary(tit_info)\n\n\n     index         title_ID             score        knowledge        \n Min.   : 1.00   Length:44          Min.   :1.000   Length:44         \n 1st Qu.:11.75   Class :character   1st Qu.:2.750   Class :character  \n Median :22.50   Mode  :character   Median :3.000   Mode  :character  \n Mean   :22.50                      Mean   :2.636                     \n 3rd Qu.:33.25                      3rd Qu.:3.000                     \n Max.   :44.00                      Max.   :4.000                     \n sub_knowledge     \n Length:44         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nShow the code\nsummary(sub_info)\n\n\n     index          class                time              state          \n Min.   :    0   Length:232818      Min.   :1.693e+09   Length:232818     \n 1st Qu.: 3880   Class :character   1st Qu.:1.697e+09   Class :character  \n Median : 7760   Mode  :character   Median :1.699e+09   Mode  :character  \n Mean   : 7967                      Mean   :1.699e+09                     \n 3rd Qu.:11640                      3rd Qu.:1.701e+09                     \n Max.   :20201                      Max.   :1.706e+09                     \n     score          title_ID            method              memory       \n Min.   :0.0000   Length:232818      Length:232818      Min.   :    0.0  \n 1st Qu.:0.0000   Class :character   Class :character   1st Qu.:  188.0  \n Median :0.0000   Mode  :character   Mode  :character   Median :  324.0  \n Mean   :0.8991                                         Mean   :  347.3  \n 3rd Qu.:2.0000                                         3rd Qu.:  356.0  \n Max.   :4.0000                                         Max.   :65536.0  \n timeconsume         student_ID       \n Length:232818      Length:232818     \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "team2/final.html#data-clean",
    "href": "team2/final.html#data-clean",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "2.2 Data Clean",
    "text": "2.2 Data Clean\n\n2.2.1 Missing Value\nFirst, we check if there is missing value in these 3 data sets.\n\n\nshow the code\nmissing_values1 &lt;- colSums(is.na(stu_info))\nprint(missing_values1)\n\n\n     index student_ID        sex        age      major \n         0          0          0          0          0 \n\n\nshow the code\nmissing_values2 &lt;- colSums(is.na(tit_info))\nprint(missing_values2)\n\n\n        index      title_ID         score     knowledge sub_knowledge \n            0             0             0             0             0 \n\n\nshow the code\nmissing_values3 &lt;- colSums(is.na(sub_info))\nprint(missing_values3)\n\n\n      index       class        time       state       score    title_ID \n          0           0           0           0           0           0 \n     method      memory timeconsume  student_ID \n          0           0           0           0 \n\n\n\n\n2.2.2 Outliers\nThere is no missing value in all 3 data sets. Now we see if there are outliers. :\n\nstateclasstime consume\n\n\n\n\nshow the code\nunique_state &lt;- unique(sub_info$state)\nprint(unique_state)\n\n\n [1] \"Absolutely_Correct\" \"Error1\"             \"Absolutely_Error\"  \n [4] \"Error6\"             \"Error4\"             \"Partially_Correct\" \n [7] \"Error2\"             \"Error3\"             \"Error5\"            \n[10] \"Error7\"             \"Error8\"             \"Error9\"            \n[13] \"�������\"           \n\n\n\n\n\n\nshow the code\nunique_class &lt;- unique(sub_info$class)\nprint(unique_class)\n\n\n [1] \"Class1\"  \"class\"   \"Class2\"  \"Class3\"  \"Class4\"  \"Class5\"  \"Class6\" \n [8] \"Class7\"  \"Class8\"  \"Class9\"  \"Class10\" \"Class11\" \"Class12\" \"Class13\"\n[15] \"Class14\" \"Class15\"\n\n\n\n\n\n\nshow the code\nunique_timeconsume &lt;- unique(sub_info$timeconsume) \nprint(unique_timeconsume)\n\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"--\"  \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"25\"  \"26\"  \"29\"  \"28\"  \"27\" \n [49] \"24\"  \"65\"  \"135\" \"63\"  \"103\" \"114\" \"258\" \"254\" \"85\"  \"66\"  \"69\"  \"90\" \n [61] \"132\" \"173\" \"48\"  \"34\"  \"272\" \"38\"  \"113\" \"116\" \"32\"  \"76\"  \"22\"  \"190\"\n [73] \"187\" \"73\"  \"215\" \"123\" \"246\" \"146\" \"57\"  \"89\"  \"88\"  \"30\"  \"245\" \"75\" \n [85] \"285\" \"70\"  \"400\" \"205\" \"36\"  \"164\" \"163\" \"162\" \"165\" \"266\" \"62\"  \"172\"\n [97] \"143\" \"184\" \"42\"  \"377\" \"160\" \"33\"  \"35\"  \"159\" \"182\" \"41\"  \"52\"  \"74\" \n[109] \"72\"  \"46\"  \"264\" \"81\"  \"153\" \"83\"  \"82\"  \"39\"  \"37\"  \"56\"  \"-\"   \"115\"\n[121] \"55\"  \"286\" \"275\" \"331\" \"280\" \"274\" \"269\" \"288\" \"271\" \"136\" \"117\" \"276\"\n[133] \"277\" \"356\" \"79\"  \"147\" \"44\"  \"350\" \"394\" \"45\"  \"315\" \"321\" \"302\" \"152\"\n[145] \"309\" \"47\"  \"53\"  \"51\"  \"307\" \"201\" \"43\"  \"109\" \"326\" \"49\"  \"77\"  \"71\" \n[157] \"385\" \"78\"  \"220\" \"217\" \"86\"  \"134\" \"84\"  \"106\" \"166\" \"124\" \"373\" \"289\"\n\n\n\n\n\nFor outliers “�������” , simply remove it.\n\nvalid_states &lt;- c(\"Absolutely_Correct\", \"Absolutely_Error\", \"Error1\", \"Error2\", \"Error3\", \"Error4\", \"Error6\", \"Error7\", \"Error8\", \"Error9\", \"Partially_Correct\")\n\n# 过滤数据，只保留 state 列中包含指定值的行\nsub_info &lt;- sub_info %&gt;%\n  filter(state %in% valid_states)\nunique(sub_info$state)\n\n [1] \"Absolutely_Correct\" \"Error1\"             \"Absolutely_Error\"  \n [4] \"Error6\"             \"Error4\"             \"Partially_Correct\" \n [7] \"Error2\"             \"Error3\"             \"Error7\"            \n[10] \"Error8\"             \"Error9\"            \n\n\nFor outliers “class” , replace with the highest frequency of the corresponding student_ID.\n\nreplace_class &lt;- function(df) {\n  df$class &lt;- as.character(df$class)\n  \n  class_indices &lt;- which(df$class == 'class')\n  \n  for (index in class_indices) {\n    student_id &lt;- df$student_ID[index]\n    student_classes &lt;- df$class[df$student_ID == student_id & df$class != 'class']\n    class_counts &lt;- table(student_classes)\n    \n    if (length(class_counts) &gt; 0) {\n      most_common_class &lt;- names(which.max(class_counts))\n      df$class[index] &lt;- most_common_class\n    }\n  }\n  \n  return(df)\n}\nsub_info &lt;- replace_class(sub_info)\nunique(sub_info$class)\n\n [1] \"Class1\"  \"Class2\"  \"Class3\"  \"Class4\"  \"Class5\"  \"Class6\"  \"Class7\" \n [8] \"Class8\"  \"Class9\"  \"Class10\" \"Class11\" \"Class12\" \"Class13\" \"Class14\"\n[15] \"Class15\"\n\n\nFor outliers ‘-’ and ‘–’, remove the corresponding rows.\n\nsub_info &lt;- sub_info %&gt;%\n  filter(!(timeconsume %in% c('-', '--')))\nunique(sub_info$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"18\"  \"61\"  \"7\"   \"59\"  \"10\" \n [13] \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\"  \"96\" \n [25] \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\"  \"21\" \n [37] \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"25\"  \"26\"  \"29\"  \"28\"  \"27\"  \"24\" \n [49] \"65\"  \"135\" \"63\"  \"103\" \"114\" \"258\" \"254\" \"85\"  \"66\"  \"69\"  \"90\"  \"132\"\n [61] \"173\" \"48\"  \"34\"  \"272\" \"38\"  \"113\" \"116\" \"32\"  \"76\"  \"22\"  \"190\" \"187\"\n [73] \"73\"  \"215\" \"123\" \"246\" \"146\" \"57\"  \"89\"  \"88\"  \"30\"  \"245\" \"75\"  \"285\"\n [85] \"70\"  \"400\" \"205\" \"36\"  \"164\" \"163\" \"162\" \"165\" \"266\" \"62\"  \"172\" \"143\"\n [97] \"184\" \"42\"  \"377\" \"160\" \"33\"  \"35\"  \"159\" \"182\" \"41\"  \"52\"  \"74\"  \"72\" \n[109] \"46\"  \"264\" \"81\"  \"153\" \"83\"  \"82\"  \"39\"  \"37\"  \"56\"  \"115\" \"55\"  \"286\"\n[121] \"275\" \"331\" \"280\" \"274\" \"269\" \"288\" \"271\" \"136\" \"117\" \"276\" \"277\" \"79\" \n[133] \"147\" \"44\"  \"350\" \"394\" \"45\"  \"315\" \"321\" \"302\" \"152\" \"47\"  \"53\"  \"51\" \n[145] \"307\" \"201\" \"43\"  \"109\" \"326\" \"49\"  \"77\"  \"71\"  \"385\" \"78\"  \"220\" \"217\"\n[157] \"86\"  \"134\" \"84\"  \"106\" \"166\" \"124\" \"373\" \"289\"\n\n\nSave the dataset and name it ‘sub_info.csv’\n\nwrite.csv(sub_info, 'data/sub_info.csv', row.names = FALSE)\nhead(sub_info)\n\n  index  class       time              state score\n1     0 Class1 1704209872 Absolutely_Correct     3\n2     1 Class1 1704209852 Absolutely_Correct     3\n3     2 Class1 1704209838 Absolutely_Correct     3\n4     3 Class1 1704208923 Absolutely_Correct     3\n5     4 Class1 1704208359 Absolutely_Correct     4\n6     5 Class1 1704208330             Error1     0\n                       title_ID                      method memory timeconsume\n1 Question_bumGRTJ0c8p4v5D6eHZa Method_Cj9Ya2R7fZd6xs1q5mNQ    320           3\n2 Question_62XbhBvJ8NUSnApgDL94 Method_gj1NLb4Jn7URf9K2kQPd    356           3\n3 Question_ZTbD7mxr2OUp8Fz6iNjy Method_5Q4KoXthUuYz3bvrTDFm    196           2\n4 Question_xqlJkmRaP0otZcX4fK3W Method_m8vwGkEZc3TSW2xqYUoR    308           2\n5 Question_FNg8X9v5zcbB1tQrxHR3 Method_Cj9Ya2R7fZd6xs1q5mNQ    320           3\n6 Question_FNg8X9v5zcbB1tQrxHR3 Method_gj1NLb4Jn7URf9K2kQPd      0           5\n            student_ID\n1 8b6d1125760bd3939b6e\n2 8b6d1125760bd3939b6e\n3 8b6d1125760bd3939b6e\n4 63eef37311aaac915a45\n5 5d89810b20079366fcc2\n6 5d89810b20079366fcc2\n\n\n\n\n2.2.3 Convert datetime\nThe time span is from August 31, 2023 to January 25, 2024, a total of 148 days. However, the content in column ‘time’ is actually in seconds. So we need to convert to datetime.\n\nsub_info &lt;- sub_info %&gt;%\n  mutate(day = wday(as.POSIXct(time, origin = \"1970-01-01\", tz = \"UTC\"), week_start = 1))\nunique(sub_info$day)\n\n[1] 2 1 6 5 4 7 3\n\n\n\n\n2.2.4 Match the unique title_ID with unique knowledge\nFrom the code below we can see some titles match multiple knowledge\n\ntitle_knowledge_check &lt;- tit_info %&gt;%\n  group_by(title_ID) %&gt;%\n  summarise(knowledge_count = n_distinct(knowledge)) %&gt;%\n  filter(knowledge_count &gt; 1)\n\nprint(title_knowledge_check)\n\n# A tibble: 5 × 2\n  title_ID                      knowledge_count\n  &lt;chr&gt;                                   &lt;int&gt;\n1 Question_QRm48lXxzdP7Tn1WgNOf               2\n2 Question_lU2wvHSZq7m43xiVroBc               2\n3 Question_oCjnFLbIs4Uxwek9rBpu               2\n4 Question_pVKXjZn0BkSwYcsa7C31               2\n5 Question_x2Fy7rZ3SwYl9jMQkpOD               2\n\n\nSince we don’t know when the students submit the questions, which knowledge they actually focus on, so we use the probability to match the knowledge.\n\ntitle_knowledge_count &lt;- tit_info %&gt;%\n  group_by(title_ID) %&gt;%\n  summarise(knowledge_list = list(unique(knowledge))) %&gt;%\n  mutate(knowledge = sapply(knowledge_list, function(x) ifelse(length(x) &gt; 0, x[1], NA)),\n         knowledge1 = sapply(knowledge_list, function(x) ifelse(length(x) &gt; 1, x[2], NA))) %&gt;%\n  select(-knowledge_list)\n\n# 合并知识信息到sub_info\nset.seed(123) # 确保结果可重复\nsub_info &lt;- sub_info %&gt;%\n  left_join(title_knowledge_count, by = \"title_ID\") %&gt;%\n  rowwise() %&gt;%\n  mutate(knowledge = ifelse(!is.na(knowledge1), \n                            sample(c(knowledge, knowledge1), 1), \n                            knowledge)) %&gt;%\n  ungroup() %&gt;%\n  select(-knowledge1)\n\n# 查看处理后的数据框前几行\nhead(sub_info)\n\n# A tibble: 6 × 12\n  index class     time state score title_ID method memory timeconsume student_ID\n  &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;     \n1     0 Class1  1.70e9 Abso…     3 Questio… Metho…    320 3           8b6d11257…\n2     1 Class1  1.70e9 Abso…     3 Questio… Metho…    356 3           8b6d11257…\n3     2 Class1  1.70e9 Abso…     3 Questio… Metho…    196 2           8b6d11257…\n4     3 Class1  1.70e9 Abso…     3 Questio… Metho…    308 2           63eef3731…\n5     4 Class1  1.70e9 Abso…     4 Questio… Metho…    320 3           5d89810b2…\n6     5 Class1  1.70e9 Erro…     0 Questio… Metho…      0 5           5d89810b2…\n# ℹ 2 more variables: day &lt;dbl&gt;, knowledge &lt;chr&gt;\n\n\nFinally, we need to calculate the average answering correct rate and average consuming time for each student.\n\nsub_info &lt;- sub_info %&gt;%\n  left_join(tit_info %&gt;% select(title_ID, score), by = \"title_ID\")\nsub_info &lt;- sub_info %&gt;%\n  mutate(rate = score.x / score.y) %&gt;%\n  select(-score.x, -score.y)\n\nhead(sub_info)\n\n# A tibble: 6 × 12\n  index class     time state title_ID method memory timeconsume student_ID   day\n  &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1     0 Class1  1.70e9 Abso… Questio… Metho…    320 3           8b6d11257…     2\n2     1 Class1  1.70e9 Abso… Questio… Metho…    356 3           8b6d11257…     2\n3     2 Class1  1.70e9 Abso… Questio… Metho…    196 2           8b6d11257…     2\n4     3 Class1  1.70e9 Abso… Questio… Metho…    308 2           63eef3731…     2\n5     4 Class1  1.70e9 Abso… Questio… Metho…    320 3           5d89810b2…     2\n6     5 Class1  1.70e9 Erro… Questio… Metho…      0 5           5d89810b2…     2\n# ℹ 2 more variables: knowledge &lt;chr&gt;, rate &lt;dbl&gt;\n\nwrite.csv(sub_info,'data/sub_info.csv',row.names = FALSE)\n\n\n\n2.2.5 Final data\nNow we merged with student information and rearrange the column for the further analysis.\n\n# 计算每个学生的平均rate\navg_rate &lt;- sub_info %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(average_rate = mean(rate, na.rm = TRUE))\n\n# 将day中的1 2 3 4 5计为'week'，6 7计为'weekend'\nsub_info &lt;- sub_info %&gt;%\n  mutate(week_category = ifelse(day %in% 1:5, \"week\", \"weekend\"))\n\n# 计算每个学生每种knowledge的百分比\nknowledge_percentage &lt;- sub_info %&gt;%\n  group_by(student_ID, knowledge) %&gt;%\n  summarise(counts = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(student_ID) %&gt;%\n  mutate(total_counts = sum(counts),\n         percentage = counts / total_counts) %&gt;%\n  select(student_ID, knowledge, percentage) %&gt;%\n  spread(key = knowledge, value = percentage, fill = 0)\n\n# 计算每个学生在week和weekend的百分比\nweekend_percentage &lt;- sub_info %&gt;%\n  group_by(student_ID, week_category) %&gt;%\n  summarise(counts = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(student_ID) %&gt;%\n  mutate(total_counts = sum(counts),\n         percentage = counts / total_counts) %&gt;%\n  select(student_ID, week_category, percentage) %&gt;%\n  spread(key = week_category, value = percentage, fill = 0)\n\n# 合并学生信息和计算结果\nfinal_data &lt;- stu_info %&gt;%\n  select(-index) %&gt;%\n  left_join(avg_rate, by = \"student_ID\") %&gt;%\n  left_join(sub_info %&gt;% select(student_ID, -day) %&gt;% distinct(), by = \"student_ID\") %&gt;%\n  left_join(knowledge_percentage, by = \"student_ID\") %&gt;%\n  left_join(weekend_percentage, by = \"student_ID\")\n\n\n# 查看结果\nhead(final_data)\n\n            student_ID    sex age  major average_rate      b3C9s      g7R2j\n1 8b6d1125760bd3939b6e female  24 J23517    0.5187266 0.07865169 0.06741573\n2 63eef37311aaac915a45 female  21 J87654    0.1597715 0.01654846 0.05437352\n3 5d89810b20079366fcc2 female  23 J87654    0.5679012 0.06172840 0.04938272\n4 47eeab842793b09300c3 female  21 J78901    0.3252688 0.05913978 0.06451613\n5 a8eea517e36b20757b2e   male  22 J40192    0.2543860 0.01754386 0.05701754\n6 e89cdaa3f159a0d3f55e   male  19 J57489    0.1880229 0.19022457 0.11756935\n        k4W1c     m3D1v      r8S3g      s8Y2f      t5V9e      y9W5d      week\n1 0.022471910 0.2247191 0.32584270 0.02247191 0.10112360 0.15730337 1.0000000\n2 0.000000000 0.4018913 0.07801418 0.05673759 0.12293144 0.26950355 0.7825059\n3 0.024691358 0.1481481 0.40740741 0.00000000 0.06172840 0.24691358 0.8271605\n4 0.010752688 0.3655914 0.16666667 0.01075269 0.09677419 0.22580645 0.8817204\n5 0.017543860 0.3684211 0.25438596 0.00000000 0.23684211 0.04824561 0.6973684\n6 0.007926024 0.1955086 0.09511229 0.05548217 0.05812417 0.28005284 0.6459709\n    weekend\n1 0.0000000\n2 0.2174941\n3 0.1728395\n4 0.1182796\n5 0.3026316\n6 0.3540291\n\nwrite.csv(final_data, 'data/final_data.csv')"
  },
  {
    "objectID": "team2/final.html#time",
    "href": "team2/final.html#time",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.1 time",
    "text": "3.1 time\nweek = 1\nweekend = 0\n\n# 读取数据集\ndata &lt;- read.csv(\"data/final_data.csv\")\n\n# 创建新的dataframe\ntime &lt;- data %&gt;%\n  select(student_ID, week, weekend)\n\n# 查看结果\nhead(time)\n\n            student_ID      week   weekend\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000\n2 63eef37311aaac915a45 0.7825059 0.2174941\n3 5d89810b20079366fcc2 0.8271605 0.1728395\n4 47eeab842793b09300c3 0.8817204 0.1182796\n5 a8eea517e36b20757b2e 0.6973684 0.3026316\n6 e89cdaa3f159a0d3f55e 0.6459709 0.3540291"
  },
  {
    "objectID": "team2/final.html#question",
    "href": "team2/final.html#question",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.2 question",
    "text": "3.2 question\n\n# 读取第二个数据集\ndata2 &lt;- read.csv(\"data/cleaned_data.csv\")\n\n# 提取每个学生提交记录中出现频率最高的title\ntitle_frequency &lt;- data2 %&gt;%\n  group_by(student_ID, title_ID) %&gt;%\n  summarise(frequency = n(), .groups = 'drop') %&gt;%\n  arrange(student_ID, desc(frequency)) %&gt;%\n  distinct(student_ID, .keep_all = TRUE) %&gt;%\n  select(student_ID, title_ID)\n\n# 合并数据框 time 和 title_frequency\nquestion &lt;- time %&gt;%\n  left_join(title_frequency, by = \"student_ID\")\n\n# 查看结果\nhead(question)\n\n            student_ID      week   weekend                      title_ID\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000 Question_q7OpB2zCMmW9wS8uNt3H\n2 63eef37311aaac915a45 0.7825059 0.2174941 Question_QRm48lXxzdP7Tn1WgNOf\n3 5d89810b20079366fcc2 0.8271605 0.1728395 Question_q7OpB2zCMmW9wS8uNt3H\n4 47eeab842793b09300c3 0.8817204 0.1182796 Question_4nHcauCQ0Y6Pm8DgKlLo\n5 a8eea517e36b20757b2e 0.6973684 0.3026316 Question_4nHcauCQ0Y6Pm8DgKlLo\n6 e89cdaa3f159a0d3f55e 0.6459709 0.3540291 Question_FNg8X9v5zcbB1tQrxHR3\n\n\n\n# 读取编码表\ntitle_encoding &lt;- read.csv(\"data/title_encode.csv\")\n\n# 重命名编码表列\ncolnames(title_encoding) &lt;- c(\"title_ID\", \"title_pre\")\n\n# 合并title_frequency和title_encoding\ntitle_frequency_encoded &lt;- title_frequency %&gt;%\n  left_join(title_encoding, by = \"title_ID\") %&gt;%\n  select(student_ID, title_pre)\n\n# 合并数据框 time 和 title_frequency_encoded\nquestion &lt;- time %&gt;%\n  left_join(title_frequency_encoded, by = \"student_ID\")\n\n# 查看结果\nhead(question)\n\n            student_ID      week   weekend title_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3\n3 63eef37311aaac915a45 0.7825059 0.2174941        21\n4 63eef37311aaac915a45 0.7825059 0.2174941        22\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3"
  },
  {
    "objectID": "team2/final.html#method",
    "href": "team2/final.html#method",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.3 method",
    "text": "3.3 method\n\n# 提取每个学生提交记录中出现频率最高的title\nmethod_frequency &lt;- data2 %&gt;%\n  group_by(student_ID, method) %&gt;%\n  summarise(frequency = n(), .groups = 'drop') %&gt;%\n  arrange(student_ID, desc(frequency)) %&gt;%\n  distinct(student_ID, .keep_all = TRUE) %&gt;%\n  select(student_ID, method)\n\n# 合并数据框 time 和 title_frequency\nmethod &lt;- question %&gt;%\n  left_join(method_frequency, by = \"student_ID\")\n\n# 查看结果\nhead(method)\n\n            student_ID      week   weekend title_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3\n3 63eef37311aaac915a45 0.7825059 0.2174941        21\n4 63eef37311aaac915a45 0.7825059 0.2174941        22\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3\n                       method\n1 Method_5Q4KoXthUuYz3bvrTDFm\n2 Method_5Q4KoXthUuYz3bvrTDFm\n3 Method_m8vwGkEZc3TSW2xqYUoR\n4 Method_m8vwGkEZc3TSW2xqYUoR\n5 Method_gj1NLb4Jn7URf9K2kQPd\n6 Method_gj1NLb4Jn7URf9K2kQPd\n\n\n\n# 读取编码表\nmethod_encoding &lt;- read.csv(\"data/method_encode.csv\")\n\n# 重命名编码表列\ncolnames(method_encoding) &lt;- c(\"method\", \"method_pre\")\n\n# 合并title_frequency和title_encoding\nmethod_frequency_encoded &lt;- method_frequency %&gt;%\n  left_join(method_encoding, by = \"method\") %&gt;%\n  select(student_ID, method_pre)\n\n# 合并数据框 time 和 title_frequency_encoded\nmethod &lt;- question %&gt;%\n  left_join(method_frequency_encoded, by = \"student_ID\")\n\n# 查看结果\nhead(method)\n\n            student_ID      week   weekend title_pre method_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2          1\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3          1\n3 63eef37311aaac915a45 0.7825059 0.2174941        21          2\n4 63eef37311aaac915a45 0.7825059 0.2174941        22          2\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2          3\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3          3\n\nwrite.csv(method, 'data/method.csv', row.names = FALSE)"
  },
  {
    "objectID": "team2/final.html#knowledge",
    "href": "team2/final.html#knowledge",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.3 knowledge",
    "text": "3.3 knowledge\n\n# 读取第一个数据集\nfinal_data &lt;- read.csv(\"data/final_data.csv\")\n\n# 指定第7到第14列的列名\ncolumns_of_interest &lt;- c('b3C9s', 'g7R2j', 'k4W1c', 'm3D1v', 'r8S3g', 's8Y2f', 't5V9e', 'y9W5d')\n\n# 查找每个学生的指定列中数值最高的列名称\nfinal_data &lt;- final_data %&gt;%\n  rowwise() %&gt;%\n  mutate(knowledge_pre = columns_of_interest[which.max(c_across(all_of(columns_of_interest)))])\n\n# 读取 method 数据集\nmethod &lt;- read.csv(\"data/method.csv\")\n\n# 合并 final_data 和 method\nfinal_data_with_knowledge &lt;- final_data %&gt;%\n  select(student_ID, knowledge_pre)\n\nknowledge &lt;- method %&gt;%\n  left_join(final_data_with_knowledge, by = \"student_ID\")\n\n# 查看结果\nhead(knowledge)\n\n            student_ID      week   weekend title_pre method_pre knowledge_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2          1         r8S3g\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3          1         r8S3g\n3 63eef37311aaac915a45 0.7825059 0.2174941        21          2         m3D1v\n4 63eef37311aaac915a45 0.7825059 0.2174941        22          2         m3D1v\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2          3         r8S3g\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3          3         r8S3g\n\n\n\n# 读取编码表\n# 读取数据集\nknowledge_encode &lt;- read.csv(\"data/knowledge_encode.csv\")\n\n# 重命名编码表的列\ncolnames(knowledge_encode) &lt;- c(\"knowledge_pre\", \"encoded_knowledge_pre\")\n\ncolnames(knowledge)\n\n[1] \"student_ID\"    \"week\"          \"weekend\"       \"title_pre\"    \n[5] \"method_pre\"    \"knowledge_pre\"\n\ncolnames(knowledge_encode)\n\n[1] \"knowledge_pre\"         \"encoded_knowledge_pre\"\n\n# 合并知识偏好和编码表\nknowledge_encoded &lt;- knowledge %&gt;%\n  left_join(knowledge_encode, by = \"knowledge_pre\") %&gt;%\n  select(-knowledge_pre) %&gt;%\n  rename(knowledge_pre = encoded_knowledge_pre)\n\n\n# 查看结果\nhead(knowledge_encoded)\n\n            student_ID      week   weekend title_pre method_pre knowledge_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2          1             5\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3          1             5\n3 63eef37311aaac915a45 0.7825059 0.2174941        21          2             4\n4 63eef37311aaac915a45 0.7825059 0.2174941        22          2             4\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2          3             5\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3          3             5\n\nwrite.csv(knowledge_encoded, 'data/knowledge.csv', row.names = FALSE)"
  },
  {
    "objectID": "team2/final.html#correct-rate-trend-score",
    "href": "team2/final.html#correct-rate-trend-score",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.4 Correct rate trend score",
    "text": "3.4 Correct rate trend score\n\n# 读取数据集\ncleaned_data &lt;- read.csv(\"data/cleaned_data.csv\")\nknowledge &lt;- read.csv(\"data/knowledge.csv\")\ntitle_info &lt;- read.csv(\"data/Data_TitleInfo.csv\")\ntitle_encode &lt;- read.csv(\"data/title_encode.csv\")\n\n# 重命名title_encode的列\ncolnames(title_encode) &lt;- c(\"title_ID\", \"encoded_title\")\n\n# 合并knowledge和title_encode，找到每位学生的题目名称\nknowledge &lt;- knowledge %&gt;%\n  left_join(title_encode, by = c(\"title_pre\" = \"encoded_title\")) %&gt;%\n  rename(title = title_pre, title_ID = title_ID)\n\n# 计算correct_rate并添加到cleaned_data\ncleaned_data &lt;- cleaned_data %&gt;%\n  left_join(title_info %&gt;% select(title_ID, score), by = \"title_ID\", suffix = c(\"\", \"_total\")) %&gt;%\n  mutate(correct_rate = score / score_total)\n\n# 初始化结果数据框\ntrend_scores &lt;- data.frame()\n\n# 定义计算正确率趋势的函数\ncalculate_trend &lt;- function(data) {\n  if (nrow(data) &lt; 2) {\n    return(NA)\n  }\n  model &lt;- lm(correct_rate ~ attempt, data = data)\n  return(coef(model)[2]) # 返回斜率\n}\n\n# 遍历每位学生\nfor (I in 1:nrow(knowledge)) {\n  student_id &lt;- knowledge$student_ID[I]\n  title_id &lt;- knowledge$title_ID[I]\n  \n  # 找到该学生在该题目的所有答题记录并排序\n  student_data &lt;- cleaned_data %&gt;%\n    filter(student_ID == student_id, title_ID == title_id) %&gt;%\n    arrange(time)\n  \n  # 增加attempt列\n  student_data &lt;- student_data %&gt;%\n    mutate(attempt = row_number() - 1)\n  \n  # 计算该学生在该题目的正确率趋势\n  trend &lt;- calculate_trend(student_data)\n  \n  # 将结果存储在结果数据框中\n  trend_scores &lt;- rbind(trend_scores, data.frame(student_ID = student_id, title_ID = title_id, trend = trend))\n}\n\n# 将title_info中的分数信息合并到结果数据框中\ntrend_scores &lt;- trend_scores %&gt;%\n  left_join(title_info %&gt;% select(title_ID, score), by = \"title_ID\") %&gt;%\n  mutate(correct_rate_trend_score = trend * score)\n\n# 查看结果\nhead(trend_scores)\n\n            student_ID                      title_ID       trend score\n1 8b6d1125760bd3939b6e Question_q7OpB2zCMmW9wS8uNt3H 0.041176471     1\n2 8b6d1125760bd3939b6e Question_q7OpB2zCMmW9wS8uNt3H 0.041176471     1\n3 8b6d1125760bd3939b6e Question_q7OpB2zCMmW9wS8uNt3H 0.041176471     1\n4 8b6d1125760bd3939b6e Question_q7OpB2zCMmW9wS8uNt3H 0.041176471     1\n5 63eef37311aaac915a45 Question_QRm48lXxzdP7Tn1WgNOf 0.001637587     3\n6 63eef37311aaac915a45 Question_QRm48lXxzdP7Tn1WgNOf 0.001637587     3\n  correct_rate_trend_score\n1              0.041176471\n2              0.041176471\n3              0.041176471\n4              0.041176471\n5              0.004912762\n6              0.004912762\n\n\n\nknowledge &lt;- read.csv(\"data/knowledge.csv\")\n\n# 确保每个 student_ID 只有一个 correct_rate_trend_score，取平均值\ntrend_scores_agg &lt;- trend_scores %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(trend_score = mean(correct_rate_trend_score, na.rm = TRUE))\n\n# 合并到 knowledge 数据框中\ncluster_data &lt;- knowledge %&gt;%\n  left_join(trend_scores_agg, by = 'student_ID')\n\n# 查看结果\nhead(cluster_data)\n\n            student_ID      week   weekend title_pre method_pre knowledge_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2          1             5\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3          1             5\n3 63eef37311aaac915a45 0.7825059 0.2174941        21          2             4\n4 63eef37311aaac915a45 0.7825059 0.2174941        22          2             4\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2          3             5\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3          3             5\n  trend_score\n1 0.041176471\n2 0.041176471\n3 0.004912762\n4 0.004912762\n5 0.019130435\n6 0.019130435\n\n\n\n# 读取数据集\ncleaned_data &lt;- read.csv(\"data/cleaned_data.csv\")\nknowledge &lt;- read.csv(\"data/knowledge.csv\")\ntitle_info &lt;- read.csv(\"data/Data_TitleInfo.csv\")\ntitle_encode &lt;- read.csv(\"data/title_encode.csv\")\n\n# 重命名title_encode的列\ncolnames(title_encode) &lt;- c(\"title_ID\", \"encoded_title\")\n\n# 合并knowledge和title_encode，找到每位学生的题目名称\nknowledge &lt;- knowledge %&gt;%\n  left_join(title_encode, by = c(\"title_pre\" = \"encoded_title\")) %&gt;%\n  rename(title = title_pre, title_ID = title_ID)\n\n# 计算correct_rate并添加到cleaned_data\ncleaned_data &lt;- cleaned_data %&gt;%\n  left_join(title_info %&gt;% select(title_ID, score), by = \"title_ID\", suffix = c(\"\", \"_total\")) %&gt;%\n  mutate(correct_rate = score / score_total)\n\n# 初始化结果数据框\ntrend_scores &lt;- data.frame()\n\n# 定义计算正确率趋势的函数\ncalculate_trend &lt;- function(data) {\n  if (nrow(data) &lt; 2) {\n    return(NA)\n  }\n  model &lt;- lm(correct_rate ~ attempt, data = data)\n  return(coef(model)[2]) # 返回斜率\n}\n# 遍历每位学生\nplot_data &lt;- data.frame()\n\nfor (I in 1:nrow(knowledge)) {\n  student_id &lt;- knowledge$student_ID[I]\n  title_id &lt;- knowledge$title_ID[I]\n  \n  # 找到该学生在该题目的所有答题记录并排序\n  student_data &lt;- cleaned_data %&gt;%\n    filter(student_ID == student_id, title_ID == title_id) %&gt;%\n    arrange(time)\n  \n  # 增加attempt列\n  student_data &lt;- student_data %&gt;%\n    mutate(attempt = row_number() - 1)\n  \n  # 保留尝试次数大于10的学生数据\n  if (nrow(student_data) &gt; 10) {\n    plot_data &lt;- rbind(plot_data, student_data)\n  }\n}\n\n# 绘制散点图和趋势线\nggplot(plot_data, aes(x = attempt, y = correct_rate)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts\",\n       x = \"Attempt\",\n       y = \"Correct Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 如果需要按学生分组进行绘图\nggplot(plot_data, aes(x = attempt, y = correct_rate, color = student_ID)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\") +\n  labs(title = \"Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts\",\n       x = \"Attempt\",\n       y = \"Correct Rate\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") # 如果学生太多，可以移除图例\n\n\n\n\n\n\n\n# 将 cluster_data 保存成 .rds 文件\nsaveRDS(plot_data, \"data/shinydata/trend_score.rds\")"
  },
  {
    "objectID": "team2/final.html#sex-age-major",
    "href": "team2/final.html#sex-age-major",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.5 sex, age, major",
    "text": "3.5 sex, age, major\n\n# 读取数据集\nstudent_info &lt;- read.csv(\"data/Data_StudentInfo.csv\")\n\n# 合并 student_info 中的 sex, age, major 列到 cluster_data 中\ncluster_data &lt;- cluster_data %&gt;%\n  left_join(student_info %&gt;% select(student_ID, sex, age, major), by = \"student_ID\")\n\n# 查看结果\nhead(cluster_data)\n\n            student_ID      week   weekend title_pre method_pre knowledge_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2          1             5\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3          1             5\n3 63eef37311aaac915a45 0.7825059 0.2174941        21          2             4\n4 63eef37311aaac915a45 0.7825059 0.2174941        22          2             4\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2          3             5\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3          3             5\n  trend_score    sex age  major\n1 0.041176471 female  24 J23517\n2 0.041176471 female  24 J23517\n3 0.004912762 female  21 J87654\n4 0.004912762 female  21 J87654\n5 0.019130435 female  23 J87654\n6 0.019130435 female  23 J87654\n\n\nfemale = 1\nmale = 0\n\n# 读取编码表\nmajor_encode &lt;- read.csv(\"data/major_encode.csv\")\n\n# 对 sex 进行编码\ncluster_data &lt;- cluster_data %&gt;%\n  mutate(sex = ifelse(sex == \"female\", 1, 0))\n\n# 合并 major 编码\nmajor_encode &lt;- major_encode %&gt;%\n  rename(major_name = major1, major_code = major_encode)\n\ncluster_data &lt;- cluster_data %&gt;%\n  left_join(major_encode, by = c(\"major\" = \"major_name\")) %&gt;%\n  select(-major) %&gt;%\n  rename(major = major_code) \n\n# 查看结果\nhead(cluster_data)\n\n            student_ID      week   weekend title_pre method_pre knowledge_pre\n1 8b6d1125760bd3939b6e 1.0000000 0.0000000         2          1             5\n2 8b6d1125760bd3939b6e 1.0000000 0.0000000         3          1             5\n3 63eef37311aaac915a45 0.7825059 0.2174941        21          2             4\n4 63eef37311aaac915a45 0.7825059 0.2174941        22          2             4\n5 5d89810b20079366fcc2 0.8271605 0.1728395         2          3             5\n6 5d89810b20079366fcc2 0.8271605 0.1728395         3          3             5\n  trend_score sex age major\n1 0.041176471   1  24     1\n2 0.041176471   1  24     1\n3 0.004912762   1  21     2\n4 0.004912762   1  21     2\n5 0.019130435   1  23     2\n6 0.019130435   1  23     2\n\n# 将 cluster_data 保存成 .rds 文件\nsaveRDS(cluster_data, \"data/cluster_data.rds\")"
  },
  {
    "objectID": "team2/final.html#k-means-clustering",
    "href": "team2/final.html#k-means-clustering",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "4.1 k-means clustering",
    "text": "4.1 k-means clustering\n\n# 检查数据中的NA值\ncolSums(is.na(cluster_data))\n\n   student_ID          week       weekend     title_pre    method_pre \n            0             0             0             0             0 \nknowledge_pre   trend_score           sex           age         major \n            0             3             0             0             0 \n\n\n\n# 检查数据中的NA值\nsum(is.na(cluster_data[, 2:10]))\n\n[1] 3\n\n# 处理NA值，使用中位数填补\ncluster_data_clean &lt;- cluster_data %&gt;%\n  mutate(across(2:10, ~ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\nwrite.csv(cluster_data_clean, 'data/cluster_data.csv', row.names = FALSE)\n# 计算相关矩阵\nSLM.cor &lt;- cor(cluster_data_clean[, 2:10], use = \"complete.obs\")\n\n# 绘制相关图\ncorrplot(SLM.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order = \"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\n# 使用 ggstatsplot 绘制相关矩阵图\nggstatsplot::ggcorrmat(\n  data = cluster_data_clean, \n  cor.vars = 2:9\n)\n\n\n\n\n\n\n\ncolSums(is.na(cluster_data_clean))\n\n   student_ID          week       weekend     title_pre    method_pre \n            0             0             0             0             0 \nknowledge_pre   trend_score           sex           age         major \n            0             0             0             0             0 \n\n\n\n# Exclude non-numeric columns\ncluster_numeric &lt;- cluster_data_clean %&gt;%\n  select(-student_ID)\n\n# Function to compute silhouette widths\nsilhouette_analysis &lt;- function(data, max_clusters) {\n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(kmeans_result$cluster, dist(data))\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 18\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(cluster_numeric, max_clusters)\n\n# Plot the average silhouette widths\nplot(1:max_clusters, avg_sil_widths, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\n\n\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 18\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(cluster_numeric, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")\n\n\n\n\n\n\n\n\n\n# Drop the student_ID column\nclustering_data &lt;- cluster_data_clean %&gt;%\n  select(-student_ID)\n\n# Standardize the data\nclustering_data_scaled &lt;- scale(clustering_data)\n\n# Perform k-means clustering\nset.seed(123)  # For reproducibility\nkmeans_result &lt;- kmeans(clustering_data_scaled, centers = 2, nstart = 25)\n\n# Add the cluster assignments to the original data\ncluster_data_clean$cluster &lt;- kmeans_result$cluster\n\n\n# Perform PCA\npca_result &lt;- prcomp(cluster_data_clean[-1], scale. = TRUE)\n\n# Get PCA scores\npca_scores &lt;- as.data.frame(predict(pca_result))\n\n# Add cluster information to PCA scores\npca_scores$cluster &lt;- factor(cluster_data_clean$cluster)\n\n# Plot PCA results with cluster color coding\npca_plot &lt;- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_discrete(name = \"Cluster\") +\n  labs(x = \"Principal Component 1\", y = \"Principal Component 2\",\n       title = \"PCA Plot of Clusters\") +\n  theme_minimal()\n\n# Display the plot\npca_plot\n\n\n\n\n\n\n\n\n\ncluster_factor &lt;- cluster_data_clean\ncluster_factor$cluster &lt;- as.character(cluster_factor$cluster)\n\nggparcoord(data = cluster_factor, \n           columns = c(2:10), \n           groupColumn = 11,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30, size = 20))\n\n\n\n\n\n\n\n\n\nggparcoord(data = cluster_factor, \n           columns = c(2:10), \n           groupColumn = 11,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30, size = 20))\n\n\n\n\n\n\n\nggparcoord(data = cluster_factor, \ncolumns = c(2:10), \n           groupColumn = 11,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n  facet_wrap(~ cluster)+\n  theme(axis.text.x = element_text(angle = 30, size = 20))\n\n\n\n\n\n\n\n\n\n# 将分类特征转换为因子类型\ncategorical_columns &lt;- c(\"sex\", \"age\", \"knowledge_pre\", \"major\", \"method_pre\", \"title_pre\", \"cluster\")\ncluster_data_clean[categorical_columns] &lt;- lapply(cluster_data_clean[categorical_columns], as.factor)\n\n# 创建alluvial plot\nggplot_alluvial &lt;- ggplot(cluster_data_clean,\n       aes(axis1 = sex, axis2 = age, axis3 = knowledge_pre, axis4 = major, axis5 = method_pre, axis6 = title_pre, axis7 = cluster,\n           y = ..count..)) +\n  scale_x_discrete(limits = c(\"Sex\", \"Age\", \"Knowledge_pre\", \"Major\", \"Method_pre\", \"Title_pre\", \"Cluster\"), expand = c(.1, .1)) +\n  geom_alluvium(aes(fill = cluster, text = cluster), width = 0.25) +\n  geom_stratum(aes(text = after_stat(stratum)), width = 0.25) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Students Data Distribution and Flow\",\n       y = \"Number of Students\",\n       x = \"\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1,size = 10))\n\n# 转换为互动图表并添加悬停信息\ninteractive_plot &lt;- ggplotly(ggplot_alluvial, tooltip = \"text\") %&gt;% layout(showlegend = FALSE)\n\n# 显示互动图表\ninteractive_plot\n\n\n\n\n\n\ncluster_factor1 &lt;- select(cluster_factor, c(1, 2:10))\ncluster_factor_matrix &lt;- data.matrix(cluster_factor1)\n\ncluster_data_d &lt;- dist(normalize(cluster_factor_matrix[, -c(1)]), method = \"euclidean\")\ndend_expend(cluster_data_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6745927\n2      unknown        ward.D2 0.6978557\n3      unknown         single 0.6625850\n4      unknown       complete 0.5503565\n5      unknown        average 0.7300015\n6      unknown       mcquitty 0.7079792\n7      unknown         median 0.5642007\n8      unknown       centroid 0.4528982\n\n\n\ncluster_data_clust &lt;- hclust(cluster_data_d, method = \"average\")\nnum_k &lt;- find_k(cluster_data_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(cluster_factor_matrix[, -c(1)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 2,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,          \n          main=\"Students' Learning Mode Clustering \\nDataTransformation using Normalise Method\",\n          xlab = \"Student_IDs\",\n          ylab = \"Learning Behaior Pattern\"\n)"
  },
  {
    "objectID": "team2/final.html#k-prototype-clustering",
    "href": "team2/final.html#k-prototype-clustering",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "4.2 k-prototype clustering",
    "text": "4.2 k-prototype clustering\n\n# 安装并加载必要的包\nif (!require(\"clustMixType\")) install.packages(\"clustMixType\")\nif (!require(\"cluster\")) install.packages(\"cluster\")\nif (!require(\"factoextra\")) install.packages(\"factoextra\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\n\nlibrary(clustMixType)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(dplyr)\n\n# 读取数据集\ncluster_data &lt;- read.csv(\"data/cluster_data.csv\")\nknowledge_encode &lt;- read.csv(\"data/knowledge_encode.csv\")\nmajor_encode &lt;- read.csv(\"data/major_encode.csv\")\nmethod_encode &lt;- read.csv(\"data/method_encode.csv\")\ntitle_encode &lt;- read.csv(\"data/title_encode.csv\")\n\n# 将编码转换为名称\ncluster_data &lt;- cluster_data %&gt;%\n  left_join(knowledge_encode, by = c(\"knowledge_pre\" = \"knowledge_encode\")) %&gt;%\n  left_join(major_encode, by = c(\"major\" = \"major_encode\")) %&gt;%\n  left_join(method_encode, by = c(\"method_pre\" = \"method_encode\")) %&gt;%\n  left_join(title_encode, by = c(\"title_pre\" = \"title_encode\")) %&gt;%\n  select(-knowledge_pre, -major, -method_pre, -title_pre)\n\n# 对 sex 进行编码\ncluster_data &lt;- cluster_data %&gt;%\n  mutate(sex = ifelse(sex == 1, \"female\", \"male\"))\n\n# 排除字符类型的列\ncluster_data_for_daisy &lt;- cluster_data %&gt;%\n  select(-student_ID)\n\n# 手动定义数值和分类特征的列索引\nnumerical_indices &lt;- c(1, 2, 3, 5)  # 手动定义数值列\ncategorical_indices &lt;- c( 4, 6, 7, 8, 9)  # 手动定义分类列\n\n# 将分类特征转换为因子类型\ncluster_data_for_daisy[, categorical_indices] &lt;- lapply(cluster_data_for_daisy[, categorical_indices], as.factor)\n\n# 标准化数值特征\ncluster_data_for_daisy[, numerical_indices] &lt;- scale(cluster_data_for_daisy[, numerical_indices])\n\n\n\n# 可视化trend score的分布情况\nggplot(cluster_data_for_daisy, aes(x = trend_score)) +\n  geom_histogram(binwidth = 0.05, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Distribution of Trend Score\", x = \"Trend Score\", y = \"Frequency\")\n\n\n\n\n\n\n\n# 计算Gower's距离\ngower_dist &lt;- daisy(cluster_data_for_daisy, metric = \"gower\")\n\n# 将Gower's距离矩阵转换为dist对象\ngower_dist &lt;- as.dist(gower_dist)\n\n# 使用K-Prototypes算法进行聚类\n# 设定聚类数k\nk &lt;- 2\nkprototypes_result &lt;- kproto(cluster_data_for_daisy[, c(categorical_indices, numerical_indices)], k, nstart = 10)\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\nEstimated lambda: 1.352633 \n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# NAs in variables:\n        sex   knowledge      major1      method       title        week \n          0           0           0           0           0           0 \n    weekend trend_score         age \n          0           0           0 \n0 observation(s) with NAs.\n\n0 observation(s) with NAs.\n\n# 聚类结果\ncluster_assignments &lt;- kprototypes_result$cluster\n\n# 检查聚类结果的长度\nlength(cluster_assignments)\n\n[1] 1623\n\n# 将聚类结果添加到原始数据\nif(length(cluster_assignments) == nrow(cluster_data)) {\n  cluster_data$cluster &lt;- factor(cluster_assignments)\n} else {\n  stop(\"The length of cluster assignments does not match the number of rows in the data.\")\n}\n\n\n# 将需要的列转换为因子类型\ncategorical_columns &lt;- c(\"sex\", \"age\", \"knowledge\", \"major1\", \"method\", \"title\", \"cluster\")\ncluster_data[categorical_columns] &lt;- lapply(cluster_data[categorical_columns], as.factor)\n\n# 计算每个分类变量中cluster 1和2的占比\nget_cluster_percentage &lt;- function(data, column) {\n  data %&gt;%\n    group_by(!!sym(column), cluster) %&gt;%\n    summarise(count = n()) %&gt;%\n    mutate(percentage = count / sum(count) * 100) %&gt;%\n    filter(cluster %in% c(1, 2))\n}\n\n# 获取每个分类变量的占比数据框\nsex_cluster_percentage &lt;- get_cluster_percentage(cluster_data, \"sex\")\nage_cluster_percentage &lt;- get_cluster_percentage(cluster_data, \"age\")\nknowledge_cluster_percentage &lt;- get_cluster_percentage(cluster_data, \"knowledge\")\nmajor_cluster_percentage &lt;- get_cluster_percentage(cluster_data, \"major1\")\nmethod_cluster_percentage &lt;- get_cluster_percentage(cluster_data, \"method\")\ntitle_cluster_percentage &lt;- get_cluster_percentage(cluster_data, \"title\")\n\n# 打印结果\nprint(sex_cluster_percentage)\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex    cluster count percentage\n  &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 female 1         270       33.5\n2 female 2         536       66.5\n3 male   1         461       56.4\n4 male   2         356       43.6\n\nprint(age_cluster_percentage)\n\n# A tibble: 14 × 4\n# Groups:   age [7]\n   age   cluster count percentage\n   &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 18    1         111       52.4\n 2 18    2         101       47.6\n 3 19    1         119       51.1\n 4 19    2         114       48.9\n 5 20    1         100       47.2\n 6 20    2         112       52.8\n 7 21    1         110       44.2\n 8 21    2         139       55.8\n 9 22    1         104       44.3\n10 22    2         131       55.7\n11 23    1          93       37.3\n12 23    2         156       62.7\n13 24    1          94       40.3\n14 24    2         139       59.7\n\nprint(knowledge_cluster_percentage)\n\n# A tibble: 12 × 4\n# Groups:   knowledge [6]\n   knowledge cluster count percentage\n   &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 b3C9s     1           2       66.7\n 2 b3C9s     2           1       33.3\n 3 g7R2j     1          26       46.4\n 4 g7R2j     2          30       53.6\n 5 m3D1v     1         366       58.5\n 6 m3D1v     2         260       41.5\n 7 r8S3g     1         145       42.3\n 8 r8S3g     2         198       57.7\n 9 t5V9e     1          22       48.9\n10 t5V9e     2          23       51.1\n11 y9W5d     1         170       30.9\n12 y9W5d     2         380       69.1\n\nprint(major_cluster_percentage)\n\n# A tibble: 10 × 4\n# Groups:   major1 [5]\n   major1 cluster count percentage\n   &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 J23517 1         149       46.9\n 2 J23517 2         169       53.1\n 3 J40192 1         157       47.4\n 4 J40192 2         174       52.6\n 5 J57489 1         180       60  \n 6 J57489 2         120       40  \n 7 J78901 1         117       32.9\n 8 J78901 2         239       67.1\n 9 J87654 1         128       40.3\n10 J87654 2         190       59.7\n\nprint(method_cluster_percentage)\n\n# A tibble: 10 × 4\n# Groups:   method [5]\n   method                      cluster count percentage\n   &lt;fct&gt;                       &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Method_5Q4KoXthUuYz3bvrTDFm 1         197       56.9\n 2 Method_5Q4KoXthUuYz3bvrTDFm 2         149       43.1\n 3 Method_BXr9AIsPQhwNvyGdZL57 1         105       32.2\n 4 Method_BXr9AIsPQhwNvyGdZL57 2         221       67.8\n 5 Method_Cj9Ya2R7fZd6xs1q5mNQ 1         147       44.1\n 6 Method_Cj9Ya2R7fZd6xs1q5mNQ 2         186       55.9\n 7 Method_gj1NLb4Jn7URf9K2kQPd 1         129       41.6\n 8 Method_gj1NLb4Jn7URf9K2kQPd 2         181       58.4\n 9 Method_m8vwGkEZc3TSW2xqYUoR 1         153       49.7\n10 Method_m8vwGkEZc3TSW2xqYUoR 2         155       50.3\n\nprint(title_cluster_percentage)\n\n# A tibble: 75 × 4\n# Groups:   title [38]\n   title                         cluster count percentage\n   &lt;fct&gt;                         &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Question_3MwAFlmNO8EKrpY5zjUd 1          99       44.4\n 2 Question_3MwAFlmNO8EKrpY5zjUd 2         124       55.6\n 3 Question_3oPyUzDmQtcMfLpGZ0jW 1          53       49.1\n 4 Question_3oPyUzDmQtcMfLpGZ0jW 2          55       50.9\n 5 Question_4nHcauCQ0Y6Pm8DgKlLo 1           8       34.8\n 6 Question_4nHcauCQ0Y6Pm8DgKlLo 2          15       65.2\n 7 Question_5fgqjSBwTPG7KUV3it6O 1         106       48.2\n 8 Question_5fgqjSBwTPG7KUV3it6O 2         114       51.8\n 9 Question_62XbhBvJ8NUSnApgDL94 1           6       46.2\n10 Question_62XbhBvJ8NUSnApgDL94 2           7       53.8\n# ℹ 65 more rows\n\n\n\n# 创建alluvial plot\nggplot_alluvial &lt;- ggplot(cluster_data,\n       aes(axis1 = sex, axis2 = age, axis3 = knowledge, axis4 = major1, axis5 = method, axis6 = title, axis7 = cluster,\n           y = ..count..)) +\n  scale_x_discrete(limits = c(\"Sex\", \"Age\", \"Knowledge\", \"Major\", \"Method\", \"Title\", \"Cluster\"), expand = c(.1, .1)) +\n  geom_alluvium(aes(fill = cluster, text = cluster), width = 0.25) +\n  geom_stratum(aes(text = after_stat(stratum)), width = 0.25) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Students Data Distribution and Flow\",\n       y = \"Number of Students\",\n       x = \"\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n# 转换为互动图表并添加悬停信息\ninteractive_plot &lt;- ggplotly(ggplot_alluvial, tooltip = \"text\") %&gt;% layout(showlegend = TRUE)\n\n# 显示互动图表\ninteractive_plot\n\n\n\n\n\n\n# 层次聚类\nhclust_result &lt;- hclust(gower_dist, method = \"ward.D2\")\n\n# 绘制树状图\ndend &lt;- as.dendrogram(hclust_result)\nplot(dend, main = \"Dendrogram of Students Data\", xlab = \"Students\", sub = \"\", ylab = \"Height\")"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html",
    "href": "Detailed_steps/Task2/Task2.html",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "",
    "text": "show the code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(fmsb)\nlibrary(reshape2)\nlibrary(networkD3)\nlibrary(ggalluvial)\nlibrary(fastDummies)\nlibrary(parallelPlot)\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial)"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#converting-the-formatt-of-the-column",
    "href": "Detailed_steps/Task2/Task2.html#converting-the-formatt-of-the-column",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "2.1 Converting the formatt of the column",
    "text": "2.1 Converting the formatt of the column\n\nmerged_data &lt;- merged_data %&gt;%\n  mutate(day = wday(as.POSIXct(time, origin = \"1970-01-01\", tz = \"UTC\"), week_start = 1))\nunique(merged_data$day)\n\n[1] 2 5 1 4 6 3 7"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#final-data",
    "href": "Detailed_steps/Task2/Task2.html#final-data",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "2.2 Final data",
    "text": "2.2 Final data\nNow we merged with student information and rearrange the column for the further analysis.\n\nstu_info &lt;- merged_data %&gt;%\n  distinct(student_ID, .keep_all = TRUE) %&gt;%\n  select(student_ID, sex, age, major)\n\nmerged_data &lt;- merged_data %&gt;%\n  mutate(rate = actual_score / question_score) %&gt;%\n  select(-actual_score, -question_score)\n\n# 计算每个学生的平均rate\navg_rate &lt;- merged_data %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(average_rate = mean(rate, na.rm = TRUE))\n\n# 将day中的1 2 3 4 5计为'week'，6 7计为'weekend'\nmerged_data &lt;- merged_data %&gt;%\n  mutate(week_category = ifelse(day %in% 1:5, \"week\", \"weekend\"))\n\n# 计算每个学生每种knowledge的百分比\nknowledge_percentage &lt;- merged_data %&gt;%\n  group_by(student_ID, knowledge) %&gt;%\n  summarise(counts = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(student_ID) %&gt;%\n  mutate(total_counts = sum(counts),\n         percentage = counts / total_counts) %&gt;%\n  select(student_ID, knowledge, percentage) %&gt;%\n  spread(key = knowledge, value = percentage, fill = 0)\n\n# 计算每个学生在week和weekend的百分比\nweekend_percentage &lt;- merged_data %&gt;%\n  group_by(student_ID, week_category) %&gt;%\n  summarise(counts = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(student_ID) %&gt;%\n  mutate(total_counts = sum(counts),\n         percentage = counts / total_counts) %&gt;%\n  select(student_ID, week_category, percentage) %&gt;%\n  spread(key = week_category, value = percentage, fill = 0)\n\n# 合并学生信息和计算结果\nfinal_data &lt;- stu_info %&gt;%\n  left_join(avg_rate, by = \"student_ID\") %&gt;%\n  left_join(merged_data %&gt;% select(student_ID, -day) %&gt;% distinct(), by = \"student_ID\") %&gt;%\n  left_join(knowledge_percentage, by = \"student_ID\") %&gt;%\n  left_join(weekend_percentage, by = \"student_ID\")\n\n\n# 查看结果\nhead(final_data)\n\n            student_ID    sex age  major average_rate       b3C9s      g7R2j\n1 d554e419f820fa5cb0ca   male  19 J40192    0.5277778 0.063063063 0.09909910\n2 b92448e12093e45dc6ff female  21 J23517    0.2256691 0.083941606 0.06204380\n3 6b22922bfe081d68c7a6 female  23 J87654    0.3587073 0.105769231 0.11858974\n4 72d383f55803c5a81bd6   male  20 J87654    0.5598291 0.087179487 0.18461538\n5 3c89c7f1db26ae691db8   male  21 J40192    0.2055268 0.007772021 0.02590674\n6 f26c1d520ac39293c68a   male  20 J40192    0.1943812 0.009111617 0.02277904\n  g7R2j_m3D1v     m3D1v      r8S3g     t5V9e      y9W5d y9W5d_k4W1c y9W5d_m3D1v\n1 0.009009009 0.2072072 0.16216216 0.1441441 0.15315315 0.090090090  0.06306306\n2 0.036496350 0.2189781 0.09854015 0.2737226 0.17883212 0.010948905  0.02554745\n3 0.003205128 0.2243590 0.09615385 0.3333333 0.05128205 0.009615385  0.02564103\n4 0.020512821 0.2256410 0.07179487 0.1589744 0.09743590 0.041025641  0.09230769\n5 0.002590674 0.4896373 0.09067358 0.2668394 0.04663212 0.028497409  0.03626943\n6 0.004555809 0.2756264 0.19589977 0.1708428 0.15945330 0.047835991  0.07972665\n  y9W5d_s8Y2f      week   weekend\n1 0.009009009 1.0000000 0.0000000\n2 0.010948905 0.7737226 0.2262774\n3 0.032051282 0.5512821 0.4487179\n4 0.020512821 0.7435897 0.2564103\n5 0.005181347 0.9119171 0.0880829\n6 0.034168565 0.8861048 0.1138952"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#question",
    "href": "Detailed_steps/Task2/Task2.html#question",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.1 question",
    "text": "3.1 question\n\n# 提取每个学生提交记录中出现频率最高的title\ntitle_frequency &lt;- merged_data %&gt;%\n  group_by(student_ID, title_ID) %&gt;%\n  summarise(frequency = n(), .groups = 'drop') %&gt;%\n  arrange(student_ID, desc(frequency)) %&gt;%\n  distinct(student_ID, .keep_all = TRUE) %&gt;%\n  select(student_ID, title_ID)\n\n# 合并数据框 time 和 title_frequency\nquestion &lt;- final_data %&gt;%\n  select(student_ID, week, weekend) %&gt;%\n  left_join(title_frequency, by = \"student_ID\")\n\n# 查看结果\nhead(question)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n\n\n\ntitle_encode &lt;- readRDS('title_encode.rds')\n# 重命名编码表列\ncolnames(title_encode) &lt;- c(\"title_ID\", \"title_pre\")\n\n# 合并title_frequency和title_encoding\ntitle_frequency_encoded &lt;- title_frequency %&gt;%\n  left_join(title_encode, by = \"title_ID\") %&gt;%\n  select(student_ID, title_pre)\n\n# 合并数据框 time 和 title_frequency_encoded\nquestion &lt;- question %&gt;%\n  left_join(title_frequency_encoded, by = \"student_ID\")\n\n# 查看结果\nhead(question)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre\n1         8\n2         8\n3        11\n4        38\n5        16\n6        15"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#method",
    "href": "Detailed_steps/Task2/Task2.html#method",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.3 method",
    "text": "3.3 method\n\n# 提取每个学生提交记录中出现频率最高的title\nmethod_frequency &lt;- merged_data %&gt;%\n  group_by(student_ID, method) %&gt;%\n  summarise(frequency = n(), .groups = 'drop') %&gt;%\n  arrange(student_ID, desc(frequency)) %&gt;%\n  distinct(student_ID, .keep_all = TRUE) %&gt;%\n  select(student_ID, method)\n\n# 合并数据框 time 和 title_frequency\nmethod &lt;- question %&gt;%\n  left_join(method_frequency, by = \"student_ID\")\n\n# 查看结果\nhead(method)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre                      method\n1         8 Method_Cj9Ya2R7fZd6xs1q5mNQ\n2         8 Method_BXr9AIsPQhwNvyGdZL57\n3        11 Method_BXr9AIsPQhwNvyGdZL57\n4        38 Method_gj1NLb4Jn7URf9K2kQPd\n5        16 Method_gj1NLb4Jn7URf9K2kQPd\n6        15 Method_m8vwGkEZc3TSW2xqYUoR\n\n\n\nmethod_encode &lt;- readRDS('method_encode.rds')\ncolnames(method_encode) &lt;- c(\"method\", \"method_pre\")\n\n# 合并title_frequency和title_encoding\nmethod_frequency_encoded &lt;- method_frequency %&gt;%\n  left_join(method_encode, by = \"method\") %&gt;%\n  select(student_ID, method, method_pre)\n\n# 合并数据框 time 和 title_frequency_encoded\nmethod &lt;- question %&gt;%\n  left_join(method_frequency_encoded, by = \"student_ID\")\n\n# 查看结果\nhead(method)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre                      method method_pre\n1         8 Method_Cj9Ya2R7fZd6xs1q5mNQ          5\n2         8 Method_BXr9AIsPQhwNvyGdZL57          4\n3        11 Method_BXr9AIsPQhwNvyGdZL57          4\n4        38 Method_gj1NLb4Jn7URf9K2kQPd          3\n5        16 Method_gj1NLb4Jn7URf9K2kQPd          3\n6        15 Method_m8vwGkEZc3TSW2xqYUoR          2"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#knowledge",
    "href": "Detailed_steps/Task2/Task2.html#knowledge",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.3 knowledge",
    "text": "3.3 knowledge\n\n# 创建 knowledge dataframe 并放入 student_ID 列的内容\nknowledge &lt;- data.frame(student_ID = final_data$student_ID)\n\n# 找到每一行 6-15 列的最高值对应的列名\nknowledge$knowledge &lt;- apply(final_data[, 6:15], 1, function(row) {\n  colnames(final_data)[6:15][which.max(row)]\n})\n\n# 查看结果\nhead(knowledge)\n\n            student_ID knowledge\n1 d554e419f820fa5cb0ca     m3D1v\n2 b92448e12093e45dc6ff     t5V9e\n3 6b22922bfe081d68c7a6     t5V9e\n4 72d383f55803c5a81bd6     m3D1v\n5 3c89c7f1db26ae691db8     m3D1v\n6 f26c1d520ac39293c68a     m3D1v\n\n\n\nknowledge_encode &lt;- readRDS('knowledge_encode.rds')\n# 重命名编码表的列\ncolnames(knowledge_encode) &lt;- c(\"knowledge\", \"knowledge_pre\")\n\n# 合并知识偏好和编码表\nknowledge &lt;- knowledge %&gt;%\n  left_join(knowledge_encode, by = \"knowledge\") \n\nknowledge &lt;- method %&gt;%\n  left_join(knowledge, by = \"student_ID\")\n\n# 查看结果\nhead(knowledge)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre                      method method_pre knowledge knowledge_pre\n1         8 Method_Cj9Ya2R7fZd6xs1q5mNQ          5     m3D1v             3\n2         8 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n3        11 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n4        38 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n5        16 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n6        15 Method_m8vwGkEZc3TSW2xqYUoR          2     m3D1v             3"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#correct-rate-trend-score",
    "href": "Detailed_steps/Task2/Task2.html#correct-rate-trend-score",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.4 Correct rate trend score",
    "text": "3.4 Correct rate trend score\n\ntitle_info &lt;- readRDS(\"title_info.rds\")\n# 初始化结果数据框\ntrend_scores &lt;- data.frame()\n\n# 定义计算正确率趋势的函数\ncalculate_trend &lt;- function(data) {\n  if (nrow(data) &lt; 2) {\n    return(NA)\n  }\n  model &lt;- lm(rate ~ attempt, data = data)\n  return(coef(model)[2]) # 返回斜率\n}\n\n# 遍历每位学生\nfor (I in 1:nrow(knowledge)) {\n  student_id &lt;- knowledge$student_ID[I]\n  title_id &lt;- knowledge$title_ID[I]\n  \n  # 找到该学生在该题目的所有答题记录并排序\n  student_data &lt;- merged_data %&gt;%\n    filter(student_ID == student_id, title_ID == title_id) %&gt;%\n    arrange(time)\n  \n  # 增加attempt列\n  student_data &lt;- student_data %&gt;%\n    mutate(attempt = row_number() - 1)\n  \n  # 计算该学生在该题目的正确率趋势\n  trend &lt;- calculate_trend(student_data)\n  \n  # 将结果存储在结果数据框中\n  trend_scores &lt;- rbind(trend_scores, data.frame(student_ID = student_id, title_ID = title_id, trend = trend))\n}\n\n# 将title_info中的分数信息合并到结果数据框中\ntrend_scores &lt;- trend_scores %&gt;%\n  left_join(title_info %&gt;% select(title_ID, score), by = \"title_ID\") %&gt;%\n  mutate(correct_rate_trend_score = trend * score)\n\n# 查看结果\nhead(trend_scores)\n\n            student_ID                      title_ID        trend score\n1 d554e419f820fa5cb0ca Question_3MwAFlmNO8EKrpY5zjUd  0.045454545     2\n2 b92448e12093e45dc6ff Question_3MwAFlmNO8EKrpY5zjUd  0.010881801     2\n3 6b22922bfe081d68c7a6 Question_s6VmP1G4UbEQWRYHK9Fd -0.002104033     2\n4 72d383f55803c5a81bd6 Question_5fgqjSBwTPG7KUV3it6O  0.005128205     3\n5 3c89c7f1db26ae691db8 Question_NixCn84GdK2tySa5rB1V  0.017742644     3\n6 f26c1d520ac39293c68a Question_TmKaGvfNoXYq4FZ2JrBu  0.008786742     3\n  correct_rate_trend_score\n1              0.090909091\n2              0.021763602\n3             -0.004208065\n4              0.015384615\n5              0.053227931\n6              0.026360225\n\n\n\n# 确保每个 student_ID 只有一个 correct_rate_trend_score，取平均值\ntrend_scores_agg &lt;- trend_scores %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(trend_score = mean(correct_rate_trend_score, na.rm = TRUE))\n\n# 合并到 knowledge 数据框中\ncluster_data &lt;- knowledge %&gt;%\n  left_join(trend_scores_agg, by = 'student_ID')\n\n# 查看结果\nhead(cluster_data)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre                      method method_pre knowledge knowledge_pre\n1         8 Method_Cj9Ya2R7fZd6xs1q5mNQ          5     m3D1v             3\n2         8 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n3        11 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n4        38 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n5        16 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n6        15 Method_m8vwGkEZc3TSW2xqYUoR          2     m3D1v             3\n   trend_score\n1  0.090909091\n2  0.021763602\n3 -0.004208065\n4  0.015384615\n5  0.053227931\n6  0.026360225\n\n\n\n# 遍历每位学生\nplot_data &lt;- data.frame()\n\nfor (I in 1:nrow(knowledge)) {\n  student_id &lt;- knowledge$student_ID[I]\n  title_id &lt;- knowledge$title_ID[I]\n  \n  # 找到该学生在该题目的所有答题记录并排序\n  student_data &lt;- merged_data %&gt;%\n    filter(student_ID == student_id, title_ID == title_id) %&gt;%\n    arrange(time)\n  \n  # 增加attempt列\n  student_data &lt;- student_data %&gt;%\n    mutate(attempt = row_number() - 1)\n  \n  # 保留尝试次数大于10的学生数据\n  if (nrow(student_data) &gt; 10) {\n    plot_data &lt;- rbind(plot_data, student_data)\n  }\n}\n\n# 绘制散点图和趋势线\nggplot(plot_data, aes(x = attempt, y = rate)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts\",\n       x = \"Attempt\",\n       y = \"Correct Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 如果需要按学生分组进行绘图\nggplot(plot_data, aes(x = attempt, y = rate, color = student_ID)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\") +\n  labs(title = \"Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts\",\n       x = \"Attempt\",\n       y = \"Correct Rate\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") # 如果学生太多，可以移除图例"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#sex-age-major",
    "href": "Detailed_steps/Task2/Task2.html#sex-age-major",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "3.5 sex, age, major",
    "text": "3.5 sex, age, major\n\n# 合并 student_info 中的 sex, age, major 列到 cluster_data 中\ncluster_data &lt;- cluster_data %&gt;%\n  left_join(stu_info %&gt;% select(student_ID, sex, age, major), by = \"student_ID\")\n\n# 查看结果\nhead(cluster_data)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre                      method method_pre knowledge knowledge_pre\n1         8 Method_Cj9Ya2R7fZd6xs1q5mNQ          5     m3D1v             3\n2         8 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n3        11 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n4        38 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n5        16 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n6        15 Method_m8vwGkEZc3TSW2xqYUoR          2     m3D1v             3\n   trend_score    sex age  major\n1  0.090909091   male  19 J40192\n2  0.021763602 female  21 J23517\n3 -0.004208065 female  23 J87654\n4  0.015384615   male  20 J87654\n5  0.053227931   male  21 J40192\n6  0.026360225   male  20 J40192\n\n\nfemale = 1\nmale = 0\n\nmajor_encode &lt;- readRDS('major_encode.rds')\n\n# 对 sex 进行编码\ncluster_data &lt;- cluster_data %&gt;%\n  mutate(sex = ifelse(sex == \"female\", 1, 0))\n\n# 合并 major 编码\nmajor_encode &lt;- major_encode %&gt;%\n  rename(major_name = major1, major_code = major_encode)\n\ncluster_data &lt;- cluster_data %&gt;%\n  left_join(major_encode, by = c(\"major\" = \"major_name\")) %&gt;%\n  select(-major) %&gt;%\n  rename(major = major_code) \n\n# 查看结果\nhead(cluster_data)\n\n            student_ID      week   weekend                      title_ID\n1 d554e419f820fa5cb0ca 1.0000000 0.0000000 Question_3MwAFlmNO8EKrpY5zjUd\n2 b92448e12093e45dc6ff 0.7737226 0.2262774 Question_3MwAFlmNO8EKrpY5zjUd\n3 6b22922bfe081d68c7a6 0.5512821 0.4487179 Question_s6VmP1G4UbEQWRYHK9Fd\n4 72d383f55803c5a81bd6 0.7435897 0.2564103 Question_5fgqjSBwTPG7KUV3it6O\n5 3c89c7f1db26ae691db8 0.9119171 0.0880829 Question_NixCn84GdK2tySa5rB1V\n6 f26c1d520ac39293c68a 0.8861048 0.1138952 Question_TmKaGvfNoXYq4FZ2JrBu\n  title_pre                      method method_pre knowledge knowledge_pre\n1         8 Method_Cj9Ya2R7fZd6xs1q5mNQ          5     m3D1v             3\n2         8 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n3        11 Method_BXr9AIsPQhwNvyGdZL57          4     t5V9e             5\n4        38 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n5        16 Method_gj1NLb4Jn7URf9K2kQPd          3     m3D1v             3\n6        15 Method_m8vwGkEZc3TSW2xqYUoR          2     m3D1v             3\n   trend_score sex age major\n1  0.090909091   0  19     4\n2  0.021763602   1  21     1\n3 -0.004208065   1  23     2\n4  0.015384615   0  20     2\n5  0.053227931   0  21     4\n6  0.026360225   0  20     4"
  },
  {
    "objectID": "Detailed_steps/Task2/Task2.html#k-means-clustering",
    "href": "Detailed_steps/Task2/Task2.html#k-means-clustering",
    "title": "Take Home Exercise 3: Learning Behavior Patterns Analysis",
    "section": "4.1 k-means clustering",
    "text": "4.1 k-means clustering\n\n# 检查数据中的NA值\ncolSums(is.na(cluster_data))\n\n   student_ID          week       weekend      title_ID     title_pre \n            0             0             0             0             0 \n       method    method_pre     knowledge knowledge_pre   trend_score \n            0             0             0             0             3 \n          sex           age         major \n            0             0             0 \n\n# 处理NA值，使用中位数填补\ncluster_data_clean &lt;- cluster_data %&gt;%\n  mutate(across(2:10, ~ifelse(is.na(.), median(., na.rm = TRUE), .)))\n\n# 计算相关矩阵\nSLM.cor &lt;- cor(cluster_data_clean[, c(2, 3, 5, 7, 9, 10, 11, 12, 13)], use = \"complete.obs\")\n\n# 绘制相关图\ncorrplot(SLM.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order = \"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\ncolSums(is.na(cluster_data_clean))\n\n   student_ID          week       weekend      title_ID     title_pre \n            0             0             0             0             0 \n       method    method_pre     knowledge knowledge_pre   trend_score \n            0             0             0             0             0 \n          sex           age         major \n            0             0             0 \n\n\n\n# Exclude non-numeric columns\ncluster_numeric &lt;- cluster_data_clean %&gt;%\n  select(-student_ID, -title_ID, -method, -knowledge)\n\n# Function to compute silhouette widths\nsilhouette_analysis &lt;- function(data, max_clusters) {\n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(kmeans_result$cluster, dist(data))\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 18\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(cluster_numeric, max_clusters)\n\n# Plot the average silhouette widths\nplot(1:max_clusters, avg_sil_widths, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\n\n\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 18\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(cluster_numeric, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")\n\n\n\n\n\n\n\n\n\n# Drop the student_ID column\nclustering_data &lt;- cluster_data_clean %&gt;%\n  select(-student_ID, -title_ID, -method, -knowledge)\n\n# Standardize the data\nclustering_data_scaled &lt;- scale(clustering_data)\n\n# Perform k-means clustering\nset.seed(123)  # For reproducibility\nkmeans_result &lt;- kmeans(clustering_data_scaled, centers = 2, nstart = 25)\n\n# Add the cluster assignments to the original data\ncluster_data_clean$cluster &lt;- kmeans_result$cluster\n\n\n# Perform PCA\npca_result &lt;- prcomp(clustering_data[-1], scale. = TRUE)\n\n# Get PCA scores\npca_scores &lt;- as.data.frame(predict(pca_result))\n\n# Add cluster information to PCA scores\npca_scores$cluster &lt;- factor(cluster_data_clean$cluster)\n\n# Plot PCA results with cluster color coding\npca_plot &lt;- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_discrete(name = \"Cluster\") +\n  labs(x = \"Principal Component 1\", y = \"Principal Component 2\",\n       title = \"PCA Plot of Clusters\") +\n  theme_minimal()\n\n# Display the plot\npca_plot\n\n\n\n\n\n\n\n\n\ncluster_factor &lt;- cluster_data_clean\ncluster_factor$cluster &lt;- as.character(cluster_factor$cluster)\n\nggparcoord(data = cluster_factor, \n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30, size = 20))\n\n\n\n\n\n\n\n\n\nggparcoord(data = cluster_factor, \n           columns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30, size = 20))\n\n\n\n\n\n\n\nggparcoord(data = cluster_factor, \ncolumns = c(2:13), \n           groupColumn = 14,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n  facet_wrap(~ cluster)+\n  theme(axis.text.x = element_text(angle = 30, size = 20))\n\n\n\n\n\n\n\n\n\n# 将分类特征转换为因子类型\ncategorical_columns &lt;- c(\"sex\", \"age\", \"knowledge_pre\", \"major\", \"method_pre\", \"title_pre\", \"cluster\")\ncluster_data_clean[categorical_columns] &lt;- lapply(cluster_data_clean[categorical_columns], as.factor)\n\n# 创建alluvial plot\nggplot_alluvial &lt;- ggplot(cluster_data_clean,\n       aes(axis1 = sex, axis2 = age, axis3 = knowledge_pre, axis4 = major, axis5 = method_pre, axis6 = title_pre, axis7 = cluster,\n           y = ..count..)) +\n  scale_x_discrete(limits = c(\"Sex\", \"Age\", \"Knowledge_pre\", \"Major\", \"Method_pre\", \"Title_pre\", \"Cluster\"), expand = c(.1, .1)) +\n  geom_alluvium(aes(fill = cluster, text = cluster), width = 0.25) +\n  geom_stratum(aes(text = after_stat(stratum)), width = 0.25) +\n  theme_minimal() +\n  labs(title = \"Alluvial Plot of Students Data Distribution and Flow\",\n       y = \"Number of Students\",\n       x = \"\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1,size = 10))\n\n# 转换为互动图表并添加悬停信息\ninteractive_plot &lt;- ggplotly(ggplot_alluvial, tooltip = \"text\") %&gt;% layout(showlegend = FALSE)\n\n# 显示互动图表\ninteractive_plot\n\n\n\n\n\n\ncluster_factor1 &lt;- select(cluster_factor, c(1, 2:13))\ncluster_factor_matrix &lt;- data.matrix(cluster_factor1)\n\ncluster_data_d &lt;- dist(normalize(cluster_factor_matrix[, -c(1)]), method = \"euclidean\")\ndend_expend(cluster_data_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6117868\n2      unknown        ward.D2 0.6499743\n3      unknown         single 0.6915407\n4      unknown       complete 0.6795278\n5      unknown        average 0.7191044\n6      unknown       mcquitty 0.6382837\n7      unknown         median 0.5486523\n8      unknown       centroid 0.2916547\n\n\n\ncluster_data_clust &lt;- hclust(cluster_data_d, method = \"average\")\nnum_k &lt;- find_k(cluster_data_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(cluster_factor_matrix[, -c(1)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 2,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,          \n          main=\"Students' Learning Mode Clustering \\nDataTransformation using Normalise Method\",\n          xlab = \"Student_IDs\",\n          ylab = \"Learning Behaior Pattern\"\n)\n\n\n\n\n\n\n# 将需要的列转换为因子类型\ncategorical_columns &lt;- c(\"sex\", \"age\", \"knowledge\", \"major\", \"method\", \"title_ID\", \"cluster\")\ncluster_data_clean[categorical_columns] &lt;- lapply(cluster_data_clean[categorical_columns], as.factor)\n\n# 计算每个分类变量中cluster 1和2的占比\nget_cluster_percentage &lt;- function(data, column) {\n  data %&gt;%\n    group_by(!!sym(column), cluster) %&gt;%\n    summarise(count = n()) %&gt;%\n    mutate(percentage = count / sum(count) * 100) %&gt;%\n    filter(cluster %in% c(1, 2))\n}\n\n# 获取每个分类变量的占比数据框\nsex_cluster_percentage &lt;- get_cluster_percentage(cluster_data_clean, \"sex\")\nage_cluster_percentage &lt;- get_cluster_percentage(cluster_data_clean, \"age\")\nknowledge_cluster_percentage &lt;- get_cluster_percentage(cluster_data_clean, \"knowledge\")\nmajor_cluster_percentage &lt;- get_cluster_percentage(cluster_data_clean, \"major\")\nmethod_cluster_percentage &lt;- get_cluster_percentage(cluster_data_clean, \"method\")\ntitle_cluster_percentage &lt;- get_cluster_percentage(cluster_data_clean, \"title_ID\")\n\n# 打印结果\nprint(sex_cluster_percentage)\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex   cluster count percentage\n  &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 0     1         445       54.7\n2 0     2         368       45.3\n3 1     1         478       59.5\n4 1     2         325       40.5\n\nprint(age_cluster_percentage)\n\n# A tibble: 14 × 4\n# Groups:   age [7]\n   age   cluster count percentage\n   &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 18    1         112       53.1\n 2 18    2          99       46.9\n 3 19    1         129       55.6\n 4 19    2         103       44.4\n 5 20    1         121       57.3\n 6 20    2          90       42.7\n 7 21    1         137       55.2\n 8 21    2         111       44.8\n 9 22    1         127       54.3\n10 22    2         107       45.7\n11 23    1         157       63.6\n12 23    2          90       36.4\n13 24    1         140       60.1\n14 24    2          93       39.9\n\nprint(knowledge_cluster_percentage)\n\n# A tibble: 18 × 4\n# Groups:   knowledge [10]\n   knowledge   cluster count percentage\n   &lt;fct&gt;       &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 b3C9s       1           7       43.8\n 2 b3C9s       2           9       56.2\n 3 g7R2j       1          67       59.8\n 4 g7R2j       2          45       40.2\n 5 g7R2j_m3D1v 2           2      100  \n 6 m3D1v       1         409       57.9\n 7 m3D1v       2         297       42.1\n 8 r8S3g       1         196       61.8\n 9 r8S3g       2         121       38.2\n10 t5V9e       1         123       64.4\n11 t5V9e       2          68       35.6\n12 y9W5d       1         115       45.6\n13 y9W5d       2         137       54.4\n14 y9W5d_k4W1c 1           4       66.7\n15 y9W5d_k4W1c 2           2       33.3\n16 y9W5d_m3D1v 1           2       20  \n17 y9W5d_m3D1v 2           8       80  \n18 y9W5d_s8Y2f 2           4      100  \n\nprint(major_cluster_percentage)\n\n# A tibble: 10 × 4\n# Groups:   major [5]\n   major cluster count percentage\n   &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 1     1         200       63.1\n 2 1     2         117       36.9\n 3 2     1         193       61.1\n 4 2     2         123       38.9\n 5 3     1         217       61.5\n 6 3     2         136       38.5\n 7 4     1         162       49.1\n 8 4     2         168       50.9\n 9 5     1         151       50.3\n10 5     2         149       49.7\n\nprint(method_cluster_percentage)\n\n# A tibble: 10 × 4\n# Groups:   method [5]\n   method                      cluster count percentage\n   &lt;fct&gt;                       &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Method_5Q4KoXthUuYz3bvrTDFm 1         154       44.9\n 2 Method_5Q4KoXthUuYz3bvrTDFm 2         189       55.1\n 3 Method_BXr9AIsPQhwNvyGdZL57 1         211       65.1\n 4 Method_BXr9AIsPQhwNvyGdZL57 2         113       34.9\n 5 Method_Cj9Ya2R7fZd6xs1q5mNQ 1         231       70.4\n 6 Method_Cj9Ya2R7fZd6xs1q5mNQ 2          97       29.6\n 7 Method_gj1NLb4Jn7URf9K2kQPd 1         171       55.9\n 8 Method_gj1NLb4Jn7URf9K2kQPd 2         135       44.1\n 9 Method_m8vwGkEZc3TSW2xqYUoR 1         156       49.5\n10 Method_m8vwGkEZc3TSW2xqYUoR 2         159       50.5\n\nprint(title_cluster_percentage)\n\n# A tibble: 74 × 4\n# Groups:   title_ID [38]\n   title_ID                      cluster count percentage\n   &lt;fct&gt;                         &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Question_3MwAFlmNO8EKrpY5zjUd 1         148       67.3\n 2 Question_3MwAFlmNO8EKrpY5zjUd 2          72       32.7\n 3 Question_3oPyUzDmQtcMfLpGZ0jW 1          67       62.6\n 4 Question_3oPyUzDmQtcMfLpGZ0jW 2          40       37.4\n 5 Question_4nHcauCQ0Y6Pm8DgKlLo 1          15       65.2\n 6 Question_4nHcauCQ0Y6Pm8DgKlLo 2           8       34.8\n 7 Question_5fgqjSBwTPG7KUV3it6O 1          98       43.8\n 8 Question_5fgqjSBwTPG7KUV3it6O 2         126       56.2\n 9 Question_62XbhBvJ8NUSnApgDL94 1           6       46.2\n10 Question_62XbhBvJ8NUSnApgDL94 2           7       53.8\n# ℹ 64 more rows"
  }
]