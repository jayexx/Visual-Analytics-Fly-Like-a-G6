---
title: "Take Home Exercise 3: Learning Behavior Patterns Analysis"
author: "Wang Yuhui"
date: "June 2, 2024"
date-modified: "last-modified"
execute: 
  warning: false
  freeze: true
---

```{r}
#| code-fold: true
#| code-summary: "show the code"
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)
library(cluster)
library(factoextra)
library(fmsb)
library(reshape2)
library(networkD3)
library(ggalluvial)
library(fastDummies)
library(parallelPlot)

pacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply, corrplot, ggalluvial) 
```

# 1 Objective

Mine **personalized learning behavior patterns** based on **learners' characteristics**.

Design and present **learners' profiles** from various perspectives, including:

**-peak answering hours,**

**-preferred question types,**

**-correct answering rates, etc.**

# 2 Data Preparation

## 2.1 Data Observation

first, we merge all the submission record.

```{r}
#| code-fold: true
#| code-summary: "show the code"
file1 <- "data/Data_SubmitRecord/SubmitRecord-Class1.csv"
file2 <- "data/Data_SubmitRecord/SubmitRecord-Class2.csv"
file3 <- "data/Data_SubmitRecord/SubmitRecord-Class3.csv"
file4 <- "data/Data_SubmitRecord/SubmitRecord-Class4.csv"
file5 <- "data/Data_SubmitRecord/SubmitRecord-Class5.csv"
file6 <- "data/Data_SubmitRecord/SubmitRecord-Class6.csv"
file7 <- "data/Data_SubmitRecord/SubmitRecord-Class7.csv"
file8 <- "data/Data_SubmitRecord/SubmitRecord-Class8.csv"
file9 <- "data/Data_SubmitRecord/SubmitRecord-Class9.csv"
file10 <- "data/Data_SubmitRecord/SubmitRecord-Class10.csv"
file11 <- "data/Data_SubmitRecord/SubmitRecord-Class11.csv"
file12 <- "data/Data_SubmitRecord/SubmitRecord-Class12.csv"
file13 <- "data/Data_SubmitRecord/SubmitRecord-Class13.csv"
file14 <- "data/Data_SubmitRecord/SubmitRecord-Class14.csv"
file15 <- "data/Data_SubmitRecord/SubmitRecord-Class15.csv"

# 读取 CSV 文件
data1 <- read.csv(file1)
data2 <- read.csv(file2)
data3 <- read.csv(file3)
data4 <- read.csv(file4)
data5 <- read.csv(file5)
data6 <- read.csv(file6)
data7 <- read.csv(file7)
data8 <- read.csv(file8)
data9 <- read.csv(file9)
data10 <- read.csv(file10)
data11 <- read.csv(file11)
data12 <- read.csv(file12)
data13 <- read.csv(file13)
data14 <- read.csv(file14)
data15 <- read.csv(file15)

submit_data <- bind_rows(data1, data2, data3, data4, data5, data6, data7, data8, data9, data10, data11, data12, data13, data14, data15,)

head(submit_data)
write.csv(submit_data, "data/submit_data.csv", row.names = FALSE)
```

Now we have 3 data sets in total, which are:

-   Student information data

-   Question title information

-   Submission record information

```{r}
#| code-fold: true
#| code-summary: "Show the code"
stu_info <- read.csv('data/Data_Studentinfo.csv')
tit_info <- read.csv('data/Data_Titleinfo.csv')
sub_info <- read.csv('data/submit_data.csv')
summary(stu_info)
summary(tit_info)
summary(sub_info)
```

## 2.2 Data Clean

### 2.2.1 Missing Value

First, we check if there is missing value in these 3 data sets.

```{r}
#| code-fold: true
#| code-summary: "show the code"
missing_values1 <- colSums(is.na(stu_info))
print(missing_values1)

missing_values2 <- colSums(is.na(tit_info))
print(missing_values2)

missing_values3 <- colSums(is.na(sub_info))
print(missing_values3)
```

### 2.2.2 Outliers

There is no missing value in all 3 data sets. Now we see if there are outliers. :

::: panel-tabset
## state

```{r}
#| code-fold: true
#| code-summary: "show the code"
unique_state <- unique(sub_info$state)
print(unique_state)
```

## class

```{r}
#| code-fold: true
#| code-summary: "show the code"
unique_class <- unique(sub_info$class)
print(unique_class)
```

## time consume

```{r}
#| code-fold: true
#| code-summary: "show the code"
unique_timeconsume <- unique(sub_info$timeconsume) 
print(unique_timeconsume)
```
:::

For outliers "�������" , simply remove it.

```{r}
valid_states <- c("Absolutely_Correct", "Absolutely_Error", "Error1", "Error2", "Error3", "Error4", "Error6", "Error7", "Error8", "Error9", "Partially_Correct")

# 过滤数据，只保留 state 列中包含指定值的行
sub_info <- sub_info %>%
  filter(state %in% valid_states)
unique(sub_info$state)
```

For outliers "class" , replace with the highest frequency of the corresponding student_ID.

```{r}
replace_class <- function(df) {
  df$class <- as.character(df$class)
  
  class_indices <- which(df$class == 'class')
  
  for (index in class_indices) {
    student_id <- df$student_ID[index]
    student_classes <- df$class[df$student_ID == student_id & df$class != 'class']
    class_counts <- table(student_classes)
    
    if (length(class_counts) > 0) {
      most_common_class <- names(which.max(class_counts))
      df$class[index] <- most_common_class
    }
  }
  
  return(df)
}
sub_info <- replace_class(sub_info)
unique(sub_info$class)
```

For outliers '-' and '--', remove the corresponding rows.

```{r}
sub_info <- sub_info %>%
  filter(!(timeconsume %in% c('-', '--')))
unique(sub_info$timeconsume)
```

Save the dataset and name it 'sub_info.csv'

```{r}
write.csv(sub_info, 'data/sub_info.csv', row.names = FALSE)
head(sub_info)
```

### 2.2.3 Convert datetime

The time span is from August 31, 2023 to January 25, 2024, a total of 148 days. However, the content in column 'time' is actually in seconds. So we need to convert to datetime.

```{r}
sub_info <- sub_info %>%
  mutate(day = wday(as.POSIXct(time, origin = "1970-01-01", tz = "UTC"), week_start = 1))
unique(sub_info$day)
```

### 2.2.4 Match the unique title_ID with unique knowledge

From the code below we can see some titles match multiple knowledge

```{r}
title_knowledge_check <- tit_info %>%
  group_by(title_ID) %>%
  summarise(knowledge_count = n_distinct(knowledge)) %>%
  filter(knowledge_count > 1)

print(title_knowledge_check)
```

Since we don't know when the students submit the questions, which knowledge they actually focus on, so we use the probability to match the knowledge.

```{r}
title_knowledge_count <- tit_info %>%
  group_by(title_ID) %>%
  summarise(knowledge_list = list(unique(knowledge))) %>%
  mutate(knowledge = sapply(knowledge_list, function(x) ifelse(length(x) > 0, x[1], NA)),
         knowledge1 = sapply(knowledge_list, function(x) ifelse(length(x) > 1, x[2], NA))) %>%
  select(-knowledge_list)

# 合并知识信息到sub_info
set.seed(123) # 确保结果可重复
sub_info <- sub_info %>%
  left_join(title_knowledge_count, by = "title_ID") %>%
  rowwise() %>%
  mutate(knowledge = ifelse(!is.na(knowledge1), 
                            sample(c(knowledge, knowledge1), 1), 
                            knowledge)) %>%
  ungroup() %>%
  select(-knowledge1)

# 查看处理后的数据框前几行
head(sub_info)
```

Finally, we need to calculate the average answering correct rate and average consuming time for each student.

```{r}
sub_info <- sub_info %>%
  left_join(tit_info %>% select(title_ID, score), by = "title_ID")
sub_info <- sub_info %>%
  mutate(rate = score.x / score.y) %>%
  select(-score.x, -score.y)

head(sub_info)
write.csv(sub_info,'data/sub_info.csv',row.names = FALSE)
```

### 2.2.5 Final data

Now we merged with student information and rearrange the column for the further analysis.

```{r}
# 计算每个学生的平均rate
avg_rate <- sub_info %>%
  group_by(student_ID) %>%
  summarise(average_rate = mean(rate, na.rm = TRUE))

# 将day中的1 2 3 4 5计为'week'，6 7计为'weekend'
sub_info <- sub_info %>%
  mutate(week_category = ifelse(day %in% 1:5, "week", "weekend"))

# 计算每个学生每种knowledge的百分比
knowledge_percentage <- sub_info %>%
  group_by(student_ID, knowledge) %>%
  summarise(counts = n()) %>%
  ungroup() %>%
  group_by(student_ID) %>%
  mutate(total_counts = sum(counts),
         percentage = counts / total_counts) %>%
  select(student_ID, knowledge, percentage) %>%
  spread(key = knowledge, value = percentage, fill = 0)

# 计算每个学生在week和weekend的百分比
weekend_percentage <- sub_info %>%
  group_by(student_ID, week_category) %>%
  summarise(counts = n()) %>%
  ungroup() %>%
  group_by(student_ID) %>%
  mutate(total_counts = sum(counts),
         percentage = counts / total_counts) %>%
  select(student_ID, week_category, percentage) %>%
  spread(key = week_category, value = percentage, fill = 0)

# 合并学生信息和计算结果
final_data <- stu_info %>%
  select(-index) %>%
  left_join(avg_rate, by = "student_ID") %>%
  left_join(sub_info %>% select(student_ID, -day) %>% distinct(), by = "student_ID") %>%
  left_join(knowledge_percentage, by = "student_ID") %>%
  left_join(weekend_percentage, by = "student_ID")


# 查看结果
head(final_data)
write.csv(final_data, 'data/final_data.csv')
```

# 3 feature

## 3.1 time

week = 1

weekend = 0

```{r}
# 读取数据集
data <- read.csv("data/final_data.csv")

# 创建新的dataframe
time <- data %>%
  select(student_ID, week, weekend)

# 查看结果
head(time)
```

## 3.2 question

```{r}
# 读取第二个数据集
data2 <- read.csv("data/cleaned_data.csv")

# 提取每个学生提交记录中出现频率最高的title
title_frequency <- data2 %>%
  group_by(student_ID, title_ID) %>%
  summarise(frequency = n(), .groups = 'drop') %>%
  arrange(student_ID, desc(frequency)) %>%
  distinct(student_ID, .keep_all = TRUE) %>%
  select(student_ID, title_ID)

# 合并数据框 time 和 title_frequency
question <- time %>%
  left_join(title_frequency, by = "student_ID")

# 查看结果
head(question)
```

```{r}
# 读取编码表
title_encoding <- read.csv("data/title_encode.csv")

# 重命名编码表列
colnames(title_encoding) <- c("title_ID", "title_pre")

# 合并title_frequency和title_encoding
title_frequency_encoded <- title_frequency %>%
  left_join(title_encoding, by = "title_ID") %>%
  select(student_ID, title_pre)

# 合并数据框 time 和 title_frequency_encoded
question <- time %>%
  left_join(title_frequency_encoded, by = "student_ID")

# 查看结果
head(question)
```

## 3.3 method

```{r}
# 提取每个学生提交记录中出现频率最高的title
method_frequency <- data2 %>%
  group_by(student_ID, method) %>%
  summarise(frequency = n(), .groups = 'drop') %>%
  arrange(student_ID, desc(frequency)) %>%
  distinct(student_ID, .keep_all = TRUE) %>%
  select(student_ID, method)

# 合并数据框 time 和 title_frequency
method <- question %>%
  left_join(method_frequency, by = "student_ID")

# 查看结果
head(method)
```

```{r}
# 读取编码表
method_encoding <- read.csv("data/method_encode.csv")

# 重命名编码表列
colnames(method_encoding) <- c("method", "method_pre")

# 合并title_frequency和title_encoding
method_frequency_encoded <- method_frequency %>%
  left_join(method_encoding, by = "method") %>%
  select(student_ID, method_pre)

# 合并数据框 time 和 title_frequency_encoded
method <- question %>%
  left_join(method_frequency_encoded, by = "student_ID")

# 查看结果
head(method)
write.csv(method, 'data/method.csv', row.names = FALSE)
```

## 3.3 knowledge

```{r}
# 读取第一个数据集
final_data <- read.csv("data/final_data.csv")

# 指定第7到第14列的列名
columns_of_interest <- c('b3C9s', 'g7R2j', 'k4W1c', 'm3D1v', 'r8S3g', 's8Y2f', 't5V9e', 'y9W5d')

# 查找每个学生的指定列中数值最高的列名称
final_data <- final_data %>%
  rowwise() %>%
  mutate(knowledge_pre = columns_of_interest[which.max(c_across(all_of(columns_of_interest)))])

# 读取 method 数据集
method <- read.csv("data/method.csv")

# 合并 final_data 和 method
final_data_with_knowledge <- final_data %>%
  select(student_ID, knowledge_pre)

knowledge <- method %>%
  left_join(final_data_with_knowledge, by = "student_ID")

# 查看结果
head(knowledge)
```

```{r}
# 读取编码表
# 读取数据集
knowledge_encode <- read.csv("data/knowledge_encode.csv")

# 重命名编码表的列
colnames(knowledge_encode) <- c("knowledge_pre", "encoded_knowledge_pre")

colnames(knowledge)
colnames(knowledge_encode)

# 合并知识偏好和编码表
knowledge_encoded <- knowledge %>%
  left_join(knowledge_encode, by = "knowledge_pre") %>%
  select(-knowledge_pre) %>%
  rename(knowledge_pre = encoded_knowledge_pre)


# 查看结果
head(knowledge_encoded)
write.csv(knowledge_encoded, 'data/knowledge.csv', row.names = FALSE)
```

## 3.4 Correct rate trend score

```{r}
# 读取数据集
cleaned_data <- read.csv("data/cleaned_data.csv")
knowledge <- read.csv("data/knowledge.csv")
title_info <- read.csv("data/Data_TitleInfo.csv")
title_encode <- read.csv("data/title_encode.csv")

# 重命名title_encode的列
colnames(title_encode) <- c("title_ID", "encoded_title")

# 合并knowledge和title_encode，找到每位学生的题目名称
knowledge <- knowledge %>%
  left_join(title_encode, by = c("title_pre" = "encoded_title")) %>%
  rename(title = title_pre, title_ID = title_ID)

# 计算correct_rate并添加到cleaned_data
cleaned_data <- cleaned_data %>%
  left_join(title_info %>% select(title_ID, score), by = "title_ID", suffix = c("", "_total")) %>%
  mutate(correct_rate = score / score_total)

# 初始化结果数据框
trend_scores <- data.frame()

# 定义计算正确率趋势的函数
calculate_trend <- function(data) {
  if (nrow(data) < 2) {
    return(NA)
  }
  model <- lm(correct_rate ~ attempt, data = data)
  return(coef(model)[2]) # 返回斜率
}

# 遍历每位学生
for (I in 1:nrow(knowledge)) {
  student_id <- knowledge$student_ID[I]
  title_id <- knowledge$title_ID[I]
  
  # 找到该学生在该题目的所有答题记录并排序
  student_data <- cleaned_data %>%
    filter(student_ID == student_id, title_ID == title_id) %>%
    arrange(time)
  
  # 增加attempt列
  student_data <- student_data %>%
    mutate(attempt = row_number() - 1)
  
  # 计算该学生在该题目的正确率趋势
  trend <- calculate_trend(student_data)
  
  # 将结果存储在结果数据框中
  trend_scores <- rbind(trend_scores, data.frame(student_ID = student_id, title_ID = title_id, trend = trend))
}

# 将title_info中的分数信息合并到结果数据框中
trend_scores <- trend_scores %>%
  left_join(title_info %>% select(title_ID, score), by = "title_ID") %>%
  mutate(correct_rate_trend_score = trend * score)

# 查看结果
head(trend_scores)
```

```{r}
knowledge <- read.csv("data/knowledge.csv")

# 确保每个 student_ID 只有一个 correct_rate_trend_score，取平均值
trend_scores_agg <- trend_scores %>%
  group_by(student_ID) %>%
  summarise(trend_score = mean(correct_rate_trend_score, na.rm = TRUE))

# 合并到 knowledge 数据框中
cluster_data <- knowledge %>%
  left_join(trend_scores_agg, by = 'student_ID')

# 查看结果
head(cluster_data)
```

```{r}
# 读取数据集
cleaned_data <- read.csv("data/cleaned_data.csv")
knowledge <- read.csv("data/knowledge.csv")
title_info <- read.csv("data/Data_TitleInfo.csv")
title_encode <- read.csv("data/title_encode.csv")

# 重命名title_encode的列
colnames(title_encode) <- c("title_ID", "encoded_title")

# 合并knowledge和title_encode，找到每位学生的题目名称
knowledge <- knowledge %>%
  left_join(title_encode, by = c("title_pre" = "encoded_title")) %>%
  rename(title = title_pre, title_ID = title_ID)

# 计算correct_rate并添加到cleaned_data
cleaned_data <- cleaned_data %>%
  left_join(title_info %>% select(title_ID, score), by = "title_ID", suffix = c("", "_total")) %>%
  mutate(correct_rate = score / score_total)

# 初始化结果数据框
trend_scores <- data.frame()

# 定义计算正确率趋势的函数
calculate_trend <- function(data) {
  if (nrow(data) < 2) {
    return(NA)
  }
  model <- lm(correct_rate ~ attempt, data = data)
  return(coef(model)[2]) # 返回斜率
}
# 遍历每位学生
plot_data <- data.frame()

for (I in 1:nrow(knowledge)) {
  student_id <- knowledge$student_ID[I]
  title_id <- knowledge$title_ID[I]
  
  # 找到该学生在该题目的所有答题记录并排序
  student_data <- cleaned_data %>%
    filter(student_ID == student_id, title_ID == title_id) %>%
    arrange(time)
  
  # 增加attempt列
  student_data <- student_data %>%
    mutate(attempt = row_number() - 1)
  
  # 保留尝试次数大于10的学生数据
  if (nrow(student_data) > 10) {
    plot_data <- rbind(plot_data, student_data)
  }
}

# 绘制散点图和趋势线
ggplot(plot_data, aes(x = attempt, y = correct_rate)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue", linetype = "dashed") +
  labs(title = "Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts",
       x = "Attempt",
       y = "Correct Rate") +
  theme_minimal()

# 如果需要按学生分组进行绘图
ggplot(plot_data, aes(x = attempt, y = correct_rate, color = student_ID)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed") +
  labs(title = "Scatter Plot of Correct Rate vs Attempt for Students with more than 10 Attempts",
       x = "Attempt",
       y = "Correct Rate") +
  theme_minimal() +
  theme(legend.position = "none") # 如果学生太多，可以移除图例
```

## 3.5 sex, age, major

```{r}
# 读取数据集
student_info <- read.csv("data/Data_StudentInfo.csv")

# 合并 student_info 中的 sex, age, major 列到 cluster_data 中
cluster_data <- cluster_data %>%
  left_join(student_info %>% select(student_ID, sex, age, major), by = "student_ID")

# 查看结果
head(cluster_data)
```

female = 1

male = 0

```{r}
# 读取编码表
major_encode <- read.csv("data/major_encode.csv")

# 对 sex 进行编码
cluster_data <- cluster_data %>%
  mutate(sex = ifelse(sex == "female", 1, 0))

# 合并 major 编码
major_encode <- major_encode %>%
  rename(major_name = title, major_code = title_encode)

cluster_data <- cluster_data %>%
  left_join(major_encode, by = c("major" = "major_name")) %>%
  select(-major) %>%
  rename(major = major_code) 

cluster_data <- cluster_data %>%
  select(-ncol(cluster_data), -(ncol(cluster_data)-1), -(ncol(cluster_data)-2))
# 查看结果
head(cluster_data)
```

# 4 visualization

## 4.1 clustering

```{r}
# 检查数据中的NA值
colSums(is.na(cluster_data))

```

```{r}
# 检查数据中的NA值
sum(is.na(cluster_data[, 2:9]))

# 处理NA值，使用中位数填补
cluster_data_clean <- cluster_data %>%
  mutate(across(2:9, ~ifelse(is.na(.), median(., na.rm = TRUE), .)))

write.csv(cluster_data_clean, 'data/cluster_data.csv', row.names = FALSE)
# 计算相关矩阵
SLM.cor <- cor(cluster_data_clean[, 2:9], use = "complete.obs")

# 绘制相关图
corrplot(SLM.cor, 
         method = "ellipse", 
         tl.pos = "lt",
         tl.col = "black",
         order = "hclust",
         hclust.method = "ward.D",
         addrect = 3)

# 使用 ggstatsplot 绘制相关矩阵图
ggstatsplot::ggcorrmat(
  data = cluster_data_clean, 
  cor.vars = 2:9
)

colSums(is.na(cluster_data_clean))
```

```{r}
# Exclude non-numeric columns
StudentLM_data_numeric <- cluster_data_clean %>%
  select(-student_ID)

# Function to compute silhouette widths
silhouette_analysis <- function(data, max_clusters) {
  avg_sil_widths <- numeric(max_clusters)
  
  for (k in 2:max_clusters) {
    # Perform k-means clustering
    kmeans_result <- kmeans(data, centers = k, nstart = 25)
    
    # Compute silhouette widths
    sil <- silhouette(kmeans_result$cluster, dist(data))
    
    # Calculate average silhouette width
    avg_sil_widths[k] <- mean(sil[, 3])
  }
  
  return(avg_sil_widths)
}

# Determine the maximum number of clusters to test
max_clusters <- 18

# Perform silhouette analysis
avg_sil_widths <- silhouette_analysis(StudentLM_data_numeric, max_clusters)

# Plot the average silhouette widths
plot(1:max_clusters, avg_sil_widths, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters", ylab = "Average silhouette width",
     main = "Silhouette Analysis for Determining Optimal Number of Clusters")

# Highlight the optimal number of clusters
optimal_clusters <- which.max(avg_sil_widths)
points(optimal_clusters, avg_sil_widths[optimal_clusters], col = "red", pch = 19)
```

```{r}
# Function to compute SSE for different numbers of clusters
compute_sse <- function(data, max_clusters) {
  sse <- numeric(max_clusters)
  
  for (k in 1:max_clusters) {
    # Perform k-means clustering
    kmeans_result <- kmeans(data, centers = k, nstart = 25)
    
    # Compute SSE
    sse[k] <- kmeans_result$tot.withinss
  }
  
  return(sse)
}

# Determine the maximum number of clusters to test
max_clusters <- 18

# Compute SSE for each number of clusters
sse_values <- compute_sse(StudentLM_data_numeric, max_clusters)

# Plot SSE against number of clusters
plot(1:max_clusters, sse_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters", ylab = "SSE",
     main = "Elbow Method for Optimal Number of Clusters")

# Add text for elbow point
elbow_point <- which.min(diff(sse_values)) + 1
text(elbow_point, sse_values[elbow_point], labels = paste("Elbow Point:", elbow_point), pos = 4, col = "red")
```

```{r}
# Drop the student_ID column
clustering_data <- cluster_data_clean %>%
  select(-student_ID)

# Standardize the data
clustering_data_scaled <- scale(clustering_data)

# Perform k-means clustering
set.seed(123)  # For reproducibility
kmeans_result <- kmeans(clustering_data_scaled, centers = 2, nstart = 25)

# Add the cluster assignments to the original data
cluster_data_clean$cluster <- kmeans_result$cluster
```

```{r}
# Perform PCA
pca_result <- prcomp(cluster_data_clean[-1], scale. = TRUE)

# Get PCA scores
pca_scores <- as.data.frame(predict(pca_result))

# Add cluster information to PCA scores
pca_scores$cluster <- factor(cluster_data_clean$cluster)

# Plot PCA results with cluster color coding
pca_plot <- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +
  geom_point(size = 3) +
  scale_color_discrete(name = "Cluster") +
  labs(x = "Principal Component 1", y = "Principal Component 2",
       title = "PCA Plot of Clusters") +
  theme_minimal()

# Display the plot
pca_plot
```

```{r}
#| fig-width: 15
#| fig-height: 12

StudentLM_data_factor <- cluster_data_clean
StudentLM_data_factor$cluster <- as.character(StudentLM_data_factor$cluster)

ggparcoord(data = StudentLM_data_factor, 
           columns = c(2:10), 
           groupColumn = 11,
           scale = "uniminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Parallel Coordinates Plot of Students' learning modes")+
   theme(axis.text.x = element_text(angle = 30))
```

```{r}
#| fig-width: 15
#| fig-height: 30
# 创建alluvial plot
ggplot(cluster_data_clean,
       aes(axis1 = sex, axis2 = major, axis3 = cluster,
           y = week)) +
  scale_x_discrete(limits = c("Sex", "Major", "Cluster"), expand = c(.1, .1)) +
  geom_alluvium(aes(fill = cluster), width = 0.25) +
  geom_stratum(width = 0.25) +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme_minimal() +
  labs(title = "Alluvial Plot of Students by Sex, Major, and Cluster",
       y = "Number of Students",
       x = "")
```

```{r}

StudentLM_data1 <- select(StudentLM_data_factor, c(1, 2:10))
StudentLM_data_matrix <- data.matrix(StudentLM_data1)

StudentLM_data_d <- dist(normalize(StudentLM_data_matrix[, -c(1)]), method = "euclidean")
dend_expend(StudentLM_data_d)[[3]]
```

```{r}
StudentLM_data_clust <- hclust(StudentLM_data_d, method = "average")
num_k <- find_k(StudentLM_data_clust)
plot(num_k)
```

```{r}
#| fig-width: 15
#| fig-height: 12
heatmaply(normalize(StudentLM_data_matrix[, -c(1)]),
          dist_method = "euclidean",
          hclust_method = "average",
          k_row = 10,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,          
          main="Students' Learning Mode Clustering \nDataTransformation using Normalise Method",
          xlab = "Student_IDs",
          ylab = "Learning Behaior Pattern"
)
```

## 4.2 hierarchical clustering

```{r}
# 安装并加载必要的包
if (!require("clustMixType")) install.packages("clustMixType")
if (!require("cluster")) install.packages("cluster")
if (!require("factoextra")) install.packages("factoextra")
if (!require("dplyr")) install.packages("dplyr")

library(clustMixType)
library(cluster)
library(factoextra)
library(dplyr)

# 读取数据集
cluster_data <- read.csv("data/cluster_data.csv")
knowledge_encode <- read.csv("data/knowledge_encode.csv")
major_encode <- read.csv("data/major_encode.csv")
method_encode <- read.csv("data/method_encode.csv")
title_encode <- read.csv("data/title_encode.csv")

# 将编码转换为名称
cluster_data <- cluster_data %>%
  left_join(knowledge_encode, by = c("knowledge_pre" = "knowledge_encode")) %>%
  left_join(major_encode, by = c("major" = "major_encode")) %>%
  left_join(method_encode, by = c("method_pre" = "method_encode")) %>%
  left_join(title_encode, by = c("title_pre" = "title_encode")) %>%
  select(-knowledge_pre, -major, -method_pre, -title_pre)

# 对 sex 进行编码
cluster_data <- cluster_data %>%
  mutate(sex = ifelse(sex == 1, "female", "male"))

# 排除字符类型的列
cluster_data_for_daisy <- cluster_data %>%
  select(-student_ID)

# 手动定义数值和分类特征的列索引
numerical_indices <- c(1, 2, 3, 5)  # 手动定义数值列
categorical_indices <- c( 4, 6, 7, 8, 9)  # 手动定义分类列

# 将分类特征转换为因子类型
cluster_data_for_daisy[, categorical_indices] <- lapply(cluster_data_for_daisy[, categorical_indices], as.factor)

# 标准化数值特征
cluster_data_for_daisy[, numerical_indices] <- scale(cluster_data_for_daisy[, numerical_indices])

# 计算Gower's距离
gower_dist <- daisy(cluster_data_for_daisy, metric = "gower")

# 将Gower's距离矩阵转换为dist对象
gower_dist <- as.dist(gower_dist)

# 使用K-Prototypes算法进行聚类
# 设定聚类数k
k <- 2
kprototypes_result <- kproto(cluster_data_for_daisy[, c(categorical_indices, numerical_indices)], k, nstart = 10)

# 聚类结果
cluster_assignments <- kprototypes_result$cluster

# 检查聚类结果的长度
length(cluster_assignments)

# 将聚类结果添加到原始数据
if(length(cluster_assignments) == nrow(cluster_data)) {
  cluster_data$cluster <- factor(cluster_assignments)
} else {
  stop("The length of cluster assignments does not match the number of rows in the data.")
}

```

```{r}
# 将需要的列转换为因子类型
categorical_columns <- c("sex", "age", "knowledge", "major1", "method", "title", "cluster")
cluster_data[categorical_columns] <- lapply(cluster_data[categorical_columns], as.factor)

# 计算每个分类变量中cluster 1和2的占比
get_cluster_percentage <- function(data, column) {
  data %>%
    group_by(!!sym(column), cluster) %>%
    summarise(count = n()) %>%
    mutate(percentage = count / sum(count) * 100) %>%
    filter(cluster %in% c(1, 2))
}

# 获取每个分类变量的占比数据框
sex_cluster_percentage <- get_cluster_percentage(cluster_data, "sex")
age_cluster_percentage <- get_cluster_percentage(cluster_data, "age")
knowledge_cluster_percentage <- get_cluster_percentage(cluster_data, "knowledge")
major_cluster_percentage <- get_cluster_percentage(cluster_data, "major1")
method_cluster_percentage <- get_cluster_percentage(cluster_data, "method")
title_cluster_percentage <- get_cluster_percentage(cluster_data, "title")

# 打印结果
print(sex_cluster_percentage)
print(age_cluster_percentage)
print(knowledge_cluster_percentage)
print(major_cluster_percentage)
print(method_cluster_percentage)
print(title_cluster_percentage)
```

```{r}
# 创建alluvial plot
ggplot_alluvial <- ggplot(cluster_data,
       aes(axis1 = sex, axis2 = age, axis3 = knowledge, axis4 = major1, axis5 = method, axis6 = title, axis7 = cluster,
           y = ..count..)) +
  scale_x_discrete(limits = c("Sex", "Age", "Knowledge", "Major", "Method", "Title", "Cluster"), expand = c(.1, .1)) +
  geom_alluvium(aes(fill = cluster, text = cluster), width = 0.25) +
  geom_stratum(aes(text = after_stat(stratum)), width = 0.25) +
  theme_minimal() +
  labs(title = "Alluvial Plot of Students Data Distribution and Flow",
       y = "Number of Students",
       x = "") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# 转换为互动图表并添加悬停信息
interactive_plot <- ggplotly(ggplot_alluvial, tooltip = "text") %>% layout(showlegend = FALSE)

# 显示互动图表
interactive_plot
```

```{r}
# 层次聚类
hclust_result <- hclust(gower_dist, method = "ward.D2")

# 绘制树状图
dend <- as.dendrogram(hclust_result)
plot(dend, main = "Dendrogram of Students Data", xlab = "Students", sub = "", ylab = "Height")
```

```{r}
# 将分类特征转换为因子类型
categorical_columns <- c("sex", "age", "knowledge", "major1", "method", "title", "cluster")
cluster_data[, categorical_columns] <- lapply(cluster_data[, categorical_columns], as.factor)

# 创建一个新的表格，显示每个分类变量中每个因子在两个聚类中的占比
create_proportion_table <- function(data, categorical_columns, cluster_col) {
  proportion_tables <- lapply(categorical_columns, function(col) {
    data %>%
      group_by(!!sym(col), !!sym(cluster_col)) %>%
      summarise(count = n(), .groups = 'drop') %>%
      mutate(proportion = count / sum(count)) %>%
      select(-count) %>%
      spread(!!sym(cluster_col), proportion, fill = 0) %>%
      rename_with(~paste0(col, "_", .), -!!sym(col))
  })
  
  # 将所有表格合并到一个数据框中
  proportion_table <- do.call(cbind, proportion_tables)
  return(proportion_table)
}

# 创建显示占比的表格
proportion_table <- create_proportion_table(cluster_data, categorical_columns[-7], "cluster")

# 查看结果
print(proportion_table)

```
